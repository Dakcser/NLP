{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 21: Automatic Summarization  \n",
    "\n",
    "We shall consider structured document containing a title, abstract and a set of subsections. We would like to build a text summarizer such that tracks important keywords in the document. For this purpose, the first step is identify these keywords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list\n",
    "# tarkista löytyykö: lxml, html5lib, requests, selenium, webdriver-manager\n",
    "# lisäohjeita task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Markus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jos nltk ei löydy asenna -> ! pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#from nltk.cluster.util import cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "Assume the initial input is given as html document (choose an example of your own), we hypothesize that important keywords are initially contained in the words of titles, abstract and possibly titles of subsections of the document. Suggest a simple python script that inputs an html document and outputs the lists of words in the title, abstract and title of section/subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 94.0.4606\n",
      "[WDM] - Get LATEST driver version for 94.0.4606\n",
      "[WDM] - Driver [C:\\Users\\Markus\\.wdm\\drivers\\chromedriver\\win32\\94.0.4606.61\\chromedriver.exe] found in cache\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Scalable Nearest Neighbor Algorithms for High Dimensional Data\n",
      "\n",
      " Abstract:For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.\n",
      "\n",
      "\n",
      "Section titles:\n",
      "1.1 Definitions and Notation\n",
      "2.1 Nearest Neighbor Matching Algorithms\n",
      "2.2 Automatic Configuration of NN Algorithms\n",
      "3.1 The Randomized k-d Tree Algorithm\n",
      "3.2 The Priority Search K-Means Tree Algorithm\n",
      "3.3 The Hierarchical Clustering Tree\n",
      "3.4 Automatic Selection of the Optimal Algorithm\n",
      "4.1 Fast Approximate Nearest Neighbor Search\n",
      "4.2 Binary Features\n",
      "5.1 Searching on a Compute Cluster\n",
      "5.2 Evaluation of Distributed Search\n",
      "\n",
      "Subsection titles:\n",
      "2.1.1 Partitioning Trees\n",
      "2.1.2 Hashing Based Nearest Neighbor Techniques\n",
      "2.1.3 Nearest Neighbor Graph Techniques\n",
      "3.2.1 Algorithm Description\n",
      "3.2.2 Analysis\n",
      "4.1.1 Data Dimensionality\n",
      "4.1.2 Search Precision\n",
      "4.1.3 Automatic Selection of Optimal Algorithm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# Kaikki sivut ei anna koko html bodyä käyttämällä pelkkää requestia. Seleniumilla näyttää toimivan useammilla. \n",
    "# pip install -U selenium\n",
    "# pip install webdriver-manager\n",
    "# jos käytät anacondaa eikä meinaa toimia niin kokeile myös $ conda update pip\n",
    "\n",
    "#Collect title, subtitles and abstract from html file\n",
    "\n",
    "url = \"https://ieeexplore.ieee.org/document/6809191\"\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "# Headline\n",
    "headline = soup.find(\"h1\", {\"class\": \"document-title\"}).text\n",
    "print(\"Headline: {}\\n\".format(headline))\n",
    "\n",
    "# Abstact\n",
    "abstract = soup.find(\"div\", {\"class\": \"abstract-text\"}).text\n",
    "print(\"{}\\n\\n\".format(abstract))\n",
    "\n",
    "# Titles of sections\n",
    "article = soup.find(\"div\", {\"id\": \"article\"})\n",
    "# Saattaa joskus heittää erroria article = None, kokeilee vaan uudestaan niin pitäis toisella kertaa mennä\n",
    "sectionTitles = article.find_all(\"h3\")\n",
    "print(\"Section titles:\")\n",
    "for title in sectionTitles:\n",
    "    print(\"{}\".format(title.text))\n",
    "\n",
    "# Titles of subsections\n",
    "subsectionTitles = article.find_all(\"h4\")\n",
    "print(\"\\nSubsection titles:\")\n",
    "for title in subsectionTitles:\n",
    "    print(\"{}\".format(title.text))\n",
    "\n",
    "#driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most computationally expensive part of many computer vision algorithms consists of searching for the most\n",
      " similar matches to high-dimensional vectors, also referred to as nearest neighbor matching. Having an efficient\n",
      " algorithm for performing fast nearest neighbor matching in large data sets can bring speed improvements of several\n",
      " orders of magnitude to many applications. Examples of such problems include finding the best matches for local image\n",
      " features in large data sets [1][2]\n",
      " clustering local features into visual words using the k-means or similar algorithms \n",
      "[3], global image feature matching for scene recognition [4], human pose\n",
      " estimation [5], matching deformable shapes for object recognition \n",
      "[6] or performing normalized cross-correlation (NCC) to compare image patches\n",
      " in large data sets [7]. The nearest neighbor search problem is also of major\n",
      " importance in many other applications, including machine learning, document retrieval, data compression,\n",
      " bio-informatics, and data analysis.\n",
      "It has been shown that using large training sets is key to obtaining good real-life performance from many computer\n",
      " vision methods [2][4]\n",
      "[7]. Today the Internet is a vast resource for such training data \n",
      "[8], but for large data sets the performance of the algorithms employed quickly\n",
      " becomes a key issue.\n",
      "When working with high dimensional features, as with most of those encountered in computer vision applications\n",
      " (image patches, local descriptors, global image descriptors), there is often no known nearest-neighbor search\n",
      " algorithm that is exact and has acceptable performance. To obtain a speed improvement, many practical applications are\n",
      " forced to settle for an approximate search, in which not all the neighbors returned are exact, meaning some are\n",
      " approximate but typically still close to the exact neighbors. In practice it is common for approximate nearest\n",
      " neighbor search algorithms to provide more than 95 percent of the correct neighbors and still be two or more orders of\n",
      " magnitude faster than linear search. In many cases the nearest neighbor search is just a part of a larger application\n",
      " containing other approximations and there is very little loss in performance from using approximate rather than exact\n",
      " neighbors.\n",
      "In this paper we evaluate the most promising nearest-neighbor search algorithms in the literature, propose new\n",
      " algorithms and improvements to existing ones, present a method for performing automatic algorithm selection and\n",
      " parameter optimization, and discuss the problem of scaling to very large data sets using compute clusters. We have\n",
      " released all this work as an open source library named fast library for approximate nearest neighbors (FLANN).\n",
      "In this paper we are concerned with the problem of efficient nearest neighbor search in metric spaces. The nearest\n",
      " neighbor search in a metric space can be defined as follows: given a set of points P=\\{\n",
      " p_1, p_2, \\ldots, p_n\\} in a metric space M and a query point q \\in M, find the element {\\rm NN}(q,P) \\in P that is the closest to \n",
      "q with respect to a metric distance  d :M \\times M\n",
      " \\rightarrow {\\bb R}:\n",
      "\n",
      "{\\rm NN}(q,P) = arg min_{x\\in P}\\ d(q,x).\n",
      "View Source\n",
      "{\\rm NN}(q,P) = arg min_{x\\in P}\\ d(q,x).\n",
      "\n",
      "The nearest neighbor problem consists of finding a method to pre-process the set P such that the operation {\\rm NN}(q,P) can be performed efficiently.\n",
      "We are often interested in finding not just the first closest neighbor, but several closest neighbors. In this case,\n",
      " the search can be performed in several ways, depending on the number of neighbors returned and their distance to the\n",
      " query point: K-nearest neighbor (KNN) search where the goal is to find the closest K points from the query point and radius nearest neighbor search (RNN), where the goal\n",
      " is to find all the points located closer than some distance R from the query point.\n",
      "We define the K-nearest neighbor search more formally in the following manner:\n",
      "\n",
      "{\\rm KNN}(q,P,K) = A,\n",
      "View Source\n",
      "{\\rm KNN}(q,P,K) = A,\n",
      "where A is a set\n",
      " that satisfies the following conditions:\n",
      "\n",
      "\\vert A\\vert = K, A \\subseteq P\n",
      "View Source\n",
      "\\vert A\\vert = K, A \\subseteq P\n",
      "\n",
      "\n",
      "\\forall x\\in A, y \\in P-A, d(q,x)\\le d(q,y).\n",
      "View Source\n",
      "\\forall x\\in A, y \\in P-A, d(q,x)\\le d(q,y).\n",
      "The K-nearest\n",
      " neighbor search has the property that it will always return exactly K neighbors (if there\n",
      " are at least K points in P).\n",
      "The radius nearest neighbor search can be defined as follows:\n",
      "\n",
      "{\\rm RNN}(q,P,R) = \\{ p\\in P, d(q,p)< R\\} .\n",
      "View Source\n",
      "{\\rm RNN}(q,P,R) = \\{ p\\in P, d(q,p)< R\\} .\n",
      "\n",
      "Depending on how the value R is chosen, the radius search can\n",
      " return any number of points between zero and the whole data set. In practice, passing a large value R to radius search and having the search return a large number of points is often very inefficient. \n",
      "Radius K-nearest neighbor (RKNN) search, is a combination of K-nearest neighbor search and radius\n",
      " search, where a limit can be placed on the number of points that the radius search should return:\n",
      "\n",
      "{\\rm RKNN}(q,P,K,R) = A,\n",
      "View Source\n",
      "{\\rm RKNN}(q,P,K,R) = A,\n",
      "such that\n",
      "\n",
      "\\vert A\\vert \\le K, A \\subseteq P\n",
      "View Source\n",
      "\\vert A\\vert \\le K, A \\subseteq P\n",
      "\n",
      "\n",
      "\\forall x \\in A, y \\in P-A, d(q,x)< R\\; {\\rm and }\\; d(q,x)\\le d(q,y) .\n",
      "View Source\n",
      "\\forall x \\in A, y \\in P-A, d(q,x)< R\\; {\\rm and }\\; d(q,x)\\le d(q,y) .\n",
      "\n",
      "Nearest-neighbor search is a fundamental part of many computer vision algorithms and of significant importance in\n",
      " many other fields, so it has been widely studied. This section presents a review of previous work in this area.\n",
      "We review the most widely used nearest neighbor techniques, classified in three categories: partitioning trees,\n",
      " hashing techniques and neighboring graph techniques.\n",
      "The kd-tree [9][10] is one of the\n",
      " best known nearest neighbor algorithms. While very effective in low dimensionality spaces, its performance quickly\n",
      " decreases for high dimensional data.\n",
      "Arya et al. [11] propose a variation of the k-d tree to be used for\n",
      " approximate search by considering (1+\\varepsilon)-approximate nearest\n",
      " neighbors, points for which dist(p, q) \\le (1 + \\varepsilon)dist(p^\\ast, q) where \n",
      "p^\\ast is the true nearest neighbor. The authors also propose the use of a priority queue to\n",
      " speed up the search. This method of approximating the nearest neighbor search is also referred to as “error\n",
      " bound” approximate search.\n",
      "Another way of approximating the nearest neighbor search is by limiting the time spent during the search, or\n",
      " “time bound” approximate search. This method is proposed in [12]\n",
      " where the k-d tree search is stopped early after examining a fixed number of leaf nodes. In practice the\n",
      " time-constrained approximation criterion has been found to give better results than the error-constrained approximate\n",
      " search.\n",
      "Multiple randomized k-d trees are proposed in [13] as a means to speed up\n",
      " approximate nearest-neighbor search. In [14] we perform a wide range of\n",
      " comparisons showing that the multiple randomized trees are one of the most effective methods for matching high\n",
      " dimensional data.\n",
      "Variations of the k-d tree using non-axis-aligned partitioning hyperplanes have been proposed: the PCA-tree \n",
      "[15], the RP-tree [16], and the\n",
      " trinary projection tree [17]. We have not found such algorithms to be more\n",
      " efficient than a randomized k-d tree decomposition, as the overhead of evaluating multiple dimensions during search\n",
      " outweighed the benefit of the better space decomposition.\n",
      "Another class of partitioning trees decompose the space using various clustering algorithms instead of using\n",
      " hyperplanes as in the case of the k-d tree and its variants. Example of such decompositions include the hierarchical\n",
      " k-means tree [18], the GNAT [19],\n",
      " the anchors hierarchy [20], the vp-tree \n",
      "[21], the cover tree [22] and the spill-tree \n",
      "[23]. Nister and Stewenius [24]\n",
      " propose the vocabulary tree, which is searched by accessing a single leaf of a hierarchical k-means tree. Leibe\n",
      " et al. [25] propose a ball-tree data structure constructed using a mixed\n",
      " partitional-agglomerative clustering algorithm. Schindler et al. [26]\n",
      " propose a new way of searching the hierarchical k-means tree. Philbin et al. [2]\n",
      " conducted experiments showing that an approximate flat vocabulary outperforms a vocabulary tree in a\n",
      " recognition task. In this paper we describe a modified k-means tree algorithm that we have found to give the best\n",
      " results for some data sets, while randomized k-d trees are best for others.\n",
      "Jégou et al. [27] propose the product quantization approach in\n",
      " which they decompose the space into low dimensional subspaces and represent the data sets points by compact codes\n",
      " computed as quantization indices in these subspaces. The compact codes are efficiently compared to the query points\n",
      " using an asymmetric approximate distance. Babenko and Lempitsky [28] propose\n",
      " the inverted multi-index, obtained by replacing the standard quantization in an inverted index with product\n",
      " quantization, obtaining a denser subdivision of the search space. Both these methods are shown to be efficient at\n",
      " searching large data sets and they should be considered for further evaluation and possible incorporation into FLANN.\n",
      "\n",
      "Perhaps the best known hashing based nearest neighbor technique is locality sensitive hashing (LSH) \n",
      "[29], which uses a large number of hash functions with the property that the\n",
      " hashes of elements that are close to each other are also likely to be close. Variants of LSH such as multi-probe LSH \n",
      "[30] improves the high storage costs by reducing the number of hash tables,\n",
      " and LSH Forest [31] adapts better to the data without requiring hand tuning\n",
      " of parameters.\n",
      "The performance of hashing methods is highly dependent on the quality of the hashing functions they use and a large\n",
      " body of research has been targeted at improving hashing methods by using data-dependent hashing functions computed\n",
      " using various learning techniques: parameter sensitive hashing [5], spectral\n",
      " hashing [32], randomized LSH hashing from learned metrics \n",
      "[33], kernelized LSH [34], learnt\n",
      " binary embeddings [35], shift-invariant kernel hashing \n",
      "[36], semi-supervised hashing [37],\n",
      " optimized kernel hashing [38] and complementary hashing \n",
      "[39].\n",
      "The different LSH algorithms provide theoretical guarantees on the search quality and have been successfully used in\n",
      " a number of projects, however our experiments reported in Section 4, show that\n",
      " in practice they are usually outperformed by algorithms using space partitioning structures such as the randomized k-d\n",
      " trees and the priority search k-means tree.\n",
      "Nearest neighbor graph methods build a graph structure in which points are vertices and edges connect each point to\n",
      " its nearest neighbors. The query points are used to explore this graph using various strategies in order to get closer\n",
      " to their nearest neighbors. In [40] the authors select a few well separated\n",
      " elements in the graph as “seeds” and start the graph exploration from those seeds in a best-first fashion.\n",
      " Similarly, the authors of [41] perform a best-first exploration of the k-NN\n",
      " graph, but use a hill-climbing strategy and pick the starting points at random. They present recent experiments that\n",
      " compare favourably to randomized KD-trees, so the proposed algorithm should be considered for future evaluation and\n",
      " possible incorporation into FLANN.\n",
      "The nearest neighbor graph methods suffer from a quite expensive construction of the k-NN graph structure. Wang\n",
      " et al. [42] improve the construction cost by building an approximate\n",
      " nearest neighbor graph.\n",
      "There have been hundreds of papers published on nearest neighbor search algorithms, but there has been little\n",
      " systematic comparison to guide the choice among algorithms and set their internal parameters. In practice, and in most\n",
      " of the nearest neighbor literature, setting the algorithm parameters is a manual process carried out by using various\n",
      " heuristics and rarely make use of more systematic approaches.\n",
      "Bawa et al. [31] show that the performance of the standard LSH\n",
      " algorithm is critically dependent on the length of the hashing key and propose the LSH Forest, a self-tuning algorithm\n",
      " that eliminates this data dependent parameter.\n",
      "In a previous paper [14] we have proposed an automatic nearest neighbor\n",
      " algorithm configuration method by combining grid search with a finer grained Nelder-Mead downhill simplex optimization\n",
      " process [43].\n",
      "There has been extensive research on algorithm configuration methods \n",
      "[44][45], however we are not aware of\n",
      " papers that apply such techniques to finding optimum parameters for nearest neighbor algorithms. Bergstra and Bengio \n",
      "[46] show that, except for small parameter spaces, random search can be a more\n",
      " efficient strategy for parameter optimization than grid search.\n",
      "Exact search is too costly for many applications, so this has generated interest in approximate nearest-neighbor\n",
      " search algorithms which return non-optimal neighbors in some cases, but can be orders of magnitude faster than exact\n",
      " search.\n",
      "After evaluating many different algorithms for approximate nearest neighbor search on data sets with a wide range of\n",
      " dimensionality [14][47], we have\n",
      " found that one of two algorithms gave the best performance: the priority search k-means tree or\n",
      " the multiple randomized k-d trees. These algorithms are described in the remainder of this section.\n",
      "\n",
      "The randomized k-d tree algorithm [13], is an approximate nearest neighbor\n",
      " search algorithm that builds multiple randomized k-d trees which are searched in parallel. The trees are built in a\n",
      " similar manner to the classic k-d tree [9]\n",
      "[10], with the difference that where the classic kd-tree algorithm splits data on the dimension with the highest\n",
      " variance, for the randomized k-d trees the split dimension is chosen randomly from the top N_D dimensions with the highest variance. We used the fixed value N_D=5\n",
      " in our\n",
      " implementation, as this performs well across all our data sets and does not benefit significantly from further tuning.\n",
      "\n",
      "When searching the randomized k-d forest, a single priority queue is maintained across all the randomized trees. The\n",
      " priority queue is ordered by increasing distance to the decision boundary of each branch in the queue, so the search\n",
      " will explore first the closest leaves from all the trees. Once a data point has been examined (compared to the query\n",
      " point) inside a tree, it is marked in order to not be re-examined in another tree. The degree of approximation is\n",
      " determined by the maximum number of leaves to be visited (across all trees), returning the best nearest neighbor\n",
      " candidates found up to that point.\n",
      "Fig. 1 shows the value of searching in many randomized kd-trees at the same\n",
      " time. It can be seen that the performance improves with the number of randomized trees up to a certain point (about 20\n",
      " random trees in this case) and that increasing the number of random trees further leads to static or decreasing\n",
      " performance. The memory overhead of using multiple random trees increases linearly with the number of trees, so at\n",
      " some point the speedup may not justify the additional memory used.\n",
      "\n",
      "Speedup obtained by using multiple randomized kd-trees (100K SIFT features data set).\n",
      "Show All\n",
      "Fig. 2 gives an intuition behind why exploring multiple randomized kd-tree\n",
      " improves the search performance. When the query point is close to one of the splitting hyperplanes, its nearest\n",
      " neighbor lies with almost equal probability on either side of the hyperplane and if it lies on the opposite side of\n",
      " the splitting hyperplane, further exploration of the tree is required before the cell containing it will be\n",
      " visited. Using multiple random decompositions increases the probability that in one of them the query point and its\n",
      " nearest neighbor will be in the same cell.\n",
      "\n",
      "Example of randomized kd-trees. The nearest neighbor is across a decision boundary from the query point in\n",
      " the first decomposition, however is in the same cell in the second decomposition.\n",
      "Show All\n",
      "We have found the randomized k-d forest to be very effective in many situations, however on other data sets a\n",
      " different algorithm, the priority search k-means tree, has been more effective at finding approximate\n",
      " nearest neighbors, especially when a high precision is required. The priority search k-means tree tries to better\n",
      " exploit the natural structure existing in the data, by clustering the data points using the full distance across all\n",
      " dimensions, in contrast to the (randomized) k-d tree algorithm which only partitions the data based on one dimension\n",
      " at a time.\n",
      "Nearest-neighbor algorithms that use hierarchical partitioning schemes based on clustering the data points have been\n",
      " previously proposed in the literature [18]\n",
      "[19][24]. These algorithms differ in the way they construct the\n",
      " partitioning tree (whether using k-means, agglomerative or some other form of clustering) and especially in the\n",
      " strategies used for exploring the hierarchical tree. We have developed an improved version that explores the k-means\n",
      " tree using a best-bin-first strategy, by analogy to what has been found to significantly improve the\n",
      " performance of the approximate kd-tree searches.\n",
      "The priority search k-means tree is constructed by partitioning the data points at each level into K distinct regions using k-means clustering, and then applying the same method recursively to the\n",
      " points in each region. The recursion is stopped when the number of points in a region is smaller than K (see Algorithm 1).\n",
      "\n",
      "Show All\n",
      "The tree is searched by initially traversing the tree from the root to the closest leaf, following at each inner\n",
      " node the branch with the closest cluster centre to the query point, and adding all unexplored branches along the path\n",
      " to a priority queue (see Algorithm 2). The priority queue is sorted in increasing distance from the query point to the\n",
      " boundary of the branch being added to the queue. After the initial tree traversal, the algorithm resumes traversing\n",
      " the tree, always starting with the top branch in the queue.\n",
      "\n",
      "Show All\n",
      "The number of clusters K to use when partitioning the data\n",
      " at each node is a parameter of the algorithm, called the branching factor and choosing \n",
      "K is important for obtaining good search performance. In \n",
      "Section 3.4 we propose an algorithm for finding the optimum algorithm\n",
      " parameters, including the optimum branching factor. Fig. 3 contains a\n",
      " visualisation of several hierarchical k-means decompositions with different branching factors.\n",
      "\n",
      "Projections of priority search k-means trees constructed using different branching factors: 4, 32, 128. The\n",
      " projections are constructed using the same technique as in [26], gray values\n",
      " indicating the ratio between the distances to the nearest and the second-nearest cluster centre at each tree level, so\n",
      " that the darkest values (ratio \\approx 1) fall near the boundaries\n",
      " between k-means regions.\n",
      "Show All\n",
      "Another parameter of the priority search k-means tree is I_{max}, the \n",
      "maximum number of iterations to perform in the k-means clustering loop. Performing fewer iterations can\n",
      " substantially reduce the tree build time and results in a slightly less than optimal clustering (if we consider the\n",
      " sum of squared errors from the points to the cluster centres as the measure of optimality). However, we have observed\n",
      " that even when using a small number of iterations, the nearest neighbor search performance is similar to that of the\n",
      " tree constructed by running the clustering until convergence, as illustrated by Fig. 4\n",
      ". It can be seen that using as few as seven iterations we get more than 90 percent of the nearest-neighbor\n",
      " performance of the tree constructed using full convergence, but requiring less than 10 percent of the build time.\n",
      "\n",
      "The influence that the number of k-means iterations has on the search speed of the k-means tree. Figure\n",
      " shows the relative search time compared to the case of using full convergence.\n",
      "Show All\n",
      "The algorithm to use when picking the initial centres in the k-means clustering can be controlled by the \n",
      "C_{alg} parameter. In our experiments (and in the FLANN library) we have used the following\n",
      " algorithms: random selection, Gonzales’ algorithm (selecting the centres to be spaced apart from each other) and\n",
      " KMeans++ algorithm [48]. We have found that the initial cluster selection\n",
      " made only a small difference in terms of the overall search efficiency in most cases and that the random initial\n",
      " cluster selection is usually a good choice for the priority search k-means tree.\n",
      "When analysing the complexity of the priority search k-means tree, we consider the tree construction time, search\n",
      " time and the memory requirements for storing the tree.\n",
      "Construction time complexity. During the construction of the k-means tree, a k-means clustering\n",
      " operation has to be performed for each inner node. Considering a node v with \n",
      "n_v associated data points, and assuming a maximum number of iterations I in the k-means clustering loop, the complexity of the clustering operation is O(n_v d K I), where d represents the data\n",
      " dimensionality. Taking into account all the inner nodes on a level, we have \\sum n_v = n\n",
      ", so the\n",
      " complexity of constructing a level in the tree is O(n d K I). Assuming a balanced tree, the\n",
      " height of the tree will be (\\log\\; n / \\log\\; K), resulting in a total tree\n",
      " construction cost of O(ndKI (\\log\\; n / \\log\\; K)).\n",
      "Search time complexity. In case of the time constrained approximate nearest neighbor search, the\n",
      " algorithm stops after examining L data points. Considering a\n",
      " complete priority search k-means tree with branching factor K, the number of top down tree\n",
      " traversals required is L/K (each leaf node contains \n",
      "K points in a complete k-means tree). During each top-down traversal, the algorithm\n",
      " needs to check O(\\log\\; n / \\log\\; K) inner nodes and one leaf node.\n",
      "For each internal node, the algorithm has to find the branch closest to the query point, so it needs to compute the\n",
      " distances to all the cluster centres of the child nodes, an O(Kd) operation. The unexplored\n",
      " branches are added to a priority queue, which can be accomplished in O(K) amortized cost when\n",
      " using binomial heaps. For the leaf node the distance between the query and all the points in the leaf needs to be\n",
      " computed which takes O(Kd) time. In summary the overall\n",
      " search cost is O(Ld (\\log\\; n / \\log\\; K)).\n",
      "Matching binary features is of increasing interest in the computer vision community with many binary visual\n",
      " descriptors being recently proposed: BRIEF [49], ORB \n",
      "[50], BRISK [51]. Many algorithms\n",
      " suitable for matching vector based features, such as the randomized kd-tree and priority search k-means tree, are\n",
      " either not efficient or not suitable for matching binary features (for example, the priority search k-means tree\n",
      " requires the points to be in a vector space where their dimensions can be independently averaged).\n",
      "Binary descriptors are typically compared using the Hamming distance, which for binary data can be computed as a\n",
      " bitwise XOR operation followed by a bit count on the result (very efficient on computers with hardware support for\n",
      " counting the number of bits set in a word).\n",
      "This section briefly presents a new data structure and algorithm, called the hierarchical clustering tree\n",
      ", which we found to be very effective at matching binary features. For a more detailed description of this\n",
      " algorithm the reader is encouraged to consult [47] and \n",
      "[52].\n",
      "The hierarchical clustering tree performs a decomposition of the search space by recursively clustering the input\n",
      " data set using random data points as the cluster centers of the non-leaf nodes (see Algorithm 3).\n",
      "\n",
      "Show All\n",
      "In contrast to the priority search k-means tree presented above, for which using more than one tree did not bring\n",
      " significant improvements, we have found that building multiple hierarchical clustering trees and searching them in\n",
      " parallel using a common priority queue (the same approach that has been found to work well for randomized kd-trees \n",
      "[13]) resulted in significant improvements in the search performance.\n",
      "Our experiments have revealed that the optimal algorithm for approximate nearest neighbor search is highly dependent\n",
      " on several factors such as the data dimensionality, size and structure of the data set (whether there is any\n",
      " correlation between the features in the data set) and the desired search precision. Additionally, each algorithm has a\n",
      " set of parameters that have significant influence on the search performance (e.g., number of randomized trees,\n",
      " branching factor, number of k-means iterations).\n",
      "As we already mention in Section 2.2, the optimum parameters for a nearest\n",
      " neighbor algorithm are typically chosen manually, using various heuristics. In this section we propose a method for\n",
      " automatic selection of the best nearest neighbor algorithm to use for a particular data set and for choosing its\n",
      " optimum parameters.\n",
      "By considering the nearest neighbor algorithm itself as a parameter of a generic nearest neighbor search routine \n",
      "A, the problem is reduced to determining the parameters \n",
      "\\theta \\in \\Theta that give the best solution, where \\Theta is also known as the \n",
      "parameter configuration space. This can be formulated as an optimization problem in the parameter\n",
      " configuration space:\n",
      "\\mathop{\\rm min}_{\\theta \\in \\Theta }c(\\theta)\n",
      "View Source\n",
      "\\mathop{\\rm min}_{\\theta \\in \\Theta }c(\\theta)\n",
      "with \n",
      "c : \\Theta \\rightarrow {\\bb R} being a cost function indicating\n",
      " how well the search algorithm A, configured with the parameters \n",
      "\\theta, performs on the given input data.\n",
      "We define the cost as a combination of the search time, tree build time, and tree memory overhead. Depending on the\n",
      " application, each of these three factors can have a different importance: in some cases we don’t care much about\n",
      " the tree build time (if we will build the tree only once and use it for a large number of queries), while in other\n",
      " cases both the tree build time and search time must be small (if the tree is built on-line and searched a small number\n",
      " of times). There are also situations when we wish to limit the memory overhead if we work in memory constrained\n",
      " environments. We define the cost function as follows:\n",
      "c(\\theta) = {{ s(\\theta)+w_b b(\\theta)\\over {\\rm min}_{\\theta \\in \\Theta }(s(\\theta)+w_b b(\\theta))} +w_m m(\\theta)},\n",
      " \\eqno{\\hbox{(1)}}\n",
      "View Source\n",
      "c(\\theta) = {{ s(\\theta)+w_b b(\\theta)\\over {\\rm min}_{\\theta \\in \\Theta }(s(\\theta)+w_b b(\\theta))} +w_m m(\\theta)},\n",
      " \\eqno{\\hbox{(1)}}\n",
      "where \n",
      "s(\\theta), b(\\theta) and \n",
      "m(\\theta) represent the search time, tree build time and memory overhead for the tree(s) constructed and\n",
      " queried with parameters \\theta. The memory overhead is measured\n",
      " as the ratio of the memory used by the tree(s) and the memory used by the data: m(\\theta) =\n",
      " m_t(\\theta)/m_d.\n",
      "The weights w_b and \n",
      "w_m are\n",
      " used to control the relative importance of the build time and memory overhead in the overall cost. The build-time\n",
      " weight (w_b) controls the importance of the tree build time relative to the search time. Search\n",
      " time is defined as the time to search for the same number of points as there are in the tree. The time overhead is\n",
      " computed relative to the optimum time cost {\\rm min}_{\\theta \\in \\Theta }(s(\\theta)+w_b\n",
      " b(\\theta)), which is defined as the optimal search and build time if memory usage were not a factor.\n",
      "We perform the above optimization in two steps: a global exploration of the parameter space using grid search,\n",
      " followed by a local optimization starting with the best solution found in the first step. The grid search is a\n",
      " feasible and effective approach in the first step because the number of parameters is relatively low. In the second\n",
      " step we use the Nelder-Mead downhill simplex method [43] to further locally\n",
      " explore the parameter space and fine-tune the best solution obtained in the first step. Although this does not\n",
      " guarantee a global minimum, our experiments have shown that the parameter values obtained are close to optimum in\n",
      " practice.\n",
      "We use random sub-sampling cross-validation to generate the data and the query points when we run the optimization.\n",
      " In FLANN the optimization can be run on the full data set for the most accurate results or using just a fraction of\n",
      " the data set to have a faster auto-tuning process. The parameter selection needs to only be performed once for each\n",
      " type of data set, and the optimum parameter values can be saved and applied to all future data sets of the same type.\n",
      "\n",
      "For the experiments presented in this section we used a selection of data sets with a wide range of sizes and data\n",
      " dimensionality. Among the data sets used are the Winder/Brown patch data set [53]\n",
      ", data sets of randomly sampled data of different dimensionality, data sets of SIFT features of different sizes\n",
      " obtained by sampling from the CD cover data set of [24] as well as a data set\n",
      " of SIFT features extracted from the overlapping images forming panoramas.\n",
      "We measure the accuracy of an approximate nearest neighbor algorithm using the search precision (or\n",
      " just precision), defined as the fraction of the neighbors returned by the approximate algorithm which\n",
      " are exact nearest neighbors. We measure the search performance of an algorithm as the time required to perform a\n",
      " linear search divided by the time required to perform the approximate search and we refer to it as the search\n",
      " speedup or just speedup.\n",
      "We present several experiments we have conducted in order to analyse the performance of the two algorithms described\n",
      " in Section 3.\n",
      "Data dimensionality is one of the factors that has a great impact on the nearest neighbor matching performance. The\n",
      " top of Fig. 5 shows how the search performance degrades as the dimensionality\n",
      " increases in the case of random vectors. The data sets in this case each contain 10^5\n",
      " vectors\n",
      " whose values are randomly sampled from the same uniform distribution. These random data sets are one of the most\n",
      " difficult problems for nearest neighbor search, as no value gives any predictive information about any other value.\n",
      "\n",
      "Search efficiency for data of varying dimensionality. We experimented on both random vectors and image\n",
      " patches, with data sets of size 100K. The random vectors (top figure) represent the hardest case in which dimensions\n",
      " have no correlations, while most real-world problems behave more like the image patches (bottom figure).\n",
      "Show All\n",
      "As can be seen in the top part of Fig. 5, the nearest-neighbor searches have\n",
      " a low efficiency for higher dimensional data (for 68 percent precision the approximate search speed is no better than\n",
      " linear search when the number of dimensions is greater than 800).\n",
      "The performance is markedly different for many real-world data sets. The bottom part of \n",
      "Fig. 5 shows the speedup as a function of dimensionality for the Winder/Brown\n",
      " image patches resampled to achieve varying dimensionality. In this case however, the speedup does not\n",
      " decrease with dimensionality, it’s actually increasing for some precisions. This can be explained by the fact\n",
      " that there exists a strong correlation between the dimensions, so that even for 64\\times\n",
      " 64\n",
      " patches (4,096 dimensions), the similarity between only a few dimensions provides strong evidence for overall patch\n",
      " similarity.\n",
      "Fig. 6 shows four examples of queries on the Trevi data set of patches for\n",
      " different patch sizes.\n",
      "\n",
      "Example of nearest neighbor queries with different patch sizes. The Trevi Fountain patch data set was\n",
      " queried using different patch sizes. The rows are arranged in decreasing order by patch size. The query patch is on\n",
      " the left of each panel, while the following five patches are the nearest neighbors from a set of 100,000 patches.\n",
      " Incorrect matches with respect to ground truth are shown with an X.\n",
      "Show All\n",
      "We use several data sets of different sizes for the experiments in Fig. 7. We\n",
      " construct 100K and 1 million SIFT feature data sets by randomly sampling a data set of over 5 million SIFT\n",
      " features extracted from a collection of CD cover images [24].\n",
      " We also use the 31 million SIFT feature data set from the same source.\n",
      "\n",
      "Search speedup for different data set sizes.\n",
      "Show All\n",
      "The desired search precision determines the degree of speedup that can be obtained with any approximate algorithm.\n",
      " Looking at Fig. 7 (the sift1M data set) we see that if we are willing to accept\n",
      " a precision as low as 60 percent, meaning that 40 percent of the neighbors returned are not the exact nearest\n",
      " neighbors, but just approximations, we can achieve a speedup of three orders of magnitude over linear search (using\n",
      " the multiple randomized kd-trees). However, if we require a precision greater than 90 percent the speedup is smaller,\n",
      " less than 2 orders of magnitude (using the priority search k-means tree).\n",
      "We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple\n",
      " randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN \n",
      "[11] and LSH algorithms [29]\n",
      " on the first data set of 100,000 SIFT features. Since the LSH implementation (the E\n",
      "^2LSH package) solves the R-near neighbor problem (finds the\n",
      " neighbors within a radius R of the query point, not the\n",
      " nearest neighbors), to find the nearest neighbors we have used the approach suggested in the E^2LSH’s user manual: we compute the R-near neighbors for increasing\n",
      " values of R. The parameters for the LSH algorithm were chosen using the parameter estimation tool\n",
      " included in the E^2LSH package. For each case we have\n",
      " computed the precision achieved as the percentage of the query points for which the nearest neighbors were correctly\n",
      " found. Fig. 8 shows that the priority search k-means algorithm outperforms both\n",
      " the ANN and LSH algorithms by about an order of magnitude. The results for ANN are consistent with the experiment in \n",
      "Fig. 1, as ANN uses only a single kd-tree and does not benefit from the speedup\n",
      " due to using multiple randomized trees.\n",
      "\n",
      "Comparison of the search efficiency for several nearest neighbor algorithms.\n",
      "Show All\n",
      "Fig. 9 compares the performance of nearest neighbor matching when the data\n",
      " set contains true matches for each feature in the test set to the case when it contains false matches. A true match is\n",
      " a match in which the query and the nearest neighbor point represent the same entity, for example, in case of SIFT\n",
      " features, they represent image patches of the same object. In this experiment we used two 100K SIFT features data\n",
      " sets, one that has ground truth determined from global image matching and one that is randomly sampled from a 5\n",
      " million SIFT features data set and it contains only false matches for each feature in the test set. Our experiments\n",
      " showed that the randomized kd-trees have a significantly better performance for true matches, when the query features\n",
      " are likely to be significantly closer than other neighbors. Similar results were reported in \n",
      "[54].\n",
      "\n",
      "Search speedup when the query points don’t have “true” matches in the data set versus the\n",
      " case when they have.\n",
      "Show All\n",
      "Fig. 10 shows the difference in performance between the randomized kd-trees\n",
      " and the priority search k-means tree for one of the Winder/Brown patches data set. In this case, the randomized\n",
      " kd-trees algorithm clearly outperforms the priority search k-means algorithm everywhere except for precisions close to\n",
      " 100 percent. It appears that the kd-tree works much better in cases when the intrinsic dimensionality of the data is\n",
      " much lower than the actual dimensionality, presumably because it can better exploit the correlations among dimensions.\n",
      " However, Fig. 7 shows that the k-means tree can perform better for other data\n",
      " sets (especially for high precisions). This shows the importance of performing algorithm selection on each data set.\n",
      "\n",
      "Search speedup for the Trevi Fountain patches data set.\n",
      "Show All\n",
      "In Table 1, we show the results from running the parameter selection\n",
      " procedure described in Section 3.4 on a data set containing 100K randomly\n",
      " sampled SIFT features. We used two different search precisions (60 and 90 percent) and several combinations of the\n",
      " tradeoff factors w_b and \n",
      "w_m.\n",
      " For the build time weight, w_b, we used three different possible\n",
      " values: 0 representing the case where we don’t care about the tree build time, 1 for the case where the tree\n",
      " build time and search time have the same importance and 0.01 representing the case where we care mainly about the\n",
      " search time but we also want to avoid a large build time. Similarly, the memory weight was chosen to be 0 for the case\n",
      " where the memory usage is not a concern, \\infty representing the case where the\n",
      " memory use is the dominant concern and 1 as a middle ground between the two cases.\n",
      "\n",
      "\n",
      "This section evaluates the performance of the hierarchical clustering tree described in \n",
      "Section 3.3.\n",
      "We use the Winder/Brown patches data set [53] to compare the nearest\n",
      " neighbor search performance of the hierarchical clustering tree to that of other well known nearest neighbor search\n",
      " algorithms. For the comparison we use a combination of both vector features such as SIFT, SURF, image patches and\n",
      " binary features such as BRIEF and ORB. The image patches have been downscaled to 16\\times\n",
      " 16\n",
      " pixels and are matched using normalized cross correlation. Fig. 11 shows the\n",
      " nearest neighbor search times for the different feature types. Each point on the graph is computed using the best\n",
      " performing algorithm for that particular feature type (randomized kd-trees or priority search k-means tree for SIFT,\n",
      " SURF, image patches and the hierarchical clustering algorithm for BRIEF and ORB). In each case the optimum choice of\n",
      " parameters that maximizes the speedup for a given precision is used.\n",
      "\n",
      "Absolute search time for different popular feature types (both binary and vector).\n",
      "Show All\n",
      "In Fig. 12 we compare the hierarchical clustering tree with a multi-probe\n",
      " locality sensitive hashing implementation [30]. For the comparison we used\n",
      " data sets of BRIEF and ORB features extracted from the recognition benchmark images data set of \n",
      "[24], containing close to 5 million features. It can be seen that the\n",
      " hierarchical clustering index outperforms the LSH implementation for this data set. The LSH implementation also\n",
      " requires significantly more memory compared to the hierarchical clustering trees for when high precision is required,\n",
      " as it needs to allocate a large number of hash tables to achieve the high search precision. In the experiment of \n",
      "Fig. 12, the multi-probe LSH required six times more memory than the\n",
      " hierarchical search for search precisions above 90 percent.\n",
      "\n",
      "Comparison between the hierarchical clustering index and LSH for the Nister/Stewenius recognition benchmark\n",
      " images data set of about 5 million features.\n",
      "Show All\n",
      "Many papers have shown that using simple non-parametric methods in conjunction with large scale data sets can lead\n",
      " to very good recognition performance [4][7]\n",
      "[55][56]. Scaling to such\n",
      " large data sets is a difficult task, one of the main challenges being the impossibility of loading the data into the\n",
      " main memory of a single machine. For example, the size of the raw tiny images data set of \n",
      "[7] is about 240 GB, which is greater than what can be found on most\n",
      " computers at present. Fitting the data in memory is even more problematic for data sets of the size of those used in \n",
      "[4][8]\n",
      "[55].\n",
      "When dealing with such large amounts of data, possible solutions include performing some dimensionality reduction on\n",
      " the data, keeping the data on the disk and loading only parts of it in the main memory or distributing the data on\n",
      " several computers and using a distributed nearest neighbor search algorithm.\n",
      "Dimensionality reduction has been used in the literature with good results ([7]\n",
      "[27][28]\n",
      "[32][57]), however even with\n",
      " dimensionality reduction it can be challenging to fit the data in the memory of a single machine for very large data\n",
      " sets. Storing the data on the disk involves significant performance penalties due to the performance gap between\n",
      " memory and disk access times. In FLANN we used the approach of performing distributed nearest neighbor search across\n",
      " multiple machines.\n",
      "In order to scale to very large data sets, we use the approach of distributing the data to multiple machines in a\n",
      " compute cluster and perform the nearest neighbor search using all the machines in parallel. The data is distributed\n",
      " equally between the machines, such that for a cluster of N machines each of them will only\n",
      " have to index and search 1/N of the whole data set (although\n",
      " the ratios can be changed to have more data on some machines than others). The final result of the nearest neighbor\n",
      " search is obtained by merging the partial results from all the machines in the cluster once they have completed the\n",
      " search.\n",
      "In order to distribute the nearest neighbor matching on a compute cluster we implemented a Map-Reduce like algorithm\n",
      " using the message passing interface (MPI) specification.\n",
      "Algorithm 4 describes the procedure for building a distributed nearest neighbor matching index. Each process in the\n",
      " cluster executes in parallel and reads from a distributed filesystem a fraction of the data set. All processes build\n",
      " the nearest neighbor search index in parallel using their respective data set fractions.\n",
      "\n",
      "Show All\n",
      "In order to search the distributed index the query is sent from a client to one of the computers in the MPI cluster,\n",
      " which we call the master server (see Fig. 13). By convention the master server\n",
      " is the process with rank 0 in the MPI cluster, however any process in the MPI cluster can play the role of master\n",
      " server.\n",
      "\n",
      "Scaling nearest neighbor search on a compute cluster using message passing interface standard.\n",
      "Show All\n",
      "The master server broadcasts the query to all of the processes in the cluster and then each process can run the\n",
      " nearest neighbor matching in parallel on its own fraction of the data. When the search is complete an MPI reduce\n",
      " operation is used to merge the results back to the master process and the final result is returned to the client.\n",
      "The master server is not a bottleneck when merging the results. The MPI reduce operation is also distributed, as the\n",
      " partial results are merged two by two in a hierarchical fashion from the servers in the cluster to the master server.\n",
      " Additionally, the merge operation is very efficient, since the distances between the query and the neighbors\n",
      " don’t have to be re-computed as they are returned by the nearest neighbor search operations on each server.\n",
      "\n",
      "Show All\n",
      "When distributing a large data set for the purpose of nearest neighbor search we chose to partition the data into\n",
      " multiple disjoint subsets and construct independent indexes for each of those subsets. During search the query is\n",
      " broadcast to all the indexes and each of them performs the nearest neighbor search within its associated data. In a\n",
      " different approach, Aly et al. [58] introduce a distributed k-d tree\n",
      " implementation where they place a root k-d tree on top of all the other trees (leaf trees) with the role of selecting\n",
      " a subset of trees to be searched and only send the query to those trees. They show the distributed k-d tree has higher\n",
      " throughput compared to using independent trees, due to the fact that only a portion of the trees need to be searched\n",
      " by each query.\n",
      "The partitioning of the data set into independent subsets, as described above and implemented in FLANN, has the\n",
      " advantage that it doesn’t depend on the type of index used (randomized kd-trees, priority search k-means tree,\n",
      " hierarchical clustering, LSH) and can be applied to any current or future nearest neighbor algorithm in FLANN. In the\n",
      " distributed k-d tree implementation of [58] the search does not backtrack in\n",
      " the root node, so it is possible that subsets of the data containing near points are not searched at all if the root\n",
      " k-d tree doesn’t select the corresponding leaf k-d trees at the beginning.\n",
      "In this section we present several experiments that demonstrate the effectiveness of the distributed nearest\n",
      " neighbor matching framework in FLANN. For these experiments we have used the 80 million patch data set of \n",
      "[7].\n",
      "In an MPI distributed system it’s possible to run multiple parallel processes on the same machine, the\n",
      " recommended approach is to run as many processes as CPU cores on the machine. Fig. 14\n",
      " presents the results of an experiment in which we run multiple MPI processes on a single machine with eight CPU\n",
      " cores. It can be seen that the overall performance improves when increasing the number of processes from 1 to 4,\n",
      " however there is a decrease in performance when moving from four to eight parallel processes. This can be explained by\n",
      " the fact that increasing the parallelism on the same machine also increases the number of requests to the main memory\n",
      " (since all processes share the same main memory), and at some point the bottleneck moves from the CPU to the memory.\n",
      " Increasing the parallelism past this point results in decreased performance. Fig. 14\n",
      " also shows the direct search performance obtained by using FLANN directly without the MPI layer. As expected,\n",
      " the direct search performance is identical to the performance obtained when using the MPI layer with a single process,\n",
      " showing no significant overhead from the MPI runtime. For this experiment and the one in \n",
      "Fig. 15 we used a subset of only 8 million tiny images to be able to run the\n",
      " experiment on a single machine.\n",
      "\n",
      "Distributing nearest neighbor search on a single multi-core machine. When the degree of parallelism\n",
      " increases beyond a certain point the memory access becomes a bottleneck. The “direct search” case\n",
      " corresponds to using the FLANN library directly, without the MPI layer.\n",
      "Show All\n",
      "\n",
      "\n",
      "The advantage of distributing the search to multiple machines. Even when using the same number of parallel\n",
      " processes, distributing the computation to multiple machines still leads to an improvement in performance due to less\n",
      " memory access overhead. “Direct search” corresponds to using FLANN without the MPI layer and is provided\n",
      " as a comparison baseline.\n",
      "Show All\n",
      "Fig. 15 shows the performance obtained by using eight parallel processes on\n",
      " one, two or three machines. Even though the same number of parallel processes are used, it can be seen that the\n",
      " performance increases when those processes are distributed on more machines. This can also be explained by the memory\n",
      " access overhead, since when more machines are used, fewer processes are running on each machine, requiring fewer\n",
      " memory accesses.\n",
      "Fig. 16 shows the search speedup for the data set of 80 million tiny images\n",
      " of [7]. The algorithm used is the radomized k-d tree forest as it was\n",
      " determined by the auto-tuning procedure to be the most efficient in this case. It can be seen that the search\n",
      " performance scales well with the data set size and it benefits from using multiple parallel processes.\n",
      "\n",
      "Matching 80 million tiny images directly using a compute cluster.\n",
      "Show All\n",
      "All the previous experiments have shown that distributing the nearest neighbor search to multiple machines results\n",
      " in an overall increase in performance in addition to the advantage of being able to use more memory. Ideally, when\n",
      " distributing the search to N machines the speedup would be \n",
      "N times higher, however in practice for approximate nearest neighbor search the speedup\n",
      " is smaller due to the fact that the search on each of the machines has sub-linear complexity in the size of the input\n",
      " data set.\n",
      "The work presented in this paper has been made publicly available as an open source library named Fast Library for\n",
      " Approximate Nearest Neighbors [59].\n",
      "FLANN is used in a large number of both research and industry projects (e.g., [60]\n",
      "[61][62]\n",
      "[63][64]) and is widely used in the\n",
      " computer vision community, in part due to its inclusion in OpenCV [65], the\n",
      " popular open source computer vision library. FLANN also is used by other well known open source projects, such as the\n",
      " point cloud library (PCL) and the robot operating system (ROS) [63]. FLANN\n",
      " has been packaged by most of the mainstream Linux distributions such as Debian, Ubuntu, Fedora, Arch, Gentoo and their\n",
      " derivatives.\n",
      "This paper addresses the problem of fast nearest neighbor search in high dimensional spaces, a core problem in many\n",
      " computer vision and machine learning algorithms and which is often the most computationally expensive part of these\n",
      " algorithms. We present and compare the algorithms we have found to work best at fast approximate search in high\n",
      " dimensional spaces: the randomized k-d trees and a newly introduced algorithm, the priority search k-means tree. We\n",
      " introduce a new algorithm for fast approximate matching of binary features. We address the issues arising when scaling\n",
      " to very large size data sets by proposing an algorithm for distributed nearest neighbor matching on compute clusters.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "paragraphs = article.find_all(\"p\")\n",
    "fullText = \"\"\n",
    "for paragraph in paragraphs:\n",
    "    fullText += paragraph.text\n",
    "    fullText += \"\\n\"\n",
    "print(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('nearest neighbor', 0.0007790693876760918)\n",
      "('neighbor search', 0.0009564815374257956)\n",
      "('search', 0.0011065993981677696)\n",
      "('data set', 0.0013219285588632105)\n",
      "('data', 0.0016762870619856539)\n",
      "('data sets', 0.0017057142695009167)\n",
      "('tree', 0.002433507022212509)\n",
      "('neighbor', 0.0028228769214348266)\n",
      "('nearest', 0.0030018171655685406)\n",
      "('k-means tree', 0.003034431236075598)\n",
      "('search k-means', 0.003075709743221117)\n",
      "('priority search', 0.003869227824687117)\n",
      "('algorithm', 0.00414042072405418)\n",
      "('set', 0.005181598982140965)\n",
      "('search time', 0.005978145884255258)\n",
      "('search performance', 0.006266237672459638)\n",
      "('trees', 0.0065940190279306695)\n",
      "('algorithms', 0.007245736267094815)\n",
      "('neighbors', 0.007527671790492871)\n",
      "('search algorithms', 0.007894556999966156)\n",
      "('sets', 0.007934323441403353)\n",
      "('theta', 0.008007684964877089)\n",
      "('neighbor algorithm', 0.008242710235967616)\n",
      "('approximate nearest', 0.008695894784643315)\n",
      "('k-means', 0.009035377409674868)\n",
      "('approximate search', 0.009429289951840407)\n",
      "('performance', 0.009603175010955593)\n",
      "('time', 0.010182304248353294)\n",
      "('points', 0.010860777469724697)\n",
      "('Fig.', 0.011457219199583757)\n",
      "('data points', 0.01148136711986525)\n",
      "('large data', 0.011872742179521648)\n",
      "('number', 0.012615459680533956)\n",
      "('LSH', 0.012657929305763385)\n",
      "('randomized', 0.013070474542661149)\n",
      "('View Source', 0.013688013642601263)\n",
      "('approximate', 0.013815647596909725)\n",
      "('point', 0.014166231482249603)\n",
      "('K-nearest neighbor', 0.014747677295027071)\n",
      "('neighbor matching', 0.01481041628790274)\n",
      "('query point', 0.015304693609007313)\n",
      "('SIFT features', 0.01580629476442623)\n",
      "('memory', 0.015857419458274083)\n",
      "('show', 0.015927032766949577)\n",
      "('priority', 0.016387238711177452)\n",
      "('tree algorithm', 0.01658482408689581)\n",
      "('source', 0.016772193567379974)\n",
      "('tree build', 0.017247158286281258)\n",
      "('FLANN', 0.01735380707794761)\n",
      "('query', 0.01763739043798271)\n"
     ]
    }
   ],
   "source": [
    "#Keyword search and analysis\n",
    "\n",
    "w_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50 #alunperin 10\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(fullText)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "Write down a simple python script that allows you to output the histogram of word frequency in the document, excluding the stopwords (see examples in online NLTK book). Use SpaCy named-entity tagger to identify person-named entities and organization-named entities in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Calculate the word frequencies and plot the histogram\n",
    "\n",
    "#Purkkapallolla koko artikkeli -> string\n",
    "article_readable = \"{}\\n\".format(headline)\n",
    "\n",
    "#article_readable += (article.find('div', class_=\"article-content\").h1.text) + \" \"\n",
    "article_readable += \"{}\\n\".format(abstract)\n",
    "\n",
    "for sectionTitle in sectionTitles:\n",
    "    article_readable += \"{} \".format(sectionTitle.text)\n",
    "\n",
    "for subsectionTitle in subsectionTitles:\n",
    "    article_readable += \"{} \".format(subsectionTitle.text)\n",
    "    \n",
    "article_readable += \"\\n{}\".format(fullText)\n",
    "    \n",
    "#print(article_readable)\n",
    "#Purkkapallo suoritettu\n",
    "\n",
    "stopwords = stopwords.words(\"english\")\n",
    "#print(stopwords)\n",
    "\n",
    "#Remove stopwords from the keywords list\n",
    "keywords_only, keywords_nums = zip(*keywords) \n",
    "keywords_no_sw =  [word for word in keywords_only if not word in stopwords]\n",
    "keywords_counts = []\n",
    "\n",
    "for kw in keywords_no_sw:\n",
    "    kw_count = article_readable.count(kw)\n",
    "    keywords_counts.append(kw_count)\n",
    "\n",
    "#print(keywords_no_sw)\n",
    "#print(keywords_counts)\n",
    "    \n",
    "#Plotting the histogram\n",
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "plt.bar(np.arange(len(keywords_no_sw)), keywords_counts, align='center')\n",
    "plt.ylabel('Keyword count')\n",
    "plt.xlabel('Keyword id')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('search', 152)\n",
      "('data', 124)\n",
      "('nearest', 90)\n",
      "('tree', 86)\n",
      "('neighbor', 82)\n",
      "('algorithm', 65)\n",
      "('using', 58)\n",
      "('set', 51)\n",
      "('k-means', 43)\n",
      "('performance', 42)\n",
      "('algorithms', 40)\n",
      "('time', 36)\n",
      "('trees', 35)\n",
      "('sets', 34)\n",
      "('randomized', 34)\n",
      "('used', 34)\n",
      "('approximate', 33)\n",
      "('number', 33)\n",
      "('memory', 31)\n",
      "('priority', 30)\n",
      "('points', 30)\n",
      "('clustering', 28)\n",
      "('show', 28)\n",
      "('neighbors', 28)\n",
      "('features', 27)\n",
      "('query', 27)\n",
      "('k-d', 26)\n",
      "('matching', 25)\n",
      "('large', 24)\n",
      "('fig', 24)\n",
      "('multiple', 23)\n",
      "('point', 23)\n",
      "('case', 22)\n",
      "('hierarchical', 21)\n",
      "('many', 20)\n",
      "('cluster', 20)\n",
      "('k', 19)\n",
      "('different', 19)\n",
      "('one', 18)\n",
      "('dimensionality', 18)\n",
      "('parameter', 18)\n",
      "('use', 18)\n",
      "('lsh', 18)\n",
      "('hashing', 17)\n",
      "('speedup', 17)\n",
      "('also', 16)\n",
      "('distributed', 16)\n",
      "('source', 16)\n",
      "('flann', 16)\n",
      "('patches', 16)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Count histogram from every word manually ###\n",
    "charsToRemove = \".,():\"\n",
    "for char in charsToRemove:\n",
    "    article_readable = article_readable.replace(char, '')\n",
    "\n",
    "article_readable = article_readable.lower()\n",
    "wordList = article_readable.split()\n",
    "wordList = [word for word in wordList if word not in stopwords]\n",
    "wordCounts = dict()\n",
    "\n",
    "for word in wordList:\n",
    "    if word in wordCounts:\n",
    "        wordCounts[word] += 1\n",
    "    else:\n",
    "        wordCounts[word] = 1\n",
    "\n",
    "wordCounts = sorted(wordCounts.items(), key=lambda x: x[1], reverse=True)\n",
    "for i in range(50):\n",
    "    print(\"{}\".format(wordCounts[i]))\n",
    "wordCounts = wordCounts[0:20]\n",
    "\n",
    "words = list(zip(*wordCounts))[0]\n",
    "occurency = list(zip(*wordCounts))[1]\n",
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "plt.bar(np.arange(len(words)), occurency, align='center')\n",
    "plt.xticks(np.arange(len(words)), words)\n",
    "plt.ylabel('Keyword count')\n",
    "plt.xlabel('Keyword id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SpaCy to identify person-named entities and organization-named entities\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "\n",
    "#vinkkiä https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "\n",
    "#Identifying person and organization-named entities\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(article_readable)\n",
    "\n",
    "for X in doc.ents:\n",
    "    print(\"{} - {}\".format(X.text, X.label_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3\n",
    "\n",
    "We would like the summarizer to contain frequent wording (excluding stopwords) and as many named-entities as possible. For this purpose, use the following heuristic to construct the summarizer. First we shall assume each sentence of the document as individual sub-document. Use TfIdf vectorizer to output the individual tfidef score of each word of each sentence (after initial preprocessing and wordnet lemmatization stage). Then consider only sentences that contain person or organization named-entities and use similar approach to output the tfidf score of the named-entities in each sentence. Finally construct the sentence (S) weight as a  weighted sum:\n",
    "<br>\n",
    "$$S_{weight}=\\sum_{w\\varepsilon S}W_{TfiDf}+2\\sum_{NM\\varepsilon S}NM_{TfiDf}+POS_s$$\n",
    "<br>\n",
    "where NMTfiDF stands for the TfIdF of named-entity NM in sentence S.  POSS corresponds to the sentence weight associated to the location of the sentence. So that the sentence location weight will be maximum (1) if located in the title of the document, 0.5 if located  in the title of one of the subsection, 0.25 if located in the title one of the subsubsection, 0.1 if located in one representative object of the document, and 0 if located only in the main text. Make sure to normalize the term tfidf and Nm tfidf weights and suggest a script to implement the preceding accordingly, so that the summarizer will contain the 10 sentences with the highest Sweight scores.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TASK 4\n",
    "Test the above approach with Opinosis dataset available at https://kavita-ganesan.com/opinosis-opinion-dataset/#.YVw6J5ozY2x,  and record the corresponding Rouge-2 and Rouge-3 evaluation score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5\n",
    "\n",
    "We would like to improve the summarization by taking into account the diversity among the sentence in the sense that we would like to minimize redundancy among sentences. For this purpose, we shall use the sentence-to-sentence semantic similarity introduced in the NLP lab. Next, instead of recording only the 10 sentences with highest Sweight scores, we shall record the 20 top sentences in terms of $S_{weight}$ scores. Then the selection of the top 10 sentences among the 20 sentences follows the following approach. First, order the 20 sentences in the decreasing order of their $S_{weight}$ scores, say S1, S2, …, S20 (where S1 is the top ranked and S20 the 20th ranked sentence). Second, we shall assume that S1 is always included in the summarizer, we shall then attempt to find the other sentences among S2 till S20 to be included into the summarizer. Calculate the sentence-to-sentence similarity Sim(S1,Si) for i=1 to 20, the Sentence Sj that yields the minimum similarity with S1 will therefore be included in the summarizer. Next, for each of the remaining sentences Sk (with k different from 1 and j), we calculate the sentence similarity with Sj. Therefore the sentence Sp that yields minimum value of “Sim(Sp, S1)+Sim(Sp,Sj)” will be included in the summarizer (Note: the quantity Sim(Sp, S1) is already calculated in previous step).  Similarly in the next phase, we should select a sentence Sl (l different from 1, j and k) so that  “Sim(Sl, S1)+Sim(Sl,Sj)+Sim(Sl,Sp)”, Etc.. You then stop once you reached 10 sentences included in the summarizer. Suggest a script that includes this process.. and illustrate its functioning in the example you chosen in 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6\n",
    "\n",
    "We would like to make the choice of keywords not based on histogram frequency but using the open source RAKE https://www.airpair.com/nlp/keyword-extraction-tutorial. Repeat the previous process of selecting the sentences that are associated to the ten first keywords generated by RAKE. Comment on the quality of this summarizer based on your observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7\n",
    "\n",
    "It is also suggested to explore alternative implementations with larger number of summarization approaches implemented- https://github.com/miso-belica/sumy. Show how each of the implemented summarizer behaves when inputted with the same document you used in previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 8\n",
    "\n",
    "Now we would like to compare the above summarizers and those in 3), 5) and 7) on a new dataset constructed as follows. First select an Elsevier journal of your own and select 10 papers highly ranked in the journal according to citation index (The journal papers should be well structured to contain Abstract, Introduction and Conclusion). For each of the ten papers, consider the introduction as the main document to seek to apply summarizer, and consider the Abstract and Conclusion as two golden summary of the document that you can use for assessment using ROUGE-1 and ROUGE-2 evaluation. Report in a table the evaluation score of each summarizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 9\n",
    "\n",
    "Design a simple GUI that allows the user to input a text or a link to a document to be summarized and output the summarizer according to 3), algorithms implemented in 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
