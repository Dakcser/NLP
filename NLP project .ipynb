{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 21: Automatic Summarization  \n",
    "\n",
    "We shall consider structured document containing a title, abstract and a set of subsections. We would like to build a text summarizer such that tracks important keywords in the document. For this purpose, the first step is identify these keywords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\markus\\anaconda3\\lib\\site-packages (21.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- -------------------\n",
      "absl-py                            0.9.0\n",
      "alabaster                          0.7.12\n",
      "anaconda-client                    1.7.2\n",
      "anaconda-navigator                 1.9.12\n",
      "anaconda-project                   0.8.3\n",
      "argh                               0.26.2\n",
      "asn1crypto                         1.3.0\n",
      "astroid                            2.3.3\n",
      "astropy                            4.0\n",
      "astunparse                         1.6.3\n",
      "async-generator                    1.10\n",
      "atomicwrites                       1.3.0\n",
      "attrs                              19.3.0\n",
      "autopep8                           1.4.4\n",
      "Babel                              2.8.0\n",
      "backcall                           0.1.0\n",
      "backports.functools-lru-cache      1.6.1\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "backports.tempfile                 1.0\n",
      "backports.weakref                  1.0.post1\n",
      "bcrypt                             3.1.7\n",
      "beautifulsoup4                     4.8.2\n",
      "bitarray                           1.2.1\n",
      "bkcharts                           0.2\n",
      "bleach                             3.1.0\n",
      "bokeh                              1.4.0\n",
      "boto                               2.49.0\n",
      "Bottleneck                         1.3.2\n",
      "cachetools                         4.1.1\n",
      "certifi                            2020.6.20\n",
      "cffi                               1.14.0\n",
      "chardet                            3.0.4\n",
      "Click                              7.0\n",
      "cloudpickle                        1.3.0\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.3\n",
      "comtypes                           1.1.7\n",
      "conda                              4.8.4\n",
      "conda-build                        3.18.11\n",
      "conda-package-handling             1.6.0\n",
      "conda-verify                       3.4.2\n",
      "contextlib2                        0.6.0.post1\n",
      "cryptography                       2.8\n",
      "cycler                             0.10.0\n",
      "Cython                             0.29.15\n",
      "cytoolz                            0.10.1\n",
      "dask                               2.11.0\n",
      "decorator                          4.4.1\n",
      "defusedxml                         0.6.0\n",
      "diff-match-patch                   20181111\n",
      "distributed                        2.11.0\n",
      "docutils                           0.16\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.0.1\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.0.12\n",
      "flake8                             3.7.9\n",
      "Flask                              1.1.1\n",
      "fsspec                             0.6.2\n",
      "future                             0.18.2\n",
      "gast                               0.3.3\n",
      "gevent                             1.4.0\n",
      "glob2                              0.7\n",
      "google-auth                        1.18.0\n",
      "google-auth-oauthlib               0.4.1\n",
      "google-pasta                       0.2.0\n",
      "greenlet                           0.4.15\n",
      "grpcio                             1.30.0\n",
      "h11                                0.12.0\n",
      "h5py                               2.10.0\n",
      "HeapDict                           1.0.1\n",
      "html5lib                           1.0.1\n",
      "hypothesis                         5.5.4\n",
      "idna                               2.8\n",
      "imageio                            2.6.1\n",
      "imagesize                          1.2.0\n",
      "importlib-metadata                 1.5.0\n",
      "intervaltree                       3.0.2\n",
      "ipykernel                          5.1.4\n",
      "ipython                            7.12.0\n",
      "ipython_genutils                   0.2.0\n",
      "ipywidgets                         7.5.1\n",
      "isort                              4.3.21\n",
      "itsdangerous                       1.1.0\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.14.1\n",
      "Jinja2                             2.11.1\n",
      "joblib                             0.14.1\n",
      "json5                              0.9.1\n",
      "jsonschema                         3.2.0\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     5.3.4\n",
      "jupyter-console                    6.1.0\n",
      "jupyter-core                       4.6.1\n",
      "jupyterlab                         1.2.6\n",
      "jupyterlab-server                  1.0.6\n",
      "Keras-Preprocessing                1.1.2\n",
      "keyring                            21.1.0\n",
      "kiwisolver                         1.1.0\n",
      "lazy-object-proxy                  1.4.3\n",
      "libarchive-c                       2.8\n",
      "llvmlite                           0.31.0\n",
      "locket                             0.2.0\n",
      "lxml                               4.5.0\n",
      "Markdown                           3.2.2\n",
      "MarkupSafe                         1.1.1\n",
      "matplotlib                         3.1.3\n",
      "mccabe                             0.6.1\n",
      "menuinst                           1.4.16\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.0.15\n",
      "mkl-random                         1.1.0\n",
      "mkl-service                        2.3.0\n",
      "mock                               4.0.1\n",
      "more-itertools                     8.2.0\n",
      "mpmath                             1.1.0\n",
      "msgpack                            0.6.1\n",
      "multipledispatch                   0.6.0\n",
      "mysql-connector-python             8.0.21\n",
      "navigator-updater                  0.2.1\n",
      "nbconvert                          5.6.1\n",
      "nbformat                           5.0.4\n",
      "networkx                           2.4\n",
      "nltk                               3.4.5\n",
      "nose                               1.3.7\n",
      "notebook                           6.0.3\n",
      "numba                              0.48.0\n",
      "numexpr                            2.7.1\n",
      "numpy                              1.18.1\n",
      "numpydoc                           0.9.2\n",
      "oauthlib                           3.1.0\n",
      "olefile                            0.46\n",
      "openpyxl                           3.0.3\n",
      "opt-einsum                         3.2.1\n",
      "outcome                            1.1.0\n",
      "packaging                          20.1\n",
      "pandas                             1.0.1\n",
      "pandocfilters                      1.4.2\n",
      "paramiko                           2.7.1\n",
      "parso                              0.5.2\n",
      "partd                              1.1.0\n",
      "path                               13.1.0\n",
      "pathlib2                           2.3.5\n",
      "pathtools                          0.1.2\n",
      "patsy                              0.5.1\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.8.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             7.0.0\n",
      "pip                                21.3\n",
      "pkginfo                            1.5.0.1\n",
      "plotly                             4.9.0\n",
      "pluggy                             0.13.1\n",
      "ply                                3.11\n",
      "prometheus-client                  0.7.1\n",
      "prompt-toolkit                     3.0.3\n",
      "protobuf                           3.12.2\n",
      "psutil                             5.6.7\n",
      "py                                 1.8.1\n",
      "pyasn1                             0.4.8\n",
      "pyasn1-modules                     0.2.8\n",
      "pycodestyle                        2.5.0\n",
      "pycosat                            0.6.3\n",
      "pycparser                          2.19\n",
      "pycrypto                           2.6.1\n",
      "pycurl                             7.43.0.5\n",
      "pydocstyle                         4.0.1\n",
      "pyflakes                           2.1.1\n",
      "Pygments                           2.5.2\n",
      "pylint                             2.4.4\n",
      "PyNaCl                             1.3.0\n",
      "pyodbc                             4.0.0-unsupported\n",
      "pyOpenSSL                          19.1.0\n",
      "pyparsing                          2.4.6\n",
      "PyQt5                              5.12.3\n",
      "PyQt5_sip                          4.19.18\n",
      "PyQtWebEngine                      5.12.1\n",
      "pyreadline                         2.1\n",
      "pyrsistent                         0.15.7\n",
      "PySocks                            1.7.1\n",
      "pytest                             5.3.5\n",
      "pytest-arraydiff                   0.3\n",
      "pytest-astropy                     0.8.0\n",
      "pytest-astropy-header              0.1.2\n",
      "pytest-doctestplus                 0.5.0\n",
      "pytest-openfiles                   0.4.0\n",
      "pytest-remotedata                  0.3.2\n",
      "python-dateutil                    2.8.1\n",
      "python-jsonrpc-server              0.3.4\n",
      "python-language-server             0.31.7\n",
      "pytz                               2019.3\n",
      "PyWavelets                         1.1.1\n",
      "pywin32                            227\n",
      "pywin32-ctypes                     0.2.0\n",
      "pywinpty                           0.5.7\n",
      "PyYAML                             5.3\n",
      "pyzmq                              18.1.1\n",
      "QDarkStyle                         2.8\n",
      "QtAwesome                          0.6.1\n",
      "qtconsole                          4.6.0\n",
      "QtPy                               1.9.0\n",
      "requests                           2.22.0\n",
      "requests-oauthlib                  1.3.0\n",
      "retrying                           1.3.3\n",
      "rope                               0.16.0\n",
      "rsa                                4.6\n",
      "Rtree                              0.9.3\n",
      "ruamel_yaml                        0.15.87\n",
      "scikit-image                       0.16.2\n",
      "scikit-learn                       0.22.1\n",
      "scipy                              1.4.1\n",
      "seaborn                            0.10.0\n",
      "selenium                           4.0.0\n",
      "Send2Trash                         1.5.0\n",
      "setuptools                         45.2.0.post20200210\n",
      "simplegeneric                      0.8.1\n",
      "singledispatch                     3.4.0.3\n",
      "six                                1.14.0\n",
      "sniffio                            1.2.0\n",
      "snowballstemmer                    2.0.0\n",
      "sortedcollections                  1.1.2\n",
      "sortedcontainers                   2.1.0\n",
      "soupsieve                          1.9.5\n",
      "Sphinx                             2.4.0\n",
      "sphinxcontrib-applehelp            1.0.1\n",
      "sphinxcontrib-devhelp              1.0.1\n",
      "sphinxcontrib-htmlhelp             1.0.2\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.2\n",
      "sphinxcontrib-serializinghtml      1.1.3\n",
      "sphinxcontrib-websupport           1.2.0\n",
      "spyder                             4.0.1\n",
      "spyder-kernels                     1.8.1\n",
      "SQLAlchemy                         1.3.13\n",
      "statsmodels                        0.11.0\n",
      "sympy                              1.5.1\n",
      "tables                             3.6.1\n",
      "tblib                              1.6.0\n",
      "tensorboard                        2.2.2\n",
      "tensorboard-plugin-wit             1.7.0\n",
      "tensorflow                         2.2.0\n",
      "tensorflow-estimator               2.2.0\n",
      "termcolor                          1.1.0\n",
      "terminado                          0.8.3\n",
      "testpath                           0.4.4\n",
      "toolz                              0.10.0\n",
      "tornado                            6.0.3\n",
      "tqdm                               4.42.1\n",
      "traitlets                          4.3.3\n",
      "trio                               0.19.0\n",
      "trio-websocket                     0.9.2\n",
      "ujson                              1.35\n",
      "umap                               0.1.1\n",
      "umap-learn                         0.4.6\n",
      "unicodecsv                         0.14.1\n",
      "urllib3                            1.26.7\n",
      "watchdog                           0.10.2\n",
      "wcwidth                            0.1.8\n",
      "webencodings                       0.5.1\n",
      "Werkzeug                           1.0.0\n",
      "wheel                              0.34.2\n",
      "widgetsnbextension                 3.5.1\n",
      "win-inet-pton                      1.1.0\n",
      "win-unicode-console                0.5\n",
      "wincertstore                       0.2\n",
      "wrapt                              1.11.2\n",
      "wsproto                            1.0.0\n",
      "xlrd                               1.2.0\n",
      "XlsxWriter                         1.2.7\n",
      "xlwings                            0.17.1\n",
      "xlwt                               1.3.0\n",
      "xmltodict                          0.12.0\n",
      "yapf                               0.28.0\n",
      "zict                               1.0.0\n",
      "zipp                               2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip list\n",
    "# tarkista löytyykö: lxml, html5lib, requests, selenium, webdriver-manager\n",
    "# lisäohjeita task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Markus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jos nltk ei löydy asenna -> ! pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#from nltk.cluster.util import cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "Assume the initial input is given as html document (choose an example of your own), we hypothesize that important keywords are initially contained in the words of titles, abstract and possibly titles of subsections of the document. Suggest a simple python script that inputs an html document and outputs the lists of words in the title, abstract and title of section/subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 94.0.4606\n",
      "[WDM] - Get LATEST driver version for 94.0.4606\n",
      "[WDM] - Driver [C:\\Users\\Markus\\.wdm\\drivers\\chromedriver\\win32\\94.0.4606.61\\chromedriver.exe] found in cache\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline: Scalable Nearest Neighbor Algorithms for High Dimensional Data\n",
      "\n",
      " Abstract:For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.\n",
      "\n",
      "\n",
      "Section titles:\n",
      "1.1 Definitions and Notation\n",
      "2.1 Nearest Neighbor Matching Algorithms\n",
      "2.2 Automatic Configuration of NN Algorithms\n",
      "3.1 The Randomized k-d Tree Algorithm\n",
      "3.2 The Priority Search K-Means Tree Algorithm\n",
      "3.3 The Hierarchical Clustering Tree\n",
      "3.4 Automatic Selection of the Optimal Algorithm\n",
      "4.1 Fast Approximate Nearest Neighbor Search\n",
      "4.2 Binary Features\n",
      "5.1 Searching on a Compute Cluster\n",
      "5.2 Evaluation of Distributed Search\n",
      "\n",
      "Subsection titles:\n",
      "2.1.1 Partitioning Trees\n",
      "2.1.2 Hashing Based Nearest Neighbor Techniques\n",
      "2.1.3 Nearest Neighbor Graph Techniques\n",
      "3.2.1 Algorithm Description\n",
      "3.2.2 Analysis\n",
      "4.1.1 Data Dimensionality\n",
      "4.1.2 Search Precision\n",
      "4.1.3 Automatic Selection of Optimal Algorithm\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "# Kaikki sivut ei anna koko html bodyä käyttämällä pelkkää requestia. Seleniumilla näyttää toimivan useammilla. \n",
    "# pip install -U selenium\n",
    "# pip install webdriver-manager\n",
    "# jos käytät anacondaa eikä meinaa toimia niin kokeile myös $ conda update pip\n",
    "\n",
    "#Collect title, subtitles and abstract from html file\n",
    "\n",
    "url = \"https://ieeexplore.ieee.org/document/6809191\"\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "#print(soup.prettify())\n",
    "\n",
    "# Headline\n",
    "headline = soup.find(\"h1\", {\"class\": \"document-title\"}).text\n",
    "print(\"Headline: {}\\n\".format(headline))\n",
    "\n",
    "# Abstact\n",
    "abstract = soup.find(\"div\", {\"class\": \"abstract-text\"}).text\n",
    "print(\"{}\\n\\n\".format(abstract))\n",
    "\n",
    "# Titles of sections\n",
    "article = soup.find(\"div\", {\"id\": \"article\"})\n",
    "sectionTitles = article.find_all(\"h3\")\n",
    "print(\"Section titles:\")\n",
    "for title in sectionTitles:\n",
    "    print(\"{}\".format(title.text))\n",
    "\n",
    "# Titles of subsections\n",
    "subsectionTitles = article.find_all(\"h4\")\n",
    "print(\"\\nSubsection titles:\")\n",
    "for title in subsectionTitles:\n",
    "    print(\"{}\".format(title.text))\n",
    "\n",
    "#driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Suomea viikoilla', 0.023458380875189744)\n",
      "('sää Koulujen', 0.04498862876540802)\n",
      "('Koulujen syyslomia', 0.04498862876540802)\n",
      "('puolilla Suomea', 0.04498862876540802)\n",
      "('tältä näyttää', 0.04940384002065631)\n",
      "('viilenee kovaa', 0.04940384002065631)\n",
      "('Saattaa', 0.08596317751626563)\n",
      "('Lapissa', 0.08596317751626563)\n",
      "('näyttää syyslomaviikkojen', 0.09700399286574239)\n",
      "('syyslomaviikkojen sää', 0.09700399286574239)\n",
      "('syyslomia vietetään', 0.09700399286574239)\n",
      "('kovaa vauhtia.Tilastojen', 0.09700399286574239)\n",
      "('kuun loppupuolella', 0.09700399286574239)\n",
      "('ehjä väliaikainen', 0.09700399286574239)\n",
      "('väliaikainen lumipeite', 0.09700399286574239)\n",
      "('Koulujen', 0.1447773057422032)\n",
      "('Suomea', 0.1447773057422032)\n",
      "('hiutaleita', 0.15831692877998726)\n",
      "('tältä', 0.15831692877998726)\n",
      "('viikoilla', 0.15831692877998726)\n",
      "('viilenee', 0.15831692877998726)\n",
      "('näyttää', 0.29736558256021506)\n",
      "('syyslomaviikkojen', 0.29736558256021506)\n",
      "('sää', 0.29736558256021506)\n",
      "('syyslomia', 0.29736558256021506)\n",
      "('vietetään', 0.29736558256021506)\n",
      "('puolilla', 0.29736558256021506)\n",
      "('kovaa', 0.29736558256021506)\n",
      "('vauhtia.Tilastojen', 0.29736558256021506)\n",
      "('kuun', 0.29736558256021506)\n",
      "('loppupuolella', 0.29736558256021506)\n",
      "('saada', 0.29736558256021506)\n",
      "('ehjä', 0.29736558256021506)\n",
      "('väliaikainen', 0.29736558256021506)\n",
      "('lumipeite', 0.29736558256021506)\n"
     ]
    }
   ],
   "source": [
    "#Keyword search and analysis\n",
    "\n",
    "w_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"fi\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50 #alunperin 10\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(text)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "Write down a simple python script that allows you to output the histogram of word frequency in the document, excluding the stopwords (see examples in online NLTK book). Use SpaCy named-entity tagger to identify person-named entities and organization-named entities in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#Calculate the word frequencies and plot the histogram\n",
    "\n",
    "#Purkkapallolla koko artikkeli -> string\n",
    "article_readable = headline\n",
    "\n",
    "article_readable += (article.find('div', class_=\"article-content\").h1.text) + \" \"\n",
    "\n",
    "subheadlines = article.find_all(\"h3\", {'class':'subheadline'})\n",
    "for subheadline in subheadlines:\n",
    "    article_readable += subheadline.text\n",
    "\n",
    "article_readable += article_readable + \" \"\n",
    "    \n",
    "summary = article.find_all(\"div\", {'class':'article-bullets'})\n",
    "for bullet in summary:\n",
    "    article_readable += \"{}\\n\".format(bullet.text)\n",
    "    \n",
    "text = article.find_all('p', {'class':'paragraph'})\n",
    "for line in text:\n",
    "    article_readable += line.text\n",
    "    \n",
    "#print(article_readable)\n",
    "#Purkkapallo suoritettu\n",
    "\n",
    "stopwords = stopwords.words(\"finnish\")\n",
    "#print(stopwords)\n",
    "\n",
    "#Remove stopwords from the keywords list\n",
    "keywords_only, keywords_nums = zip(*keywords) \n",
    "keywords_no_sw =  [word for word in keywords_only if not word in stopwords]\n",
    "keywords_counts = []\n",
    "\n",
    "for kw in keywords_no_sw:\n",
    "    kw_count = article_readable.count(kw)\n",
    "    keywords_counts.append(kw_count)\n",
    "\n",
    "#print(keywords_no_sw)\n",
    "#print(keywords_counts)\n",
    "    \n",
    "#Plotting the histogram\n",
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "plt.bar(np.arange(len(keywords_no_sw)), keywords_counts, align='center')\n",
    "plt.ylabel('Keyword count')\n",
    "plt.xlabel('Keyword id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koulujen - GPE\n",
      "41 - CARDINAL\n",
      "42 - DATE\n",
      "43.Sää - CARDINAL\n",
      "kuun loppupuolella voidaan saada jo - PERSON\n",
      "Lapissa - PERSON\n",
      "Koulujen - GPE\n",
      "viikon ajan - PERSON\n",
      "Suomea - ORG\n",
      "minkä jälkeen muualla - PERSON\n",
      "42 - CARDINAL\n",
      "jokaiselle - GPE\n",
      "kolmelle viikolle - PERSON\n",
      "hieman sateita - PERSON\n",
      "kehittymistä ja ennusteita - ORG\n",
      "Koko - PERSON\n",
      "melko pilvinen ja sateinen - PERSON\n",
      "kertoo Forecan - PERSON\n",
      "Joanna Rinne - PERSON\n",
      "viikon aikana - PERSON\n",
      "Lapissa - PERSON\n",
      "jo lähelle viittä - ORG\n",
      "kun - CARDINAL\n",
      "tällä kuluvalla viikolla - ORG\n",
      "Yölämpötilat - GPE\n",
      "jo pakkasen puolella - PERSON\n",
      "Rinne kertoo - PERSON\n",
      "Hajanaisia - GPE\n",
      "viikon ajan - PERSON\n",
      "Rinne - PERSON\n",
      "viikon kohdalla ennusteissa siirrytään - PERSON\n",
      "Rinne - PERSON\n",
      "Viikolla 42 - ORG\n",
      "vietetään eri puolilla maata - ORG\n",
      "esimerkiksi pääkaupunkiseudulla - GPE\n",
      "Rovaniemellä - PERSON\n",
      "lähellä keskiarvoa - ORG\n",
      "kuin keskimääräisesti - PERSON\n",
      "Rinne - PERSON\n",
      "Hyvin - PERSON\n",
      "Lokakuussa Suomen sää - PERSON\n",
      "lähellä - ORG\n",
      "ehkä pikkaisen - ORG\n",
      "sateisempaa kuin vuodenaikaan keskimäärin - PERSON\n",
      "Rinne kertoo - PERSON\n",
      "10 - CARDINAL\n",
      "kuun - PERSON\n",
      "8–10 - CARDINAL\n",
      "ja kuun - PERSON\n",
      "4–8 - DATE\n",
      "Maan keskivaiheilla - PERSON\n",
      "Oulun - NORP\n",
      "ja kuun lopussa keskimäärin - PERSON\n",
      "Lapin - PERSON\n",
      "Lapissa - PERSON\n",
      "0–4 - CARDINAL\n",
      "ja kuun - PERSON\n",
      "nollan ja - PERSON\n",
      "koko ajan lisääntyy - PERSON\n",
      "Rinne - PERSON\n",
      "Kuun - PERSON\n",
      "jo maan keskivaiheilla tulla ajoittain lumisateita - ORG\n",
      "Rinne kertoo - PERSON\n",
      "Mahdollinen - PERSON\n",
      "vielä maahan jää - PERSON\n",
      "Pohjois-Lappiin - WORK_OF_ART\n",
      "eli sellainen - ORG\n",
      "jonkin aikaa - PERSON\n",
      "ja Keski-Lapissa - PERSON\n",
      "Pohjois-Pohjanmaalla - ORG\n",
      "Kainuussa ja - PERSON\n",
      "Pohjois-Karjalaa - PERSON\n",
      "vielä kuun - PERSON\n",
      "vaikea - ORG\n",
      "Kyllä - ORG\n",
      "Suomea - ORG\n",
      "ja - NORP\n"
     ]
    }
   ],
   "source": [
    "#Use SpaCy to identify person-named entities and organization-named entities\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "\n",
    "#vinkkiä https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "\n",
    "#Identifying person and organization-named entities\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(article_readable)\n",
    "\n",
    "for X in doc.ents:\n",
    "    print(\"{} - {}\".format(X.text, X.label_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3\n",
    "\n",
    "We would like the summarizer to contain frequent wording (excluding stopwords) and as many named-entities as possible. For this purpose, use the following heuristic to construct the summarizer. First we shall assume each sentence of the document as individual sub-document. Use TfIdf vectorizer to output the individual tfidef score of each word of each sentence (after initial preprocessing and wordnet lemmatization stage). Then consider only sentences that contain person or organization named-entities and use similar approach to output the tfidf score of the named-entities in each sentence. Finally construct the sentence (S) weight as a  weighted sum:\n",
    "<br>\n",
    "$$S_{weight}=\\sum_{w\\varepsilon S}W_{TfiDf}+2\\sum_{NM\\varepsilon S}NM_{TfiDf}+POS_s$$\n",
    "<br>\n",
    "where NMTfiDF stands for the TfIdF of named-entity NM in sentence S.  POSS corresponds to the sentence weight associated to the location of the sentence. So that the sentence location weight will be maximum (1) if located in the title of the document, 0.5 if located  in the title of one of the subsection, 0.25 if located in the title one of the subsubsection, 0.1 if located in one representative object of the document, and 0 if located only in the main text. Make sure to normalize the term tfidf and Nm tfidf weights and suggest a script to implement the preceding accordingly, so that the summarizer will contain the 10 sentences with the highest Sweight scores.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TASK 4\n",
    "Test the above approach with Opinosis dataset available at https://kavita-ganesan.com/opinosis-opinion-dataset/#.YVw6J5ozY2x,  and record the corresponding Rouge-2 and Rouge-3 evaluation score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5\n",
    "\n",
    "We would like to improve the summarization by taking into account the diversity among the sentence in the sense that we would like to minimize redundancy among sentences. For this purpose, we shall use the sentence-to-sentence semantic similarity introduced in the NLP lab. Next, instead of recording only the 10 sentences with highest Sweight scores, we shall record the 20 top sentences in terms of $S_{weight}$ scores. Then the selection of the top 10 sentences among the 20 sentences follows the following approach. First, order the 20 sentences in the decreasing order of their $S_{weight}$ scores, say S1, S2, …, S20 (where S1 is the top ranked and S20 the 20th ranked sentence). Second, we shall assume that S1 is always included in the summarizer, we shall then attempt to find the other sentences among S2 till S20 to be included into the summarizer. Calculate the sentence-to-sentence similarity Sim(S1,Si) for i=1 to 20, the Sentence Sj that yields the minimum similarity with S1 will therefore be included in the summarizer. Next, for each of the remaining sentences Sk (with k different from 1 and j), we calculate the sentence similarity with Sj. Therefore the sentence Sp that yields minimum value of “Sim(Sp, S1)+Sim(Sp,Sj)” will be included in the summarizer (Note: the quantity Sim(Sp, S1) is already calculated in previous step).  Similarly in the next phase, we should select a sentence Sl (l different from 1, j and k) so that  “Sim(Sl, S1)+Sim(Sl,Sj)+Sim(Sl,Sp)”, Etc.. You then stop once you reached 10 sentences included in the summarizer. Suggest a script that includes this process.. and illustrate its functioning in the example you chosen in 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6\n",
    "\n",
    "We would like to make the choice of keywords not based on histogram frequency but using the open source RAKE https://www.airpair.com/nlp/keyword-extraction-tutorial. Repeat the previous process of selecting the sentences that are associated to the ten first keywords generated by RAKE. Comment on the quality of this summarizer based on your observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7\n",
    "\n",
    "It is also suggested to explore alternative implementations with larger number of summarization approaches implemented- https://github.com/miso-belica/sumy. Show how each of the implemented summarizer behaves when inputted with the same document you used in previous case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 8\n",
    "\n",
    "Now we would like to compare the above summarizers and those in 3), 5) and 7) on a new dataset constructed as follows. First select an Elsevier journal of your own and select 10 papers highly ranked in the journal according to citation index (The journal papers should be well structured to contain Abstract, Introduction and Conclusion). For each of the ten papers, consider the introduction as the main document to seek to apply summarizer, and consider the Abstract and Conclusion as two golden summary of the document that you can use for assessment using ROUGE-1 and ROUGE-2 evaluation. Report in a table the evaluation score of each summarizer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 9\n",
    "\n",
    "Design a simple GUI that allows the user to input a text or a link to a document to be summarized and output the summarizer according to 3), algorithms implemented in 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
