{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 21: Automatic Summarization  \n",
    "\n",
    "We shall consider structured document containing a title, abstract and a set of subsections. We would like to build a text summarizer such that tracks important keywords in the document. For this purpose, the first step is identify these keywords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\markus\\anaconda3\\lib\\site-packages (21.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                            Version\n",
      "---------------------------------- -----------------\n",
      "absl-py                            0.9.0\n",
      "aiohttp                            3.8.0\n",
      "aiosignal                          1.2.0\n",
      "alabaster                          0.7.12\n",
      "anaconda-client                    1.8.0\n",
      "anaconda-navigator                 1.9.12\n",
      "anaconda-project                   0.10.1\n",
      "anyio                              2.2.0\n",
      "argh                               0.26.2\n",
      "argon2-cffi                        20.1.0\n",
      "asn1crypto                         1.4.0\n",
      "astroid                            2.6.6\n",
      "astropy                            4.3.1\n",
      "astunparse                         1.6.3\n",
      "async-generator                    1.10\n",
      "async-timeout                      4.0.0\n",
      "asynctest                          0.13.0\n",
      "atomicwrites                       1.4.0\n",
      "attrs                              21.2.0\n",
      "autopep8                           1.5.7\n",
      "Babel                              2.9.1\n",
      "backcall                           0.2.0\n",
      "backports.functools-lru-cache      1.6.4\n",
      "backports.shutil-get-terminal-size 1.0.0\n",
      "backports.tempfile                 1.0\n",
      "backports.weakref                  1.0.post1\n",
      "bcrypt                             3.2.0\n",
      "beautifulsoup4                     4.10.0\n",
      "bitarray                           2.3.0\n",
      "bkcharts                           0.2\n",
      "bleach                             4.0.0\n",
      "blis                               0.7.5\n",
      "bokeh                              2.3.3\n",
      "boto                               2.49.0\n",
      "Bottleneck                         1.3.2\n",
      "breadability                       0.1.20\n",
      "brotlipy                           0.7.0\n",
      "cachetools                         4.1.1\n",
      "catalogue                          2.0.6\n",
      "certifi                            2021.10.8\n",
      "cffi                               1.14.6\n",
      "chardet                            4.0.0\n",
      "charset-normalizer                 2.0.4\n",
      "click                              8.0.1\n",
      "cloudpickle                        2.0.0\n",
      "clyent                             1.2.2\n",
      "colorama                           0.4.4\n",
      "comtypes                           1.1.10\n",
      "conda                              4.10.3\n",
      "conda-build                        3.18.11\n",
      "conda-pack                         0.6.0\n",
      "conda-package-handling             1.7.3\n",
      "conda-verify                       3.4.2\n",
      "configparser                       5.0.2\n",
      "contextlib2                        0.6.0.post1\n",
      "crayons                            0.4.0\n",
      "cryptography                       3.4.8\n",
      "cycler                             0.10.0\n",
      "cymem                              2.0.6\n",
      "Cython                             0.29.24\n",
      "cytoolz                            0.11.0\n",
      "dask                               2021.8.1\n",
      "dataframe-image                    0.1.1\n",
      "decorator                          5.1.0\n",
      "defusedxml                         0.7.1\n",
      "diff-match-patch                   20200713\n",
      "distributed                        2021.8.1\n",
      "docopt                             0.6.2\n",
      "docutils                           0.17.1\n",
      "en-core-web-sm                     3.1.0\n",
      "entrypoints                        0.3\n",
      "et-xmlfile                         1.1.0\n",
      "fastcache                          1.1.0\n",
      "filelock                           3.0.12\n",
      "flake8                             3.9.2\n",
      "Flask                              2.0.2\n",
      "fonttools                          4.25.0\n",
      "frozenlist                         1.2.0\n",
      "fsspec                             2021.8.1\n",
      "future                             0.18.2\n",
      "gast                               0.3.3\n",
      "gevent                             21.8.0\n",
      "glob2                              0.7\n",
      "gmpy2                              2.0.8\n",
      "google-auth                        1.18.0\n",
      "google-auth-oauthlib               0.4.1\n",
      "google-pasta                       0.2.0\n",
      "greenlet                           1.1.1\n",
      "grpcio                             1.30.0\n",
      "h11                                0.12.0\n",
      "h5py                               2.10.0\n",
      "HeapDict                           1.0.1\n",
      "html5lib                           1.1\n",
      "idna                               3.2\n",
      "imagecodecs                        2021.6.8\n",
      "imageio                            2.9.0\n",
      "imagesize                          1.2.0\n",
      "importlib-metadata                 4.8.1\n",
      "iniconfig                          1.1.1\n",
      "intervaltree                       3.1.0\n",
      "ipykernel                          5.3.4\n",
      "ipython                            7.22.0\n",
      "ipython-genutils                   0.2.0\n",
      "ipywidgets                         7.6.5\n",
      "isort                              5.9.3\n",
      "itsdangerous                       2.0.1\n",
      "jdcal                              1.4.1\n",
      "jedi                               0.14.1\n",
      "jellyfish                          0.8.8\n",
      "Jinja2                             3.0.1\n",
      "joblib                             1.0.1\n",
      "json5                              0.9.6\n",
      "jsonschema                         3.2.0\n",
      "jupyter                            1.0.0\n",
      "jupyter-client                     7.0.1\n",
      "jupyter-console                    6.4.0\n",
      "jupyter-core                       4.8.1\n",
      "jupyter-server                     1.4.1\n",
      "jupyterlab                         3.1.7\n",
      "jupyterlab-pygments                0.1.2\n",
      "jupyterlab-server                  2.8.2\n",
      "jupyterlab-widgets                 1.0.0\n",
      "Keras-Preprocessing                1.1.2\n",
      "keyring                            23.1.0\n",
      "kiwisolver                         1.3.1\n",
      "lazy-object-proxy                  1.6.0\n",
      "libarchive-c                       2.9\n",
      "llvmlite                           0.36.0\n",
      "locket                             0.2.1\n",
      "lxml                               4.6.3\n",
      "Markdown                           3.2.2\n",
      "MarkupSafe                         2.0.1\n",
      "matplotlib                         3.4.2\n",
      "mccabe                             0.6.1\n",
      "menuinst                           1.4.16\n",
      "mistune                            0.8.4\n",
      "mkl-fft                            1.3.0\n",
      "mkl-random                         1.2.2\n",
      "mkl-service                        2.4.0\n",
      "mock                               4.0.3\n",
      "more-itertools                     8.8.0\n",
      "mpmath                             1.2.1\n",
      "msgpack                            1.0.2\n",
      "multidict                          5.2.0\n",
      "multipledispatch                   0.6.0\n",
      "munkres                            1.1.4\n",
      "murmurhash                         1.0.6\n",
      "mysql-connector-python             8.0.21\n",
      "navigator-updater                  0.2.1\n",
      "nbclassic                          0.2.6\n",
      "nbclient                           0.5.3\n",
      "nbconvert                          6.1.0\n",
      "nbformat                           5.1.3\n",
      "nest-asyncio                       1.5.1\n",
      "networkx                           2.6.3\n",
      "nltk                               3.6.5\n",
      "nose                               1.3.7\n",
      "notebook                           6.4.3\n",
      "numba                              0.53.1\n",
      "numexpr                            2.7.3\n",
      "numpy                              1.21.2\n",
      "numpydoc                           1.1.0\n",
      "oauthlib                           3.1.0\n",
      "olefile                            0.46\n",
      "openpyxl                           3.0.9\n",
      "opt-einsum                         3.2.1\n",
      "outcome                            1.1.0\n",
      "packaging                          21.0\n",
      "pandas                             1.3.3\n",
      "pandocfilters                      1.4.3\n",
      "paramiko                           2.7.2\n",
      "parso                              0.5.2\n",
      "partd                              1.2.0\n",
      "path                               16.0.0\n",
      "pathlib2                           2.3.6\n",
      "pathtools                          0.1.2\n",
      "pathy                              0.6.0\n",
      "patsy                              0.5.2\n",
      "pep8                               1.7.1\n",
      "pexpect                            4.8.0\n",
      "pickleshare                        0.7.5\n",
      "Pillow                             8.3.1\n",
      "pip                                21.3.1\n",
      "pkginfo                            1.7.1\n",
      "plotly                             4.9.0\n",
      "pluggy                             0.13.1\n",
      "ply                                3.11\n",
      "preshed                            3.0.6\n",
      "prometheus-client                  0.11.0\n",
      "prompt-toolkit                     3.0.20\n",
      "protobuf                           3.12.2\n",
      "psutil                             5.8.0\n",
      "ptyprocess                         0.7.0\n",
      "py                                 1.10.0\n",
      "pyasn1                             0.4.8\n",
      "pyasn1-modules                     0.2.8\n",
      "pycodestyle                        2.7.0\n",
      "pycosat                            0.6.3\n",
      "pycountry                          20.7.3\n",
      "pycparser                          2.20\n",
      "pycrypto                           2.6.1\n",
      "pycurl                             7.44.1\n",
      "pydantic                           1.8.2\n",
      "pydocstyle                         6.1.1\n",
      "pyerfa                             2.0.0\n",
      "pyflakes                           2.3.1\n",
      "Pygments                           2.10.0\n",
      "pylint                             2.9.6\n",
      "PyNaCl                             1.4.0\n",
      "pyodbc                             4.0.0-unsupported\n",
      "pyOpenSSL                          20.0.1\n",
      "pyparsing                          2.4.7\n",
      "PyQt5                              5.12.3\n",
      "PyQt5_sip                          4.19.18\n",
      "PyQtWebEngine                      5.12.1\n",
      "pyreadline                         2.1\n",
      "pyrsistent                         0.17.3\n",
      "PySocks                            1.7.1\n",
      "pytest                             6.2.4\n",
      "python-dateutil                    2.8.2\n",
      "python-jsonrpc-server              0.4.0\n",
      "python-language-server             0.31.7\n",
      "pytz                               2021.3\n",
      "PyWavelets                         1.1.1\n",
      "pywin32                            228\n",
      "pywin32-ctypes                     0.2.0\n",
      "pywinpty                           0.5.7\n",
      "PyYAML                             5.4.1\n",
      "pyzmq                              22.2.1\n",
      "QDarkStyle                         3.0.2\n",
      "QtAwesome                          1.0.2\n",
      "qtconsole                          5.1.1\n",
      "QtPy                               1.10.0\n",
      "regex                              2021.8.3\n",
      "requests                           2.26.0\n",
      "requests-oauthlib                  1.3.0\n",
      "retrying                           1.3.3\n",
      "rope                               0.19.0\n",
      "rougescore                         0.1.0\n",
      "rsa                                4.6\n",
      "Rtree                              0.9.7\n",
      "ruamel-yaml-conda                  0.15.100\n",
      "scikit-image                       0.18.1\n",
      "scikit-learn                       0.24.2\n",
      "scipy                              1.7.1\n",
      "seaborn                            0.11.2\n",
      "segtok                             1.5.10\n",
      "selenium                           4.0.0\n",
      "Send2Trash                         1.8.0\n",
      "setuptools                         58.0.4\n",
      "simplegeneric                      0.8.1\n",
      "singledispatch                     3.7.0\n",
      "sip                                4.19.25\n",
      "six                                1.16.0\n",
      "smart-open                         5.2.1\n",
      "sniffio                            1.2.0\n",
      "snowballstemmer                    2.1.0\n",
      "sortedcollections                  2.1.0\n",
      "sortedcontainers                   2.4.0\n",
      "soupsieve                          2.2.1\n",
      "spacy                              3.1.3\n",
      "spacy-legacy                       3.0.8\n",
      "Sphinx                             4.2.0\n",
      "sphinxcontrib-applehelp            1.0.2\n",
      "sphinxcontrib-devhelp              1.0.2\n",
      "sphinxcontrib-htmlhelp             2.0.0\n",
      "sphinxcontrib-jsmath               1.0.1\n",
      "sphinxcontrib-qthelp               1.0.3\n",
      "sphinxcontrib-serializinghtml      1.1.5\n",
      "sphinxcontrib-websupport           1.2.4\n",
      "spyder                             4.0.1\n",
      "spyder-kernels                     1.8.1\n",
      "SQLAlchemy                         1.4.22\n",
      "srsly                              2.4.1\n",
      "statsmodels                        0.12.2\n",
      "sumy                               0.9.0\n",
      "sympy                              1.8\n",
      "tables                             3.6.1\n",
      "tabulate                           0.8.9\n",
      "tblib                              1.7.0\n",
      "tensorboard                        2.2.2\n",
      "tensorboard-plugin-wit             1.7.0\n",
      "tensorflow                         2.2.0\n",
      "tensorflow-estimator               2.2.0\n",
      "termcolor                          1.1.0\n",
      "terminado                          0.9.4\n",
      "testpath                           0.5.0\n",
      "thinc                              8.0.11\n",
      "threadpoolctl                      2.2.0\n",
      "tifffile                           2021.7.2\n",
      "toml                               0.10.2\n",
      "toolz                              0.11.1\n",
      "tornado                            6.1\n",
      "tqdm                               4.62.2\n",
      "traitlets                          5.1.0\n",
      "trio                               0.19.0\n",
      "trio-websocket                     0.9.2\n",
      "typed-ast                          1.4.3\n",
      "typer                              0.4.0\n",
      "typing-extensions                  3.10.0.2\n",
      "ujson                              4.0.2\n",
      "umap                               0.1.1\n",
      "umap-learn                         0.4.6\n",
      "unicodecsv                         0.14.1\n",
      "urllib3                            1.26.7\n",
      "wasabi                             0.8.2\n",
      "watchdog                           2.1.3\n",
      "wcwidth                            0.2.5\n",
      "webdriver-manager                  3.5.1\n",
      "webencodings                       0.5.1\n",
      "Werkzeug                           2.0.1\n",
      "wheel                              0.37.0\n",
      "widgetsnbextension                 3.5.1\n",
      "win-inet-pton                      1.1.0\n",
      "win-unicode-console                0.5\n",
      "wincertstore                       0.2\n",
      "wrapt                              1.12.1\n",
      "wsproto                            1.0.0\n",
      "xlrd                               2.0.1\n",
      "XlsxWriter                         3.0.1\n",
      "xlwings                            0.24.7\n",
      "xlwt                               1.3.0\n",
      "xmltodict                          0.12.0\n",
      "yake                               0.4.8\n",
      "yapf                               0.31.0\n",
      "yarl                               1.7.2\n",
      "zict                               2.0.0\n",
      "zipp                               3.6.0\n",
      "zope.event                         4.5.0\n",
      "zope.interface                     5.4.0\n"
     ]
    }
   ],
   "source": [
    "!pip list\n",
    "# tarkista löytyykö: lxml, html5lib, requests, selenium, webdriver-manager\n",
    "# lisäohjeita task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Markus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#jos nltk ei löydy asenna -> ! pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#from nltk.cluster.util import cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "Assume the initial input is given as html document (choose an example of your own), we hypothesize that important keywords are initially contained in the words of titles, abstract and possibly titles of subsections of the document. Suggest a simple python script that inputs an html document and outputs the lists of words in the title, abstract and title of section/subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\Markus\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      "Scalable Nearest Neighbor Algorithms for High Dimensional Data. \n",
      "\n",
      "\n",
      "Abstract:\n",
      " Abstract:For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching. \n",
      "\n",
      "\n",
      "Section titles:\n",
      "Fast Approximate NN Matching. Scaling Nearest Neighbor Search. The FLANN Library. \n",
      "\n",
      "\n",
      "Subsection titles:\n",
      "1.1 Definitions and Notation. 2.1 Nearest Neighbor Matching Algorithms. 2.2 Automatic Configuration of NN Algorithms. 3.1 The Randomized k-d Tree Algorithm. 3.2 The Priority Search K-Means Tree Algorithm. 3.3 The Hierarchical Clustering Tree. 3.4 Automatic Selection of the Optimal Algorithm. 4.1 Fast Approximate Nearest Neighbor Search. 4.2 Binary Features. 5.1 Searching on a Compute Cluster. 5.2 Evaluation of Distributed Search. \n",
      "\n",
      "\n",
      "Subsubsection titles:\n",
      "2.1.1 Partitioning Trees. 2.1.2 Hashing Based Nearest Neighbor Techniques. 2.1.3 Nearest Neighbor Graph Techniques. 3.2.1 Algorithm Description. 3.2.2 Analysis. 4.1.1 Data Dimensionality. 4.1.2 Search Precision. 4.1.3 Automatic Selection of Optimal Algorithm. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "# Kaikki sivut ei anna koko html bodyä käyttämällä pelkkää requestia. Seleniumilla näyttää toimivan useammilla. \n",
    "# pip install -U selenium\n",
    "# pip install webdriver-manager\n",
    "# jos käytät anacondaa eikä meinaa toimia niin kokeile myös $ conda update pip\n",
    "\n",
    "# Collect title, subtitles, abstract and body text from html file.\n",
    "# Print out titles and abstract and construct one string based on\n",
    "# the elements.\n",
    "\n",
    "def _convertHtmlToStr(elements):\n",
    "    str = \"\"\n",
    "    for element in elements:\n",
    "        if len(element.text.split()) > 1:\n",
    "            str += element.text\n",
    "            if not str.endswith(\".\"):\n",
    "                str += \".\"\n",
    "            str += \" \"\n",
    "    sentences = sent_tokenize(str)\n",
    "    return str, len(sentences)\n",
    "\n",
    "def scrape_article(url):\n",
    "    article = \"\"\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for article to fully load\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    strElement = \"\"\n",
    "    countTitle, countAbstract, countH2, countH3, countH4, countP = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    strElement, countTitle = _convertHtmlToStr(soup.find(\"h1\", {\"class\": \"document-title\"}))\n",
    "    print(\"Title:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "    article += \". \"\n",
    "    strElement, countAbstract = _convertHtmlToStr(soup.find(\"div\", {\"class\": \"abstract-text\"}))\n",
    "    print(\"Abstract:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "\n",
    "    articleHtmlBody = soup.find(\"div\", {\"id\": \"article\"})\n",
    "    if articleHtmlBody == None:\n",
    "        raise ValueError\n",
    "\n",
    "    strElement, countH2 = _convertHtmlToStr(articleHtmlBody.find_all(\"h2\"))\n",
    "    print(\"Section titles:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "    strElement, countH3 = _convertHtmlToStr(articleHtmlBody.find_all(\"h3\"))\n",
    "    print(\"Subsection titles:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "    strElement, countH4 = _convertHtmlToStr(articleHtmlBody.find_all(\"h4\"))\n",
    "    print(\"Subsubsection titles:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "    strElement, countP = _convertHtmlToStr(articleHtmlBody.find_all(\"p\"))\n",
    "    article += strElement\n",
    "    countP += 1\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    counts = [countTitle, countAbstract, countH2, countH3, countH4, countP]\n",
    "    return article, counts\n",
    "\n",
    "url = \"https://ieeexplore.ieee.org/document/6809191\"\n",
    "article, counts = scrape_article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keyword search and analysis\n",
    "\n",
    "w_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50 #alunperin 10\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "Write down a simple python script that allows you to output the histogram of word frequency in the document, excluding the stopwords (see examples in online NLTK book). Use SpaCy named-entity tagger to identify person-named entities and organization-named entities in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = word_tokenize(article)\n",
    "tokens_without_sw = [word for word in text_tokens if word.isalpha() and word not in all_stopwords]\n",
    "#print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "### Count histogram from every word manually ###\n",
    "#charsToRemove = \".,()\"\n",
    "#wordCounts = Counter(tokens_without_sw)\n",
    "wordCounts = Counter(tokens_without_sw)\n",
    "wordCounts = wordCounts.most_common()\n",
    "\n",
    "wordCounts = wordCounts[0:20]\n",
    "\n",
    "words = list(zip(*wordCounts))[0]\n",
    "occurency = list(zip(*wordCounts))[1]\n",
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "plt.bar(np.arange(len(words)), occurency, align='center')\n",
    "plt.xticks(np.arange(len(words)), words, rotation='vertical')\n",
    "plt.ylabel('Keyword count')\n",
    "plt.xlabel('Keyword id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCC - ORG\n",
      "NN - ORG\n",
      "NN - ORG\n",
      "KNN - ORG\n",
      "RNN - ORG\n",
      "KNN - ORG\n",
      "P K View Source KNN - ORG\n",
      "RKNN - ORG\n",
      "RKNN - ORG\n",
      "Variations - ORG\n",
      "Leibe - PERSON\n",
      "Schindler - PERSON\n",
      "Jégou - PERSON\n",
      "Babenko Lempitsky - PERSON\n",
      "LSH - ORG\n",
      "Variants LSH LSH - ORG\n",
      "LSH Forest - ORG\n",
      "LSH - ORG\n",
      "Nearest - ORG\n",
      "Wang - PERSON\n",
      "Bawa - PERSON\n",
      "LSH Forest - ORG\n",
      "downhill simplex - PERSON\n",
      "Bergstra Bengio - PERSON\n",
      "Exact - PERSON\n",
      "Fig - PERSON\n",
      "Fig - ORG\n",
      "Fig - PERSON\n",
      "ORB - ORG\n",
      "Hamming - ORG\n",
      "Depending - PERSON\n",
      "Section Data - ORG\n",
      "Fig - ORG\n",
      "Fig - ORG\n",
      "Fig - PERSON\n",
      "Fig - PERSON\n",
      "Trevi - PERSON\n",
      "Trevi Fountain - PERSON\n",
      "Fig - PERSON\n",
      "SIFT - ORG\n",
      "SIFT - PERSON\n",
      "Fig - PERSON\n",
      "Fig ANN - PERSON\n",
      "Comparison - ORG\n",
      "SIFT - ORG\n",
      "Trevi Fountain - ORG\n",
      "SIFT SURF - PERSON\n",
      "ORB - ORG\n",
      "Fig - ORG\n",
      "SIFT SURF - PERSON\n",
      "ORB - ORG\n",
      "ORB - ORG\n",
      "Fig LSH - PERSON\n",
      "Comparison - ORG\n",
      "Fitting - ORG\n",
      "N - ORG\n",
      "MPI - ORG\n",
      "Fig - PERSON\n",
      "MPI - ORG\n",
      "Aly - PERSON\n",
      "LSH - ORG\n",
      "MPI - ORG\n",
      "Fig - ORG\n",
      "MPI - ORG\n",
      "Fig - PERSON\n",
      "Fig - ORG\n",
      "Direct - ORG\n",
      "Fig - ORG\n",
      "N - ORG\n",
      "N times - ORG\n",
      "Fast Library Approximate Nearest Neighbors FLANN - ORG\n",
      "FLANN - PERSON\n",
      "PCL - ORG\n",
      "ROS FLANN - ORG\n",
      "Ubuntu Fedora Arch Gentoo - PERSON\n"
     ]
    }
   ],
   "source": [
    "#Use SpaCy to identify person-named entities and organization-named entities\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "\n",
    "#vinkkiä https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "\n",
    "#Identifying person and organization-named entities\n",
    "wordsInStr = \"\"\n",
    "for word in tokens_without_sw:\n",
    "    wordsInStr += word\n",
    "    wordsInStr += \" \"\n",
    "    \n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(wordsInStr)\n",
    "\n",
    "#Print only ORG or PERSON labeled entities\n",
    "if doc.ents:\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" or ent.label_ == \"PERSON\":\n",
    "            print(ent.text+ \" - \" + ent.label_)\n",
    "else:\n",
    "    print(\"No named entities found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3\n",
    "\n",
    "We would like the summarizer to contain frequent wording (excluding stopwords) and as many named-entities as possible. For this purpose, use the following heuristic to construct the summarizer. First we shall assume each sentence of the document as individual sub-document. Use TfIdf vectorizer to output the individual tfidef score of each word of each sentence (after initial preprocessing and wordnet lemmatization stage). Then consider only sentences that contain person or organization named-entities and use similar approach to output the tfidf score of the named-entities in each sentence. Finally construct the sentence (S) weight as a  weighted sum:\n",
    "<br>\n",
    "$$S_{weight}=\\sum_{w\\varepsilon S}W_{TfiDf}+2\\sum_{NM\\varepsilon S}NM_{TfiDf}+POS_s$$\n",
    "<br>\n",
    "where NMTfiDF stands for the TfIdF of named-entity NM in sentence S.  POSS corresponds to the sentence weight associated to the location of the sentence. So that the sentence location weight will be maximum (1) if located in the title of the document, 0.5 if located  in the title of one of the subsection, 0.25 if located in the title one of the subsubsection, 0.1 if located in one representative object of the document, and 0 if located only in the main text. Make sure to normalize the term tfidf and Nm tfidf weights and suggest a script to implement the preceding accordingly, so that the summarizer will contain the 10 sentences with the highest Sweight scores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Markus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Markus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"1\": 1,\n",
    "    \"2\": 0.1,\n",
    "    \"3\": 0.5,\n",
    "    \"4\": 0.25,\n",
    "    \"else\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculateFullScores(sentenceScores, namedEntityScores, counts):\n",
    "    scaler = MinMaxScaler()\n",
    "    weightList= []\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        if counts[2] == 0:\n",
    "            counts.pop[2]\n",
    "    else:\n",
    "        counts = [0, 0, 0, 0, len(sentenceScores)]\n",
    "\n",
    "    for i in range(len(counts)):\n",
    "        for j in range(counts[i]):\n",
    "            if i > 3:\n",
    "                weightList.append(weights[\"else\"])\n",
    "            else:\n",
    "                weightList.append(weights[str(i+1)])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Weights\": weightList,\n",
    "        \"SentenceScores\": sentenceScores,\n",
    "        \"EntityScores\": namedEntityScores,\n",
    "    })\n",
    "\n",
    "    df[[\"SentencesScaled\"]] = scaler.fit_transform(df[[\"SentenceScores\"]])\n",
    "    df[[\"EntitiesScaled\"]] = scaler.fit_transform(df[[\"EntityScores\"]])\n",
    "    df[\"S_weight\"] = df[\"SentencesScaled\"] + (2 * df[\"EntitiesScaled\"]) + df[\"Weights\"]\n",
    "    return df[\"S_weight\"].tolist()\n",
    "\n",
    "\n",
    "def _getNamedEntities(article):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    doc = nlp(article)\n",
    "    namedEntities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" or ent.label_ == \"PERSON\":\n",
    "                namedEntities.append(ent.text)\n",
    "\n",
    "    return namedEntities\n",
    "\n",
    "\n",
    "def _getSentencesWithMaxWeights(weights, sentences, numberOfSentences):\n",
    "    arr = np.array(weights)\n",
    "    indexes = np.argpartition(arr, -numberOfSentences)[-numberOfSentences:]\n",
    "    sentences = np.array(sentences)\n",
    "    return sentences[indexes]\n",
    "\n",
    "\n",
    "def _preProcess(document):\n",
    "    stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    WN_lemmatizer = WordNetLemmatizer()\n",
    "    sentences = sent_tokenize(document)\n",
    "    processedSentences = []\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [WN_lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "\n",
    "        # get rid of numbers and Stopwords\n",
    "        words = [word for word in words if word.isalpha() and word not in stopwords]\n",
    "        processedSentences.append(' '.join(word for word in words))\n",
    "        tokens.extend(words)\n",
    "\n",
    "    return processedSentences, tokens\n",
    "\n",
    "\n",
    "def _tfidfScores(corpus, sentences):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    fittedVectorizer = tfidf.fit(corpus)\n",
    "    vectors = fittedVectorizer.transform(sentences).toarray()\n",
    "\n",
    "    scores = []\n",
    "    for i in range(len(vectors)):\n",
    "        score = 0\n",
    "        for j in range(len(vectors[i])):\n",
    "            score = score + vectors[i][j]\n",
    "\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our experiments have revealed that the optimal algorithm for approximate nearest neighbor search is highly dependent\n",
      " on several factors such as the data dimensionality, size and structure of the data set (whether there is any\n",
      " correlation between the features in the data set) and the desired search precision.\n",
      "\n",
      "For the comparison we use a combination of both vector features such as SIFT, SURF, image patches and\n",
      " binary features such as BRIEF and ORB.\n",
      "\n",
      "After evaluating many different algorithms for approximate nearest neighbor search on data sets with a wide range of\n",
      " dimensionality [14][47], we have\n",
      " found that one of two algorithms gave the best performance: the priority search k-means tree or\n",
      " the multiple randomized k-d trees.\n",
      "\n",
      "We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set.\n",
      "\n",
      "Scalable Nearest Neighbor Algorithms for High Dimensional Data.\n",
      "\n",
      "All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.\n",
      "\n",
      "Each point on the graph is computed using the best\n",
      " performing algorithm for that particular feature type (randomized kd-trees or priority search k-means tree for SIFT,\n",
      " SURF, image patches and the hierarchical clustering algorithm for BRIEF and ORB).\n",
      "\n",
      "We have found the randomized k-d forest to be very effective in many situations, however on other data sets a\n",
      " different algorithm, the priority search k-means tree, has been more effective at finding approximate\n",
      " nearest neighbors, especially when a high precision is required.\n",
      "\n",
      "We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple\n",
      " randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN \n",
      "[11] and LSH algorithms [29]\n",
      " on the first data set of 100,000 SIFT features.\n",
      "\n",
      "The partitioning of the data set into independent subsets, as described above and implemented in FLANN, has the\n",
      " advantage that it doesn’t depend on the type of index used (randomized kd-trees, priority search k-means tree,\n",
      " hierarchical clustering, LSH) and can be applied to any current or future nearest neighbor algorithm in FLANN.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def findTopSentences(document, numberOfSentences, isUrl):\n",
    "    sentences, tokens = _preProcess(document)\n",
    "    sentenceTfidfScores = _tfidfScores(tokens, sentences)\n",
    "    namedEntitiesTfidfScores = _tfidfScores(_getNamedEntities(document), sentences)\n",
    "    time.sleep(0.1)\n",
    "    SWeight = []\n",
    "    if isUrl:\n",
    "        SWeight = _calculateFullScores(sentenceTfidfScores, namedEntitiesTfidfScores, counts)\n",
    "    else:\n",
    "        SWeight = _calculateFullScores(sentenceTfidfScores, namedEntitiesTfidfScores, [])\n",
    "    topSentences = _getSentencesWithMaxWeights(SWeight, sent_tokenize(document), numberOfSentences)\n",
    "    return list(topSentences)\n",
    "\n",
    "topSentences = findTopSentences(article, 10, True)\n",
    "for sentence in topSentences:\n",
    "    print(\"{}\\n\".format(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TASK 4\n",
    "Test the above approach with Opinosis dataset available at https://kavita-ganesan.com/opinosis-opinion-dataset/#.YVw6J5ozY2x,  and record the corresponding Rouge-2 and Rouge-3 evaluation score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge 2 ja 3 scoring\n",
    "#https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460\n",
    "#pip install git+git://github.com/bdusell/rougescore.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rougescore as rouge\n",
    "\n",
    "def getRouge(peer, model):\n",
    "    rougeBi = rouge.rouge_2(peer, model, 1)\n",
    "    rougeTri = rouge.rouge_3(peer, model, 1)\n",
    "    \n",
    "    return rougeBi,rougeTri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_folder(dir):\n",
    "    \n",
    "    topic = []\n",
    "    \n",
    "    for file in os.listdir(dir):   \n",
    "        with open(os.path.join(dir + file)) as f:\n",
    "            doc = f.readlines()\n",
    "            f.close()\n",
    "            doc = \" \".join(doc)\n",
    "            topic.append(doc)\n",
    "            \n",
    "    return topic           \n",
    "\n",
    "def create_model(dir):\n",
    "    \n",
    "    model = []\n",
    "    \n",
    "    for folder in os.listdir(directory + \"summaries-gold/\"):\n",
    "        \n",
    "        gold = read_folder(directory + \"summaries-gold/\" + folder + \"/\")\n",
    "        model.append(gold)\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(dir):\n",
    "    \n",
    "    list_summary = []\n",
    "    for file in os.listdir(directory + \"topics/\"):\n",
    "            \n",
    "        with open(os.path.join(dir + \"topics/\" + file)) as f:\n",
    "            doc = f.readlines()\n",
    "            f.close()\n",
    "            doc = \" \".join(doc)\n",
    "            \n",
    "            summary = findTopSentences(doc, 10, False)\n",
    "            list_summary.append(summary)\n",
    "            \n",
    "    return list_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"C:/Users/Markus/Documents/studies/NLP/data/Opinosis_dataset/\"\n",
    "\n",
    "summary = summary(directory)\n",
    "model = create_model(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lause = \"\"\n",
    "#for senctence in summary:\n",
    "#    lause += sentence\n",
    "\n",
    "#malli = [\"The battery life of the ipod nano is very short. It seems to continue\", \"using battery even when the ipod is not in use, otherwise, it's a great product.\"]\n",
    "#result = rouge.rouge_2(lause, model, 1)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary = ['It has worked well for local driving giving accurate directions for roads and streets .', \",  Very Accurate but with one small glitch I found ,  I'll explain in the CONS\\n This is a great GPS, it is so easy to use and it is always accurate .\", 'The Garmin is loaded with very accurate maps that generally know the roads in even the remotest areas .', \"I used it the day I bought it,   and then this morning, and as soon as it comes on it is  ready to navigate  The only downfall of this product, and the only reason I did not give it 5 stars is the fact that the speed limit it displays for the road you are on isn't 100% accurate .\", 'Depending on what you are using it for, it is a nice adjunct to a travel trip and the directions are accurate and usually the quickest, but not always .', \"I'm really glad I bought it though, and like the easy to read graphics, the voice used to tell you the name of the street you are to turn on, the uncannily accurate estimates of mileage and time of arrival at your destination .\", 'My new Garmin 255w had very Easy Set Up, Accurate Directions to locations, User Friendly Unit to anyone in my vehicle who tried it .', 'In closing, this is a fantastic GPS with some very nice features and is very accurate in directions .', 'but for the most part, we find that the Garmin software provides accurate directions, whereever we intend to go .', '0 out of 5 stars Inexpensive, accurate, plenty of features, August 6, 2009\\n  The only glitch I have found so far is that the speed limits are not 100% accurate, although the GPS, amazingly, is able to very accurately tell you how fast your vehicle is moving .']\n",
    "#summary = \" \".join(summary)\n",
    "#print(type(summary))\n",
    "#model = ['This unit is generally quite accurate.  \\n Set-up and usage are considered to be very easy. \\n The maps can be updated, and tend to be reliable.\\n', \"The Garmin seems to be generally very accurate.\\n It's easy to use with an intuitive interface.\", 'It is very accurate, even in destination time.\\n', 'Very accurate with travel and destination time.\\n Negatives are not accurate with speed limits and rural roads.', 'Its accurate, fast and its simple operations make this a for sure buy.']\n",
    "#bi, tri = getRouge(summary, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_score = []\n",
    "\n",
    "for i in range(len(summary)):\n",
    "    summary_str = \" \".join(summary[i])\n",
    "    bi, tri = getRouge(summary_str, model[i])\n",
    "    list_score.append((bi,tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>topic number</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rouge3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052142</td>\n",
       "      <td>0.041615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.067094</td>\n",
       "      <td>0.052058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065919</td>\n",
       "      <td>0.051844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.052494</td>\n",
       "      <td>0.038054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045128</td>\n",
       "      <td>0.033889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.043790</td>\n",
       "      <td>0.032983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.074895</td>\n",
       "      <td>0.057781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.046805</td>\n",
       "      <td>0.037890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.063447</td>\n",
       "      <td>0.044701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.040630</td>\n",
       "      <td>0.029268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050883</td>\n",
       "      <td>0.035592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.031318</td>\n",
       "      <td>0.024251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.056122</td>\n",
       "      <td>0.042389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.054089</td>\n",
       "      <td>0.042948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.034907</td>\n",
       "      <td>0.026539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.074092</td>\n",
       "      <td>0.058873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.034845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.046476</td>\n",
       "      <td>0.036178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.072588</td>\n",
       "      <td>0.049904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.091080</td>\n",
       "      <td>0.067815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.059640</td>\n",
       "      <td>0.047376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.038121</td>\n",
       "      <td>0.031804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.054201</td>\n",
       "      <td>0.040379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.039851</td>\n",
       "      <td>0.032070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.084475</td>\n",
       "      <td>0.055157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.066485</td>\n",
       "      <td>0.051427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.073361</td>\n",
       "      <td>0.055776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.051705</td>\n",
       "      <td>0.039956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.064855</td>\n",
       "      <td>0.050348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.052850</td>\n",
       "      <td>0.033281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.025995</td>\n",
       "      <td>0.021940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.048163</td>\n",
       "      <td>0.037322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.029744</td>\n",
       "      <td>0.024631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.048331</td>\n",
       "      <td>0.041015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.048601</td>\n",
       "      <td>0.033480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.038725</td>\n",
       "      <td>0.032448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.030233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.076742</td>\n",
       "      <td>0.062539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.062967</td>\n",
       "      <td>0.046832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.026699</td>\n",
       "      <td>0.020693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.040480</td>\n",
       "      <td>0.032542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.062765</td>\n",
       "      <td>0.051399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.038750</td>\n",
       "      <td>0.029326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.051791</td>\n",
       "      <td>0.040655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.055079</td>\n",
       "      <td>0.043170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.038325</td>\n",
       "      <td>0.028673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.039497</td>\n",
       "      <td>0.028184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.044591</td>\n",
       "      <td>0.035512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.075248</td>\n",
       "      <td>0.059222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.037406</td>\n",
       "      <td>0.029803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.034094</td>\n",
       "      <td>0.025618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.052076</td>\n",
       "      <td>0.039848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "topic number    rouge2    rouge3\n",
       "0             0.052142  0.041615\n",
       "1             0.067094  0.052058\n",
       "2             0.065919  0.051844\n",
       "3             0.052494  0.038054\n",
       "4             0.045128  0.033889\n",
       "5             0.043790  0.032983\n",
       "6             0.074895  0.057781\n",
       "7             0.046805  0.037890\n",
       "8             0.063447  0.044701\n",
       "9             0.040630  0.029268\n",
       "10            0.050883  0.035592\n",
       "11            0.031318  0.024251\n",
       "12            0.056122  0.042389\n",
       "13            0.054089  0.042948\n",
       "14            0.034907  0.026539\n",
       "15            0.074092  0.058873\n",
       "16            0.043627  0.034845\n",
       "17            0.046476  0.036178\n",
       "18            0.072588  0.049904\n",
       "19            0.091080  0.067815\n",
       "20            0.059640  0.047376\n",
       "21            0.038121  0.031804\n",
       "22            0.054201  0.040379\n",
       "23            0.039851  0.032070\n",
       "24            0.084475  0.055157\n",
       "25            0.066485  0.051427\n",
       "26            0.073361  0.055776\n",
       "27            0.051705  0.039956\n",
       "28            0.064855  0.050348\n",
       "29            0.052850  0.033281\n",
       "30            0.025995  0.021940\n",
       "31            0.048163  0.037322\n",
       "32            0.029744  0.024631\n",
       "33            0.048331  0.041015\n",
       "34            0.048601  0.033480\n",
       "35            0.038725  0.032448\n",
       "36            0.038835  0.030233\n",
       "37            0.076742  0.062539\n",
       "38            0.062967  0.046832\n",
       "39            0.026699  0.020693\n",
       "40            0.040480  0.032542\n",
       "41            0.062765  0.051399\n",
       "42            0.038750  0.029326\n",
       "43            0.051791  0.040655\n",
       "44            0.055079  0.043170\n",
       "45            0.038325  0.028673\n",
       "46            0.039497  0.028184\n",
       "47            0.044591  0.035512\n",
       "48            0.075248  0.059222\n",
       "49            0.037406  0.029803\n",
       "50            0.034094  0.025618\n",
       "mean          0.052076  0.039848"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataframe_image as dfi\n",
    "\n",
    "df = pd.DataFrame(list_score)\n",
    "df.columns=['rouge2', 'rouge3']\n",
    "#df.rename(columns={'0': })\n",
    "df.loc['mean'] = df.mean()\n",
    "df.columns.names = ['topic number']\n",
    "df\n",
    "#dfi.export(df, 'dataframe.png')\n",
    "#df.mean(axis=0)\n",
    "#for list_score in list_scores:\n",
    "#    df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5\n",
    "\n",
    "[x] We would like to improve the summarization by taking into account the diversity among the sentence in the sense that we would like to minimize redundancy among sentences. For this purpose, we shall use the sentence-to-sentence semantic similarity introduced in the NLP lab. \n",
    "\n",
    "[x] Next, instead of recording only the 10 sentences with highest Sweight scores, we shall record the 20 top sentences in terms of $S_{weight}$ scores. Then the selection of the top 10 sentences among the 20 sentences follows the following approach. \n",
    "\n",
    "[x] First, order the 20 sentences in the decreasing order of their $S_{weight}$ scores, say S1, S2, …, S20 (where S1 is the top ranked and S20 the 20th ranked sentence). \n",
    "\n",
    "[x] Second, we shall assume that S1 is always included in the summarizer, we shall then attempt to find the other sentences among S2 till S20 to be included into the summarizer. \n",
    "\n",
    "[x] Calculate the sentence-to-sentence similarity Sim(S1,Si) for i=1 to 20, the Sentence Sj that yields the minimum similarity with S1 will therefore be included in the summarizer. \n",
    "\n",
    "[x] Next, for each of the remaining sentences Sk (with k different from 1 and j), we calculate the sentence similarity with Sj. Therefore the sentence Sp that yields minimum value of “Sim(Sp, S1)+Sim(Sp,Sj)” will be included in the summarizer (Note: the quantity Sim(Sp, S1) is already calculated in previous step).  \n",
    "\n",
    "[x] Similarly in the next phase, we should select a sentence Sl (l different from 1, j and k) so that  “Sim(Sl, S1)+Sim(Sl,Sj)+Sim(Sl,Sp)”, Etc.. \n",
    "\n",
    "[x] You then stop once you reached 10 sentences included in the summarizer. \n",
    "\n",
    "[ ] Suggest a script that includes this process.. and illustrate its functioning in the example you chosen in 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kirjoitan tähän itselleni että pysyn ohjeiden perässä\n",
    "#1.Luo 20 lauseen lista, missä lauseiden s(weight) pisteet ovat suurimmat (s1,s2,s3,...,s20)\n",
    "#2.s1 on tiivistelmän ensimmäinen lause \n",
    "#    2.1 poista s1 listalta\n",
    "#3.Vertaa loppuja lauseita s1. Lause joka on vähiten samanlainen s1 kanssa lisätään tiivistelmään, ja kutsutaan s(j)\n",
    "#    3.1 poista s(j) listalta\n",
    "#4.Vertaa loppuja lauseita s(j) ja taas alin arvo lisätään tiivistelmään. Lisätty lause s(p)\n",
    "#    4.1 poista lause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download larger pipeline package for spaCy\n",
    "#python -m spacy download en_core_web_lg #tarkempi mutta 770mb kokoinen\n",
    "\n",
    "#python -m spacy download en_core_web_sm #paljon pienempi mutta ei yhtä tarkka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 määritys\n",
    "def find_first_sentence(sentences):\n",
    "    picked_sentences = []\n",
    "\n",
    "    #choose dictionary\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    #nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    #löydä ensimmäinen lause, korkein s(weigth)\n",
    "    for sentence in sentences: \n",
    "        #lisää koodi s(weight) laskemiseen, tai valitse ensimmäinen lause jos lista on järjestyksessä\n",
    "        s1 = sentence\n",
    "\n",
    "    #poista valinta listasta ja lisää tiivistelmä listaan    \n",
    "    picked_sentences.append(s1)\n",
    "    sentences.remove(s1)\n",
    "\n",
    "    #print(picked_sentences)\n",
    "    return picked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loppujen yhdeksän lauseen valinta\n",
    "def sentence_to_sentence(sentences):\n",
    "#lista samanlaisuus pisteistä\n",
    "    sim_score = []\n",
    "    picked_sentences = find_first_sentence(sentences)\n",
    "    #while pyörii kunnes 10 lausetta on löydetty\n",
    "    while(len(picked_sentences)<10):\n",
    "        sim_score.clear()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            nlp_sentence = nlp(str(sentence))\n",
    "            score = 0\n",
    "\n",
    "            for p_sentence in picked_sentences:\n",
    "                #vertaa kahta lausetta\n",
    "                nlp_p_sentence = nlp(str(p_sentence))\n",
    "\n",
    "                score += nlp_p_sentence.similarity(nlp_sentence)\n",
    "\n",
    "            sim_score.append(score)\n",
    "\n",
    "\n",
    "        #print(sim_score)\n",
    "        min_value = min(sim_score)\n",
    "        min_index = sim_score.index(min_value)   \n",
    "\n",
    "        #print(\"Sentences left in the list: \" + str(len(sentences)))\n",
    "        #print(\"Smallest value: \" + str(min_value))\n",
    "        #print(sentences[min_index])\n",
    "\n",
    "        picked_sentences.append(sentences[min_index])\n",
    "        sentences.remove(sentences[min_index])\n",
    "    return picked_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n"
     ]
    }
   ],
   "source": [
    "picked_sentences = sentence_to_sentence(findTopSentences(article, 20, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text\n",
      "['The partitioning of the data set into independent subsets, as described above and implemented in FLANN, has the\\n advantage that it doesn’t depend on the type of index used (randomized kd-trees, priority search k-means tree,\\n hierarchical clustering, LSH) and can be applied to any current or future nearest neighbor algorithm in FLANN.', 'Scalable Nearest Neighbor Algorithms for High Dimensional Data.', 'For the comparison we use a combination of both vector features such as SIFT, SURF, image patches and\\n binary features such as BRIEF and ORB.', 'We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature.', 'We have found the randomized k-d forest to be very effective in many situations, however on other data sets a\\n different algorithm, the priority search k-means tree, has been more effective at finding approximate\\n nearest neighbors, especially when a high precision is required.', 'We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple\\n randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN \\n[11] and LSH algorithms [29]\\n on the first data set of 100,000 SIFT features.', 'In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper.', '[31] show that the performance of the standard LSH\\n algorithm is critically dependent on the length of the hashing key and propose the LSH Forest, a self-tuning algorithm\\n that eliminates this data dependent parameter.', 'Having an efficient\\n algorithm for performing fast nearest neighbor matching in large data sets can bring speed improvements of several\\n orders of magnitude to many applications.', 'The work presented in this paper has been made publicly available as an open source library named Fast Library for\\n Approximate Nearest Neighbors [59].']\n"
     ]
    }
   ],
   "source": [
    "print(\"Summarized text\")\n",
    "print(picked_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6\n",
    "\n",
    "We would like to make the choice of keywords not based on histogram frequency but using the open source RAKE https://www.airpair.com/nlp/keyword-extraction-tutorial. Repeat the previous process of selecting the sentences that are associated to the ten first keywords generated by RAKE. Comment on the quality of this summarizer based on your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repossa ollut asennus tiedosto ei kyennyt asentumaan windows ympäristössä ilman korjausta\n",
    "#git clone https://github.com/zelandiya/RAKE-tutorial\n",
    "#cd RAKE-tutorial\n",
    "\n",
    "#Ennen asennusta mene setup.py tiedostoon ja poista slash (/) poluista: \n",
    "#package_dir={'nlp_rake': './'} ja \n",
    "#package_data={'nlp_rake': ['data/']}\n",
    "\n",
    "#muutin \"nlp-rake\" nimen pelkäksi \"rake\" asennus tiedostossa.\n",
    "\n",
    "#kuva setup_korjaus löytyy githubista, jonka jälkeen paketin asennus toimii\n",
    "#python setup.py install \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asensin moduulin eri paikkaan kuin missä jupyter serveri polku, korjasin tällä polun\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/Markus/Documents/studies/NLP/RAKE-tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake \n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korjaa polku, tiedosto löytyy githubista\n",
    "rake_object = rake.Rake(\"C:/Users/Markus/Documents/studies/NLP/NLP/SmartStoplist.txt\", 5, 3, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample_file = open(\"C:/NLP/RAKE-tutorial/data/docs/fao_test/w2167e.txt\", 'r') #aseta teksti minkä haluat käsitellä\n",
    "#text = sample_file.read()\n",
    "sentenceList = rake.split_sentences(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nearest neighbor matching', 'matching binary features', 'large data sets', 'tree build time', 'nearest neighbor search', 'hierarchical clustering tree', 'nearest neighbor', 'data sets', 'build time', 'data set']\n"
     ]
    }
   ],
   "source": [
    "keywords = rake_object.run(article)\n",
    "#print(\"Keywords:\", keywords[0:10]) #10 ensimmäistä\n",
    "keywords_topten = []\n",
    "\n",
    "for i in range(10):\n",
    "    keywords_topten.append(keywords[i][0])\n",
    "    \n",
    "print(keywords_topten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " algorithm for performing fast nearest neighbor matching in large data sets can bring speed improvements of several\n"
     ]
    }
   ],
   "source": [
    "#extract sentences using keywords\n",
    "dct = {}\n",
    "for sentence in sentenceList:\n",
    "    dct[sentence] = sum(1 for word in keywords_topten if word in sentence)\n",
    "\n",
    "rake_sentences = [key for key,value in dct.items() if value == max(dct.values())]\n",
    "\n",
    "\n",
    "print(\"\\n\".join(rake_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in original text: 1873, summarized amount: 1\n"
     ]
    }
   ],
   "source": [
    "#Comparing results\n",
    "print(\"Sentences in original text: {}, summarized amount: {}\".format(len(sentenceList),len(rake_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment on results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7\n",
    "\n",
    "It is also suggested to explore alternative implementations with larger number of summarization approaches implemented- https://github.com/miso-belica/sumy. Show how each of the implemented summarizer behaves when inputted with the same document you used in previous case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/miso-belica/sumy\n",
    "#pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as LSASummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer as LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer as LuhnSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1 Nearest Neighbor Matching Algorithms.', 'After evaluating many different algorithms for approximate nearest neighbor search on data sets with a wide range of dimensionality [14][47], we have found that one of two\\xa0algorithms gave the best performance: the priority search k-means tree or the multiple randomized k-d trees.', 'The randomized k-d tree algorithm [13], is an approximate nearest neighbor search algorithm that builds multiple randomized k-d trees which are searched in parallel.', 'Our experiments have revealed that the optimal algorithm for approximate nearest neighbor search is highly dependent on several factors such as the data dimensionality, size and structure of the data set (whether there is any correlation between the features in the data set) and the desired search precision.', 'We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN [11] and LSH algorithms [29] on the first data set of 100,000 SIFT features.', '10 shows the difference in performance between the randomized kd-trees and the priority search k-means tree for one of the Winder/Brown patches data set.', '7 shows that the k-means tree can perform better for other data sets (especially for high precisions).', 'We use the Winder/Brown patches data set [53] to compare the nearest neighbor search performance of the hierarchical clustering tree to that of other well known nearest neighbor search algorithms.', 'Each point on the graph is computed using the best performing algorithm for that particular feature type (randomized kd-trees or priority search k-means tree for SIFT, SURF, image patches and the hierarchical clustering algorithm for BRIEF and ORB).', 'In order to scale to very large data sets, we use the approach of distributing the data to multiple machines in a compute cluster and perform the nearest neighbor search using all the machines in parallel.']\n",
      "\n",
      "\n",
      "['We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature.', 'In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper.', 'Having an efficient algorithm for performing fast nearest neighbor matching in large data sets can bring speed improvements of several orders of magnitude to many applications.', 'In practice, and in most of the nearest neighbor literature, setting the algorithm parameters is a manual process carried out by using various heuristics and rarely make use of more systematic approaches.', 'In a previous paper [14] we have proposed an automatic nearest neighbor algorithm configuration method by combining grid search with a finer grained Nelder-Mead downhill simplex optimization process [43].', 'The number of clusters K to use when partitioning the data at each node is a parameter of the algorithm, called the branching factor and choosing K is important for obtaining good search performance.', 'However, we have observed that even when using a small number of iterations, the nearest neighbor search performance is similar to that of the tree constructed by running the clustering until convergence, as illustrated by Fig.', 'In order to distribute the nearest neighbor matching on a compute cluster we implemented a Map-Reduce like algorithm using the message passing interface (MPI) specification.', 'All the previous experiments have shown that distributing the nearest neighbor search to multiple machines results in an overall increase in performance in addition to the advantage of being able to use more memory.', 'We address the issues arising when scaling to very large size data sets by proposing an algorithm for distributed nearest neighbor matching on compute clusters.']\n",
      "\n",
      "\n",
      "['However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data.', 'Examples of such problems include finding the best matches for local image features in large data sets [1][2] clustering local features into visual words using the k-means or similar algorithms [3], global image feature matching for scene recognition [4], human pose estimation [5], matching deformable shapes for object recognition [6] or performing normalized cross-correlation (NCC) to compare image patches in large data sets [7].', 'In this paper we evaluate the most promising nearest-neighbor search algorithms in the literature, propose new algorithms and improvements to existing ones, present a method for performing automatic algorithm selection and parameter optimization, and discuss the problem of scaling to very large data sets using compute clusters.', 'In this case, the search can be performed in several ways, depending on the number of neighbors returned and their distance to the query point: K-nearest neighbor (KNN) search where the goal is to find the closest K points from the query point and radius nearest neighbor search (RNN), where the goal is to find all the points located closer than some distance R from the query point.', 'The performance of hashing methods is highly dependent on the quality of the hashing functions they use and a large body of research has been targeted at improving hashing methods by using data-dependent hashing functions computed using various learning techniques: parameter sensitive hashing [5], spectral hashing [32], randomized LSH hashing from learned metrics [33], kernelized LSH [34], learnt binary embeddings [35], shift-invariant kernel hashing [36], semi-supervised hashing [37], optimized kernel hashing [38] and complementary hashing [39].', 'The trees are built in a similar manner to the classic k-d tree [9] [10], with the difference that where the classic kd-tree algorithm splits data on the dimension with the highest variance, for the randomized k-d trees the split dimension is chosen randomly\\xa0from the top ND dimensions with the highest variance.', 'Many algorithms suitable for matching vector based features, such as the randomized kd-tree and priority search k-means tree, are either not efficient or not suitable for matching binary features (for example, the priority search k-means tree requires the points to be in a vector space where their dimensions can be independently averaged).', 'In contrast to the priority search k-means tree presented above, for which using more than one tree did not bring significant improvements, we have found that building multiple hierarchical clustering trees and searching them in parallel using a common priority queue (the same approach that has been found to work well for randomized kd-trees [13]) resulted in significant improvements in the search performance.', 'Among the data sets used are the Winder/Brown patch data set [53] , data sets of randomly sampled data of different dimensionality, data sets of SIFT features of different sizes obtained by sampling from the CD cover data set of [24] as well as a data set of SIFT features extracted from the overlapping images forming panoramas.', 'We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN [11] and LSH algorithms [29] on the first data set of 100,000 SIFT features.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sumySummarize(article):\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizers = [LexRankSummarizer(stemmer), LSASummarizer(stemmer), LuhnSummarizer(stemmer)]\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer(LANGUAGE))\n",
    "    results = []\n",
    "    \n",
    "    for summarizer in summarizers:\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "        sentences = []\n",
    "        for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "            sentences.append(str(sentence))\n",
    "        results.append(sentences)\n",
    "    \n",
    "    return results\n",
    "\n",
    "sumySentences = sumySummarize(article)\n",
    "for sentences in sumySentences:\n",
    "    print(\"{}\\n\\n\".format(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 8\n",
    "\n",
    "Now we would like to compare the above summarizers and those in 3), 5) and 7) on a new dataset constructed as follows. First select an Elsevier journal of your own and select 10 papers highly ranked in the journal according to citation index (The journal papers should be well structured to contain Abstract, Introduction and Conclusion). \n",
    "\n",
    "For each of the ten papers, consider the introduction as the main document to seek to apply summarizer, and consider the Abstract and Conclusion as two golden summary of the document that you can use for assessment using ROUGE-1 and ROUGE-2 evaluation. \n",
    "\n",
    "Report in a table the evaluation score of each summarizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge 1 & 2 pisteytyts koodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "golds = []\n",
    "for i in range(1,11):\n",
    "    with open('C:/Users/Markus/Documents/studies/NLP/NLP/Data/Task8_articles/article{}.txt'.format(i), encoding=\"utf8\") as f:\n",
    "        text = f.readlines()\n",
    "        text = \" \".join(text)\n",
    "        res = text.split(\"\\n \\n\")\n",
    "        files.append(res[1])\n",
    "        temp = []\n",
    "        temp.append(res[0])\n",
    "        temp.append(res[2])\n",
    "        golds.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0.7864167178856791, 0.721709717097171), (0.8364620938628159, 0.7644508670520231), (0.8817796610169492, 0.770568278201866), (0.7937766931055522, 0.7442002442002442), (0.7781047675103001, 0.7234982332155477)], [(0.8145559210526315, 0.7194570135746606), (0.8654527061203844, 0.7520242914979757), (0.9138026607538803, 0.7900721020521353), (0.928252551020408, 0.8044033184428845), (0.8651053864168619, 0.7703842549203374)], [(0.77209797657082, 0.6369206180074587), (0.8163615560640732, 0.6645678305666858), (0.7537662337662338, 0.6577442827442828), (0.7364006100660905, 0.6223296032553408), (0.7260891337005508, 0.6445390781563126)], [(0.8496332518337408, 0.761986301369863), (0.9427586206896552, 0.8240165631469979), (0.847242206235012, 0.7624760076775432), (0.8669046404895462, 0.7910714285714285), (0.7898406374501992, 0.7140294938222399)], [(0.7668957617411227, 0.6478510028653295), (0.8256666666666667, 0.7068045363575717), (0.8474810213940649, 0.7261740331491713), (0.7615470228158041, 0.65228285077951), (0.6684356252884172, 0.569713758079409)], [(0.4908874801901743, 0.4304399524375743), (0.5229957805907173, 0.4525116082735332), (0.6093444227005871, 0.5237395986294665), (0.5762751520823585, 0.49789325842696636), (0.44045936395759716, 0.3955461293743372)], [(0.7102581521739131, 0.6267845003399048), (0.7275985663082437, 0.6506456241032998), (0.7878521126760564, 0.7079295154185022), (0.7184499314128944, 0.6455044612216884), (0.7386617100371747, 0.6555059523809523)], [(0.9300168634064081, 0.8298650168728909), (0.9628647214854111, 0.8506967485069675), (0.944962143273151, 0.8508158508158508), (0.9551829268292683, 0.8453325198291641), (0.9500580720092915, 0.8486345148169668)], [(0.805026455026455, 0.7379565907887772), (0.8732212160413971, 0.7896440129449839), (0.9160028964518465, 0.8155797101449276), (0.8444835680751174, 0.7739283617146212), (0.8467741935483871, 0.7698744769874477)], [(0.821067415730337, 0.7327150084317032), (0.8219525959367946, 0.7258610954263128), (0.863125, 0.776735459662289), (0.8444835680751174, 0.7739283617146212), (0.8240689271817676, 0.7513904338153504)]]\n"
     ]
    }
   ],
   "source": [
    "def summarize_introduction(introduction):\n",
    "    task_3_result = findTopSentences(introduction, 10, False)\n",
    "    task_5_result = sentence_to_sentence(findTopSentences(introduction, 20, False))\n",
    "    task_7_results = sumySummarize(introduction)\n",
    "    result = [task_3_result, task_5_result, task_7_results[0], task_7_results[1], task_7_results[2]]\n",
    "    #result = [task_5_result]\n",
    "    return result\n",
    "\n",
    "#peer str(summary), model list(gold_summaries)\n",
    "def calculate_rouge(peer, model):\n",
    "\n",
    "    rougeUno = rouge.rouge_1(peer, model, 1)\n",
    "    rougeBi = rouge.rouge_2(peer, model, 1)\n",
    "\n",
    "    return rougeUno,rougeBi\n",
    "\n",
    "task8_rouge_scores = []\n",
    "\n",
    "for i in range(10):\n",
    "    summarization_results = summarize_introduction(files[i])\n",
    "    results = []\n",
    "    for result in summarization_results:\n",
    "        result_str = \" \".join(result)\n",
    "        rouge_score = calculate_rouge(result_str, golds[i])\n",
    "        results.append(rouge_score)\n",
    "    task8_rouge_scores.append(results)\n",
    "        \n",
    "print(task8_rouge_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.7864167178856791, 0.721709717097171)</td>\n",
       "      <td>(0.8364620938628159, 0.7644508670520231)</td>\n",
       "      <td>(0.8817796610169492, 0.770568278201866)</td>\n",
       "      <td>(0.7937766931055522, 0.7442002442002442)</td>\n",
       "      <td>(0.7781047675103001, 0.7234982332155477)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.8145559210526315, 0.7194570135746606)</td>\n",
       "      <td>(0.8654527061203844, 0.7520242914979757)</td>\n",
       "      <td>(0.9138026607538803, 0.7900721020521353)</td>\n",
       "      <td>(0.928252551020408, 0.8044033184428845)</td>\n",
       "      <td>(0.8651053864168619, 0.7703842549203374)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.77209797657082, 0.6369206180074587)</td>\n",
       "      <td>(0.8163615560640732, 0.6645678305666858)</td>\n",
       "      <td>(0.7537662337662338, 0.6577442827442828)</td>\n",
       "      <td>(0.7364006100660905, 0.6223296032553408)</td>\n",
       "      <td>(0.7260891337005508, 0.6445390781563126)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.8496332518337408, 0.761986301369863)</td>\n",
       "      <td>(0.9427586206896552, 0.8240165631469979)</td>\n",
       "      <td>(0.847242206235012, 0.7624760076775432)</td>\n",
       "      <td>(0.8669046404895462, 0.7910714285714285)</td>\n",
       "      <td>(0.7898406374501992, 0.7140294938222399)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(0.7668957617411227, 0.6478510028653295)</td>\n",
       "      <td>(0.8256666666666667, 0.7068045363575717)</td>\n",
       "      <td>(0.8474810213940649, 0.7261740331491713)</td>\n",
       "      <td>(0.7615470228158041, 0.65228285077951)</td>\n",
       "      <td>(0.6684356252884172, 0.569713758079409)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(0.4908874801901743, 0.4304399524375743)</td>\n",
       "      <td>(0.5229957805907173, 0.4525116082735332)</td>\n",
       "      <td>(0.6093444227005871, 0.5237395986294665)</td>\n",
       "      <td>(0.5762751520823585, 0.49789325842696636)</td>\n",
       "      <td>(0.44045936395759716, 0.3955461293743372)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(0.7102581521739131, 0.6267845003399048)</td>\n",
       "      <td>(0.7275985663082437, 0.6506456241032998)</td>\n",
       "      <td>(0.7878521126760564, 0.7079295154185022)</td>\n",
       "      <td>(0.7184499314128944, 0.6455044612216884)</td>\n",
       "      <td>(0.7386617100371747, 0.6555059523809523)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(0.9300168634064081, 0.8298650168728909)</td>\n",
       "      <td>(0.9628647214854111, 0.8506967485069675)</td>\n",
       "      <td>(0.944962143273151, 0.8508158508158508)</td>\n",
       "      <td>(0.9551829268292683, 0.8453325198291641)</td>\n",
       "      <td>(0.9500580720092915, 0.8486345148169668)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(0.805026455026455, 0.7379565907887772)</td>\n",
       "      <td>(0.8732212160413971, 0.7896440129449839)</td>\n",
       "      <td>(0.9160028964518465, 0.8155797101449276)</td>\n",
       "      <td>(0.8444835680751174, 0.7739283617146212)</td>\n",
       "      <td>(0.8467741935483871, 0.7698744769874477)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(0.821067415730337, 0.7327150084317032)</td>\n",
       "      <td>(0.8219525959367946, 0.7258610954263128)</td>\n",
       "      <td>(0.863125, 0.776735459662289)</td>\n",
       "      <td>(0.8444835680751174, 0.7739283617146212)</td>\n",
       "      <td>(0.8240689271817676, 0.7513904338153504)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0  \\\n",
       "0   (0.7864167178856791, 0.721709717097171)   \n",
       "1  (0.8145559210526315, 0.7194570135746606)   \n",
       "2    (0.77209797657082, 0.6369206180074587)   \n",
       "3   (0.8496332518337408, 0.761986301369863)   \n",
       "4  (0.7668957617411227, 0.6478510028653295)   \n",
       "5  (0.4908874801901743, 0.4304399524375743)   \n",
       "6  (0.7102581521739131, 0.6267845003399048)   \n",
       "7  (0.9300168634064081, 0.8298650168728909)   \n",
       "8   (0.805026455026455, 0.7379565907887772)   \n",
       "9   (0.821067415730337, 0.7327150084317032)   \n",
       "\n",
       "                                          1  \\\n",
       "0  (0.8364620938628159, 0.7644508670520231)   \n",
       "1  (0.8654527061203844, 0.7520242914979757)   \n",
       "2  (0.8163615560640732, 0.6645678305666858)   \n",
       "3  (0.9427586206896552, 0.8240165631469979)   \n",
       "4  (0.8256666666666667, 0.7068045363575717)   \n",
       "5  (0.5229957805907173, 0.4525116082735332)   \n",
       "6  (0.7275985663082437, 0.6506456241032998)   \n",
       "7  (0.9628647214854111, 0.8506967485069675)   \n",
       "8  (0.8732212160413971, 0.7896440129449839)   \n",
       "9  (0.8219525959367946, 0.7258610954263128)   \n",
       "\n",
       "                                          2  \\\n",
       "0   (0.8817796610169492, 0.770568278201866)   \n",
       "1  (0.9138026607538803, 0.7900721020521353)   \n",
       "2  (0.7537662337662338, 0.6577442827442828)   \n",
       "3   (0.847242206235012, 0.7624760076775432)   \n",
       "4  (0.8474810213940649, 0.7261740331491713)   \n",
       "5  (0.6093444227005871, 0.5237395986294665)   \n",
       "6  (0.7878521126760564, 0.7079295154185022)   \n",
       "7   (0.944962143273151, 0.8508158508158508)   \n",
       "8  (0.9160028964518465, 0.8155797101449276)   \n",
       "9             (0.863125, 0.776735459662289)   \n",
       "\n",
       "                                           3  \\\n",
       "0   (0.7937766931055522, 0.7442002442002442)   \n",
       "1    (0.928252551020408, 0.8044033184428845)   \n",
       "2   (0.7364006100660905, 0.6223296032553408)   \n",
       "3   (0.8669046404895462, 0.7910714285714285)   \n",
       "4     (0.7615470228158041, 0.65228285077951)   \n",
       "5  (0.5762751520823585, 0.49789325842696636)   \n",
       "6   (0.7184499314128944, 0.6455044612216884)   \n",
       "7   (0.9551829268292683, 0.8453325198291641)   \n",
       "8   (0.8444835680751174, 0.7739283617146212)   \n",
       "9   (0.8444835680751174, 0.7739283617146212)   \n",
       "\n",
       "                                           4  \n",
       "0   (0.7781047675103001, 0.7234982332155477)  \n",
       "1   (0.8651053864168619, 0.7703842549203374)  \n",
       "2   (0.7260891337005508, 0.6445390781563126)  \n",
       "3   (0.7898406374501992, 0.7140294938222399)  \n",
       "4    (0.6684356252884172, 0.569713758079409)  \n",
       "5  (0.44045936395759716, 0.3955461293743372)  \n",
       "6   (0.7386617100371747, 0.6555059523809523)  \n",
       "7   (0.9500580720092915, 0.8486345148169668)  \n",
       "8   (0.8467741935483871, 0.7698744769874477)  \n",
       "9   (0.8240689271817676, 0.7513904338153504)  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(task8_rouge_scores)\n",
    "df.columns=['Sweight', 'Sentence to sentence', 'LexRank', 'LSA', 'Luhn']\n",
    "df.columns.names = ['Article number']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 9\n",
    "\n",
    "Design a simple GUI that allows the user to input a text or a link to a document to be summarized and output the summarizer according to 3), algorithms implemented in 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simpleGUI.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
