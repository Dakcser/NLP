{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 21: Automatic Summarization  \n",
    "\n",
    "We shall consider structured document containing a title, abstract and a set of subsections. We would like to build a text summarizer such that tracks important keywords in the document. For this purpose, the first step is identify these keywords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list\n",
    "# tarkista löytyykö: lxml, html5lib, requests, selenium, webdriver-manager\n",
    "# lisäohjeita task1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jos nltk ei löydy asenna -> ! pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#from nltk.cluster.util import cosine_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "Assume the initial input is given as html document (choose an example of your own), we hypothesize that important keywords are initially contained in the words of titles, abstract and possibly titles of subsections of the document. Suggest a simple python script that inputs an html document and outputs the lists of words in the title, abstract and title of section/subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "# Kaikki sivut ei anna koko html bodyä käyttämällä pelkkää requestia. Seleniumilla näyttää toimivan useammilla. \n",
    "# pip install -U selenium\n",
    "# pip install webdriver-manager\n",
    "# jos käytät anacondaa eikä meinaa toimia niin kokeile myös $ conda update pip\n",
    "\n",
    "# Collect title, subtitles, abstract and body text from html file.\n",
    "# Print out titles and abstract and construct one string based on\n",
    "# the elements.\n",
    "\n",
    "def _convertHtmlToStr(elements):\n",
    "    str = \"\"\n",
    "    for element in elements:\n",
    "        if len(element.text.split()) > 1:\n",
    "            str += element.text\n",
    "            if not str.endswith(\".\"):\n",
    "                str += \".\"\n",
    "            str += \" \"\n",
    "    sentences = sent_tokenize(str)\n",
    "    return str, len(sentences)\n",
    "\n",
    "url = \"https://ieeexplore.ieee.org/document/6809191\"\n",
    "article = \"\"\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for article to fully load\n",
    "time.sleep(3)\n",
    "\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "strElement = \"\"\n",
    "countTitle, countAbstract, countH2, countH3, countH4, countP = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "strElement, countTitle = _convertHtmlToStr(soup.find(\"h1\", {\"class\": \"document-title\"}))\n",
    "print(\"Title:\\n{}\\n\\n\".format(strElement))\n",
    "article += strElement\n",
    "article += \". \"\n",
    "strElement, countAbstract = _convertHtmlToStr(soup.find(\"div\", {\"class\": \"abstract-text\"}))\n",
    "print(\"Abstract:\\n{}\\n\\n\".format(strElement))\n",
    "article += strElement\n",
    "\n",
    "articleHtmlBody = soup.find(\"div\", {\"id\": \"article\"})\n",
    "if articleHtmlBody == None:\n",
    "    raise ValueError\n",
    "\n",
    "strElement, countH2 = _convertHtmlToStr(articleHtmlBody.find_all(\"h2\"))\n",
    "print(\"Section titles:\\n{}\\n\\n\".format(strElement))\n",
    "article += strElement\n",
    "strElement, countH3 = _convertHtmlToStr(articleHtmlBody.find_all(\"h3\"))\n",
    "print(\"Subsection titles:\\n{}\\n\\n\".format(strElement))\n",
    "article += strElement\n",
    "strElement, countH4 = _convertHtmlToStr(articleHtmlBody.find_all(\"h4\"))\n",
    "print(\"Subsubsection titles:\\n{}\\n\\n\".format(strElement))\n",
    "article += strElement\n",
    "strElement, countP = _convertHtmlToStr(articleHtmlBody.find_all(\"p\"))\n",
    "article += strElement\n",
    "countP += 1\n",
    "\n",
    "driver.close()\n",
    "\n",
    "counts = [countTitle, countAbstract, countH2, countH3, countH4, countP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keyword search and analysis\n",
    "\n",
    "w_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50 #alunperin 10\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(article)\n",
    "\n",
    "for kw in keywords:\n",
    "    print(kw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "Write down a simple python script that allows you to output the histogram of word frequency in the document, excluding the stopwords (see examples in online NLTK book). Use SpaCy named-entity tagger to identify person-named entities and organization-named entities in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = word_tokenize(article)\n",
    "tokens_without_sw = [word for word in text_tokens if word.isalpha() and word not in all_stopwords]\n",
    "\n",
    "print(tokens_without_sw)\n",
    "#print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### Count histogram from every word manually ###\n",
    "#charsToRemove = \".,()\"\n",
    "#wordCounts = Counter(tokens_without_sw)\n",
    "wordCounts = Counter(tokens_without_sw)\n",
    "wordCounts = wordCounts.most_common()\n",
    "\n",
    "print(wordCounts)\n",
    "\n",
    "wordCounts = wordCounts[0:20]\n",
    "\n",
    "words = list(zip(*wordCounts))[0]\n",
    "occurency = list(zip(*wordCounts))[1]\n",
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "plt.bar(np.arange(len(words)), occurency, align='center')\n",
    "plt.xticks(np.arange(len(words)), words, rotation='vertical')\n",
    "plt.ylabel('Keyword count')\n",
    "plt.xlabel('Keyword id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use SpaCy to identify person-named entities and organization-named entities\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "\n",
    "#vinkkiä https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da\n",
    "\n",
    "#Identifying person and organization-named entities\n",
    "wordsInStr = \"\"\n",
    "for word in tokens_without_sw:\n",
    "    wordsInStr += word\n",
    "    wordsInStr += \" \"\n",
    "    \n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(wordsInStr)\n",
    "\n",
    "#Print only ORG or PERSON labeled entities\n",
    "if doc.ents:\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" or ent.label_ == \"PERSON\":\n",
    "            print(ent.text+ \" - \" + ent.label_)\n",
    "else:\n",
    "    print(\"No named entities found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3\n",
    "\n",
    "We would like the summarizer to contain frequent wording (excluding stopwords) and as many named-entities as possible. For this purpose, use the following heuristic to construct the summarizer. First we shall assume each sentence of the document as individual sub-document. Use TfIdf vectorizer to output the individual tfidef score of each word of each sentence (after initial preprocessing and wordnet lemmatization stage). Then consider only sentences that contain person or organization named-entities and use similar approach to output the tfidf score of the named-entities in each sentence. Finally construct the sentence (S) weight as a  weighted sum:\n",
    "<br>\n",
    "$$S_{weight}=\\sum_{w\\varepsilon S}W_{TfiDf}+2\\sum_{NM\\varepsilon S}NM_{TfiDf}+POS_s$$\n",
    "<br>\n",
    "where NMTfiDF stands for the TfIdF of named-entity NM in sentence S.  POSS corresponds to the sentence weight associated to the location of the sentence. So that the sentence location weight will be maximum (1) if located in the title of the document, 0.5 if located  in the title of one of the subsection, 0.25 if located in the title one of the subsubsection, 0.1 if located in one representative object of the document, and 0 if located only in the main text. Make sure to normalize the term tfidf and Nm tfidf weights and suggest a script to implement the preceding accordingly, so that the summarizer will contain the 10 sentences with the highest Sweight scores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"1\": 1,\n",
    "    \"2\": 0.1,\n",
    "    \"3\": 0.5,\n",
    "    \"4\": 0.25,\n",
    "    \"else\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculateFullScores(sentenceScores, namedEntityScores, counts):\n",
    "    scaler = MinMaxScaler()\n",
    "    weightList= []\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        if counts[2] == 0:\n",
    "            counts.pop[2]\n",
    "    else:\n",
    "        counts = [0, 0, 0, 0, len(sentenceScores)]\n",
    "\n",
    "    for i in range(len(counts)):\n",
    "        for j in range(counts[i]):\n",
    "            if i > 3:\n",
    "                weightList.append(weights[\"else\"])\n",
    "            else:\n",
    "                weightList.append(weights[str(i+1)])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Weights\": weightList,\n",
    "        \"SentenceScores\": sentenceScores,\n",
    "        \"EntityScores\": namedEntityScores,\n",
    "    })\n",
    "\n",
    "    df[[\"SentencesScaled\"]] = scaler.fit_transform(df[[\"SentenceScores\"]])\n",
    "    df[[\"EntitiesScaled\"]] = scaler.fit_transform(df[[\"EntityScores\"]])\n",
    "    df[\"S_weight\"] = df[\"SentencesScaled\"] + (2 * df[\"EntitiesScaled\"]) + df[\"Weights\"]\n",
    "\n",
    "    return df[\"S_weight\"].tolist()\n",
    "\n",
    "\n",
    "def _getNamedEntities(article):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    doc = nlp(article)\n",
    "    namedEntities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" or ent.label_ == \"PERSON\":\n",
    "                namedEntities.append(ent.text)\n",
    "\n",
    "    return namedEntities\n",
    "\n",
    "\n",
    "def _getSentencesWithMaxWeights(weights, sentences, numberOfSentences):\n",
    "    arr = np.array(weights)\n",
    "    indexes = np.argpartition(arr, -numberOfSentences)[-numberOfSentences:]\n",
    "    sentences = np.array(sentences)\n",
    "    return sentences[indexes]\n",
    "\n",
    "\n",
    "def _preProcess(document):\n",
    "    stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    WN_lemmatizer = WordNetLemmatizer()\n",
    "    sentences = sent_tokenize(document)\n",
    "    processedSentences = []\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [WN_lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "\n",
    "        # get rid of numbers and Stopwords\n",
    "        words = [word for word in words if word.isalpha() and word not in stopwords]\n",
    "        processedSentences.append(' '.join(word for word in words))\n",
    "        tokens.extend(words)\n",
    "\n",
    "    return processedSentences, tokens\n",
    "\n",
    "\n",
    "def _tfidfScores(corpus, sentences):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    fittedVectorizer = tfidf.fit(corpus)\n",
    "    vectors = fittedVectorizer.transform(sentences).toarray()\n",
    "\n",
    "    scores = []\n",
    "    for i in range(len(vectors)):\n",
    "        score = 0\n",
    "        for j in range(len(vectors[i])):\n",
    "            score = score + vectors[i][j]\n",
    "\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTopSentences(document, numberOfSentences, isUrl):\n",
    "    sentences, tokens = _preProcess(document)\n",
    "    sentenceTfidfScores = _tfidfScores(tokens, sentences)\n",
    "    namedEntitiesTfidfScores = _tfidfScores(_getNamedEntities(document), sentences)\n",
    "    if isUrl:\n",
    "        SWeight = _calculateFullScores(sentenceTfidfScores, namedEntitiesTfidfScores, counts)\n",
    "    else:\n",
    "        SWeight = _calculateFullScores(sentenceTfidfScores, namedEntitiesTfidfScores, [])\n",
    "    topSentences = _getSentencesWithMaxWeights(SWeight, sent_tokenize(document), numberOfSentences)\n",
    "    return topSentences\n",
    "\n",
    "topSentences = findTopSentences(article, 10, True)\n",
    "for sentence in topSentences:\n",
    "    print(\"{}\\n\".format(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDoc = \"\"\n",
    "with open(\"C:/Users/Markus/Documents/studies/NLP/NLP/accuracy_garmin_nuvi_255W_gps.data\") as f:\n",
    "    testDoc = f.readlines()\n",
    "f.close()\n",
    "testDoc = \" \".join(testDoc)\n",
    "\n",
    "s = findTopSentences(testDoc, 20, False)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TASK 4\n",
    "Test the above approach with Opinosis dataset available at https://kavita-ganesan.com/opinosis-opinion-dataset/#.YVw6J5ozY2x,  and record the corresponding Rouge-2 and Rouge-3 evaluation score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge 2 ja 3 scoring\n",
    "https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install git+git://github.com/bdusell/rougescore.git\n",
    "\n",
    "'''\n",
    "In ROUGE, a \"peer\" summary produced by a machine summarization system is compared against \n",
    "one or more hand-written \"model\" summaries and then assigned a score from 0 to 1. This score is the\n",
    "F-measure of recall vs. precision, and the evaluator can adjust a parameter α to control whether this \n",
    "score favors recall (does the peer summary contain all of the information in the model summaries?) \n",
    "or precision (does the peer summary contain only information in the model summaries?). \n",
    "\n",
    "When α ≈ 0, this score favors recall; when α ≈ 1, it favors precision. \n",
    "In the DUC conferences, α was set to 0, and a hard length limit was imposed on generated summaries. \n",
    "The original ROUGE implementation uses α = 0.5 by default.\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testi teksti\n",
    "peer = \"\"\"\n",
    ", and is very, very accurate .\n",
    " but for the most part, we find that the Garmin software provides accurate directions, whereever we intend to go .\n",
    " This function is not accurate if you don't leave it in battery mode say, when you stop at the Cracker Barrell for lunch and to play one of those trangle games with the tees .\n",
    " It provides immediate alternatives if the route from the online map program was inaccurate or blocked by an obstacle .\n",
    " I've used other GPS units, as well as GPS built into cars   and to this day NOTHING beats the accuracy of a Garmin GPS .\n",
    " It got me from point A to point B with 100% accuracy everytime .\n",
    " It has yet to disappoint, getting me everywhere with 100% accuracy .\n",
    "0 out of 5 stars Honest, accurate review, , PLEASE READ !\n",
    " Aside from that, every destination I've thrown at has been 100% accurate .\n",
    "In closing, this is a fantastic GPS with some very nice features and is very accurate in directions .\n",
    " Plus, I've always heard that there are  quirks  with any GPS being accurate, having POIs, etc .\n",
    " DESTINATION TIME, , This is pretty accurate too .\n",
    " But, it's always very accurate .\n",
    " The map is pretty accurate and the Point of interest database also is good .\n",
    " Most of the times, this info was very accurate .\n",
    "I've even used it in the  pedestrian  mode, and it's amazing how accurate it is .\n",
    "  ONLY  is only accurate when an ad says,  Top sirloin steak, ONLY $1 .\n",
    " The most accurate review stated that these machines are adjunct to a good map and signs on the interstate .\n",
    " The directions are highly accurate down to a  T  .\n",
    " Depending on what you are using it for, it is a nice adjunct to a travel trip and the directions are accurate and usually the quickest, but not always .\n",
    " The screen is easy to see, the voice tells you where you are and it's very accurate .\n",
    " It was accurate to the minute when it told me when I would arrive home .\n",
    "0 out of 5 stars GPS Navigator doesn't navigate accurately on a straight road .\n",
    " I was familiar with the streets and only used the Nuvi to get an accurate arrival time estimate .\n",
    " but after that it is very easy and quite accurate to use .\n",
    " The accuracy at this point is very good .\n",
    "While the 255W routing seems generally accurate and logical, on my first use I discovered that it does have some errors in its internal map .\n",
    " Bottom line is I wanted a unit that is accurate and had reliable satellite connection .\n",
    " I've used it around town and find it to be extremely accurate .\n",
    "I found the maps to be inaccurate at first, but after I updated them from Garmin's website everything is golden .\n",
    " A lot of my friends' addresses are inaccurate by any GPS .\n",
    " It loads quickly, have pretty accurate directions, and can recalculate quickly when I miss a turn .\n",
    " Because the accuracy is good to the street address level, it may not be able to guide you to the exact location if your destination is inside a shopping mall .\n",
    "I updated to the latest 2010 map soon after I received the unit, so the map is accurate to me .\n",
    " I was blown away at the accuracy and routing capability this thing had .\n",
    " I used it the day I bought it,   and then this morning, and as soon as it comes on it is  ready to navigate  The only downfall of this product, and the only reason I did not give it 5 stars is the fact that the speed limit it displays for the road you are on isn't 100% accurate .\n",
    " If your looking for a nice, accurate GPS for not so much money, got with this one .\n",
    "0 out of 5 stars Inexpensive, accurate, plenty of features, August 6, 2009\n",
    " The only glitch I have found so far is that the speed limits are not 100% accurate, although the GPS, amazingly, is able to very accurately tell you how fast your vehicle is moving .\n",
    " I was a little disappointed in the inaccuracy of the posted speed limit, as I'm guilty of not paying close enough attention to those signs, especially w  interstate speed traps that are constantly changing up and down .\n",
    " The closest one that gives the most accurate route that I usually take is the Navigon .\n",
    " After 2 weeks, it has yet to make a mistake, and is always completely accurate ,  even to the point of telling me which side of the street my destination is on .\n",
    " It has worked well for local driving giving accurate directions for roads and streets .\n",
    "The estimated time to arrival does not seem to calculate the travelling time accurately .\n",
    "Accuracy is as good as any other unit, they all sometimes tell you you have arrived when you haven't, or continue to tell you to turn when you're already there .\n",
    " Accuracy is determined by the maps .\n",
    " Less traveled rural roads will not be accurate on any unit .\n",
    " Accuracy is within a few yards .\n",
    "What the 255w does best is find a street address, business, point of interest, hospital or airport and give you turn, by, turn directions with amazing accuracy .\n",
    " The Garmin is loaded with very accurate maps that generally know the roads in even the remotest areas .\n",
    "I'm really glad I bought it though, and like the easy to read graphics, the voice used to tell you the name of the street you are to turn on, the uncannily accurate estimates of mileage and time of arrival at your destination .\n",
    "My new Garmin 255w had very Easy Set Up, Accurate Directions to locations, User Friendly Unit to anyone in my vehicle who tried it .\n",
    " I had a GPS 10, years ago when I owned a boat that was difficult to use and with very poor accuracy so I had assumed that the road GPS wasn't any better .\n",
    " Practiced visiting places I already knew to see how accurate the directions and maps would be .\n",
    " Easy to use, excellent accuracy, nice and intuitive interface .\n",
    " The directions provided have all been quite accurate thus far .\n",
    ",  Very Accurate but with one small glitch I found ,  I'll explain in the CONS\n",
    "This is a great GPS, it is so easy to use and it is always accurate .\n",
    "Very easy to operate and pretty accurate as well, only led me astray once and that was in northern Maine where roads are few and paved ones fewer .\n",
    " Easy to use and amazed at how accurate this item is .\n",
    "To date it's been a very easy to use and accurate .\n",
    " Mounted really easily and has been very accurate .\n",
    " seems to be rather accurate .\n",
    " It was accurate on determing original directions and recalculating when necessary .\n",
    "Highly accurate, POIs are great .\n",
    " I can't believe how accurate and detailed the information estimated time of arrival,speed limits along the way,and detailed map of my route, to name a few .\n",
    " Speed of calculation, accuracy, and simplicity of operation are top notch .\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "model = [\n",
    "    \"\"\"The voice is a bit robotic.\n",
    "    The voice is very clear and loud enough.\"\"\",\n",
    "    \"\"\"Voice is clear and sweet.\n",
    "    Voice commands are kindly fantastic.\"\"\",\n",
    "    \"\"\"The voice is very clear and loud.\"\"\",\n",
    "    \"\"\"The voices sound robotic.\n",
    "    TTS mode is the most problematic.\"\"\",\n",
    "    \"\"\"255W garmin gps has more than 750 voices but the most of them sound like robots.\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rougescore as rouge\n",
    "\n",
    "rougeBi = rouge.rouge_2(peer, model, 1)\n",
    "print(\"Rouge 2: \", rougeBi)\n",
    "rougeTri = rouge.rouge_3(peer, model, 1)\n",
    "print(\"Rouge 3: \", rougeTri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tulosten vertailu toisella laskurilla\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge2', 'rouge3'], use_stemmer=False)\n",
    "scores = scorer.score(peer, \"The voice is a bit robotic. The voice is very clear and loud enough.\")\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5\n",
    "\n",
    "[x] We would like to improve the summarization by taking into account the diversity among the sentence in the sense that we would like to minimize redundancy among sentences. For this purpose, we shall use the sentence-to-sentence semantic similarity introduced in the NLP lab. \n",
    "\n",
    "[x] Next, instead of recording only the 10 sentences with highest Sweight scores, we shall record the 20 top sentences in terms of $S_{weight}$ scores. Then the selection of the top 10 sentences among the 20 sentences follows the following approach. \n",
    "\n",
    "[x] First, order the 20 sentences in the decreasing order of their $S_{weight}$ scores, say S1, S2, …, S20 (where S1 is the top ranked and S20 the 20th ranked sentence). \n",
    "\n",
    "[x] Second, we shall assume that S1 is always included in the summarizer, we shall then attempt to find the other sentences among S2 till S20 to be included into the summarizer. \n",
    "\n",
    "[x] Calculate the sentence-to-sentence similarity Sim(S1,Si) for i=1 to 20, the Sentence Sj that yields the minimum similarity with S1 will therefore be included in the summarizer. \n",
    "\n",
    "[x] Next, for each of the remaining sentences Sk (with k different from 1 and j), we calculate the sentence similarity with Sj. Therefore the sentence Sp that yields minimum value of “Sim(Sp, S1)+Sim(Sp,Sj)” will be included in the summarizer (Note: the quantity Sim(Sp, S1) is already calculated in previous step).  \n",
    "\n",
    "[x] Similarly in the next phase, we should select a sentence Sl (l different from 1, j and k) so that  “Sim(Sl, S1)+Sim(Sl,Sj)+Sim(Sl,Sp)”, Etc.. \n",
    "\n",
    "[x] You then stop once you reached 10 sentences included in the summarizer. \n",
    "\n",
    "[ ] Suggest a script that includes this process.. and illustrate its functioning in the example you chosen in 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kirjoitan tähän itselleni että pysyn ohjeiden perässä\n",
    "1.Luo 20 lauseen lista, missä lauseiden s(weight) pisteet ovat suurimmat (s1,s2,s3,...,s20)\n",
    "2.s1 on tiivistelmän ensimmäinen lause \n",
    "    2.1 poista s1 listalta\n",
    "3.Vertaa loppuja lauseita s1. Lause joka on vähiten samanlainen s1 kanssa lisätään tiivistelmään, ja kutsutaan s(j)\n",
    "    3.1 poista s(j) listalta\n",
    "4.Vertaa loppuja lauseita s(j) ja taas alin arvo lisätään tiivistelmään. Lisätty lause s(p)\n",
    "    4.1 poista lause"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"There have been days when I wished to be separated from my body, but today wasn’t one of those days.\",\n",
    "    \"There are no heroes in a punk rock band.\",\n",
    "    \"In the end, he realized he could see sound and hear words.\",\n",
    "    \"She had a habit of taking showers in lemonade.\",\n",
    "    \"He hated that he loved what she hated about hate.\",\n",
    "    \"Mr. Montoya knows the way to the bakery even though he's never been there.\",\n",
    "    \"Karen realized the only way she was getting into heaven was to cheat.\",\n",
    "    \"Thirty years later, she still thought it was okay to put the toilet paper roll under rather than over.\",\n",
    "    \"He appeared to be confusingly perplexed.\",\n",
    "    \"Sometimes you have to just give up and win by cheating.\",\n",
    "    \"It's never been my responsibility to glaze the donuts.\",\n",
    "    \"Mom didn’t understand why no one else wanted a hot tub full of jello.\",\n",
    "    \"He poured rocks in the dungeon of his mind.\",\n",
    "    \"He is good at eating pickles and telling women about his emotional problems.\",\n",
    "    \"He picked up trash in his spare time to dump in his neighbor's yard.\",\n",
    "    \"I thought red would have felt warmer in summer but I didn't think about the equator.\",\n",
    "    \"The family’s excitement over going to Disneyland was crazier than she anticipated.\",\n",
    "    \"You're good at English when you know the difference between a man eating chicken and a man-eating chicken.\",\n",
    "    \"With the high wind warning\",\n",
    "    \"This made him feel like an old-style rootbeer float smells.\"\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download larger pipeline package for spaCy\n",
    "python -m spacy download en_core_web_lg #tarkempi mutta 770mb kokoinen\n",
    "\n",
    "python -m spacy download en_core_web_sm #paljon pienempi mutta ei yhtä tarkka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 määritys\n",
    "picked_sentences = []\n",
    "\n",
    "#choose dictionary\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "#nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "#löydä ensimmäinen lause, korkein s(weigth)\n",
    "for sentence in sentences: \n",
    "    #lisää koodi s(weight) laskemiseen, tai valitse ensimmäinen lause jos lista on järjestyksessä\n",
    "    s1 = sentence\n",
    "\n",
    "#poista valinta listasta ja lisää tiivistelmä listaan    \n",
    "picked_sentences.append(s1)\n",
    "sentences.remove(s1)\n",
    "\n",
    "print(picked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loppujen yhdeksän lauseen valinta\n",
    "\n",
    "#lista samanlaisuus pisteistä\n",
    "sim_score = []\n",
    "\n",
    "#while pyörii kunnes 10 lausetta on löydetty\n",
    "while(len(picked_sentences)<10):\n",
    "    sim_score.clear()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        nlp_sentence = nlp(sentence)\n",
    "        score = 0\n",
    "\n",
    "        for p_sentence in picked_sentences:\n",
    "            #vertaa kahta lausetta\n",
    "            nlp_p_sentence = nlp(p_sentence)\n",
    "\n",
    "            score += nlp_p_sentence.similarity(nlp_sentence)\n",
    "                \n",
    "        sim_score.append(score)\n",
    "        \n",
    "        \n",
    "    print(sim_score)\n",
    "    min_value = min(sim_score)\n",
    "    min_index = sim_score.index(min_value)   \n",
    "\n",
    "    print(\"Sentences left in the list: \" + str(len(sentences)))\n",
    "    print(\"Smallest value: \" + str(min_value))\n",
    "    print(sentences[min_index])\n",
    "\n",
    "    picked_sentences.append(sentences[min_index])\n",
    "    sentences.remove(sentences[min_index])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summarized text\")\n",
    "print(picked_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6\n",
    "\n",
    "We would like to make the choice of keywords not based on histogram frequency but using the open source RAKE https://www.airpair.com/nlp/keyword-extraction-tutorial. Repeat the previous process of selecting the sentences that are associated to the ten first keywords generated by RAKE. Comment on the quality of this summarizer based on your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repossa ollut asennus tiedosto ei kyennyt asentumaan windows ympäristössä ilman korjausta\n",
    "git clone https://github.com/zelandiya/RAKE-tutorial\n",
    "cd RAKE-tutorial\n",
    "\n",
    "#Ennen asennusta mene setup.py tiedostoon ja poista slash (/) poluista: \n",
    "#package_dir={'nlp_rake': './'} ja \n",
    "#package_data={'nlp_rake': ['data/']}\n",
    "\n",
    "#muutin \"nlp-rake\" nimen pelkäksi \"rake\" asennus tiedostossa.\n",
    "\n",
    "#kuva setup_korjaus löytyy githubista, jonka jälkeen paketin asennus toimii\n",
    "python setup.py install \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Asensin moduulin eri paikkaan kuin missä jupyter serveri polku, korjasin tällä polun\n",
    "#import sys \n",
    "#sys.path.append(\"C:/NLP/RAKE-tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake \n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Korjaa polku, tiedosto löytyy githubista\n",
    "rake_object = rake.Rake(\"C:/NLP/RAKE-tutorial/data/stoplists/SmartStoplist.txt\", 5, 3, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_file = open(\"C:/NLP/RAKE-tutorial/data/docs/fao_test/w2167e.txt\", 'r') #aseta teksti minkä haluat käsitellä\n",
    "text = sample_file.read()\n",
    "sentenceList = rake.split_sentences(text)\n",
    "print(sentenceList[0:1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = rake_object.run(text)\n",
    "#print(\"Keywords:\", keywords[0:10]) #10 ensimmäistä\n",
    "keywords_topten = []\n",
    "\n",
    "for i in range(10):\n",
    "    keywords_topten.append(keywords[i][0])\n",
    "    \n",
    "print(keywords_topten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract sentences using keywords\n",
    "dct = {}\n",
    "for sentence in sentenceList:\n",
    "    dct[sentence] = sum(1 for word in keywords_topten if word in sentence)\n",
    "\n",
    "rake_sentences = [key for key,value in dct.items() if value == max(dct.values())]\n",
    "\n",
    "\n",
    "print(\"\\n\".join(rake_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing results\n",
    "print(\"Sentences in original text: {}, summarized amount: {}\".format(len(sentenceList),len(rake_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Comment on results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7\n",
    "\n",
    "It is also suggested to explore alternative implementations with larger number of summarization approaches implemented- https://github.com/miso-belica/sumy. Show how each of the implemented summarizer behaves when inputted with the same document you used in previous case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/miso-belica/sumy\n",
    "#pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as LSASummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer as LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer as LuhnSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sumySummarize(article):\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizers = [LexRankSummarizer(stemmer), LSASummarizer(stemmer), LuhnSummarizer(stemmer)]\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer(LANGUAGE))\n",
    "    results = []\n",
    "    \n",
    "    for summarizer in summarizers:\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "        sentences = []\n",
    "        for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "            sentences.append(sentence)\n",
    "        results.append(sentences)\n",
    "    \n",
    "    return results\n",
    "\n",
    "sumySentences = sumySummarize(article)\n",
    "for sentences in sumySentences:\n",
    "    print(\"{}\\n\\n\".format(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 8\n",
    "\n",
    "Now we would like to compare the above summarizers and those in 3), 5) and 7) on a new dataset constructed as follows. First select an Elsevier journal of your own and select 10 papers highly ranked in the journal according to citation index (The journal papers should be well structured to contain Abstract, Introduction and Conclusion). \n",
    "\n",
    "For each of the ten papers, consider the introduction as the main document to seek to apply summarizer, and consider the Abstract and Conclusion as two golden summary of the document that you can use for assessment using ROUGE-1 and ROUGE-2 evaluation. \n",
    "\n",
    "Report in a table the evaluation score of each summarizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rouge 1 & 2 pisteytyts koodi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 9\n",
    "\n",
    "Design a simple GUI that allows the user to input a text or a link to a document to be summarized and output the summarizer according to 3), algorithms implemented in 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simpleGUI.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
