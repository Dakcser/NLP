{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project 21: Automatic Summarization  \n",
    "\n",
    "We shall consider structured document containing a title, abstract and a set of subsections. We would like to build a text summarizer such that tracks important keywords in the document. For this purpose, the first step is identify these keywords.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Markus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1\n",
    "Assume the initial input is given as html document (choose an example of your own), we hypothesize that important keywords are initially contained in the words of titles, abstract and possibly titles of subsections of the document. Suggest a simple python script that inputs an html document and outputs the lists of words in the title, abstract and title of section/subsections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\Markus\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:27: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:\n",
      "Scalable Nearest Neighbor Algorithms for High Dimensional Data. \n",
      "\n",
      "\n",
      "Abstract:\n",
      " Abstract:For many computer vision and machine learning problems, large training sets are key for good performance. However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data. We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms. For matching high dimensional features, we find two algorithms to be the most efficient: the randomized k-d forest and a new algorithm proposed in this paper, the priority search k-means tree. We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature. We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set. In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper. All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching. \n",
      "\n",
      "\n",
      "Section titles:\n",
      "Fast Approximate NN Matching. Scaling Nearest Neighbor Search. The FLANN Library. \n",
      "\n",
      "\n",
      "Subsection titles:\n",
      "1.1 Definitions and Notation. 2.1 Nearest Neighbor Matching Algorithms. 2.2 Automatic Configuration of NN Algorithms. 3.1 The Randomized k-d Tree Algorithm. 3.2 The Priority Search K-Means Tree Algorithm. 3.3 The Hierarchical Clustering Tree. 3.4 Automatic Selection of the Optimal Algorithm. 4.1 Fast Approximate Nearest Neighbor Search. 4.2 Binary Features. 5.1 Searching on a Compute Cluster. 5.2 Evaluation of Distributed Search. \n",
      "\n",
      "\n",
      "Subsubsection titles:\n",
      "2.1.1 Partitioning Trees. 2.1.2 Hashing Based Nearest Neighbor Techniques. 2.1.3 Nearest Neighbor Graph Techniques. 3.2.1 Algorithm Description. 3.2.2 Analysis. 4.1.1 Data Dimensionality. 4.1.2 Search Precision. 4.1.3 Automatic Selection of Optimal Algorithm. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import time\n",
    "# pip install -U selenium\n",
    "# pip install webdriver-manager\n",
    "\n",
    "# Collect title, subtitles, abstract and body text from html file.\n",
    "# Print out titles and abstract and construct one string based on\n",
    "# the elements.\n",
    "\n",
    "def _convertHtmlToStr(elements):\n",
    "    str = \"\"\n",
    "    for element in elements:\n",
    "        # Do not add if len is 1\n",
    "        if len(element.text.split()) > 1:\n",
    "            str += element.text\n",
    "            if not str.endswith(\".\"):\n",
    "                str += \".\"\n",
    "            str += \" \"\n",
    "    sentences = sent_tokenize(str)\n",
    "    return str, len(sentences)\n",
    "\n",
    "def scrape_article(url):\n",
    "    article = \"\"\n",
    "    driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for article to fully load\n",
    "    time.sleep(3)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    strElement = \"\"\n",
    "    countTitle, countAbstract, countH2, countH3, countH4, countP = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "    strElement, countTitle = _convertHtmlToStr(soup.find(\"h1\", {\"class\": \"document-title\"}))\n",
    "    print(\"Title:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "    article += \". \"\n",
    "    strElement, countAbstract = _convertHtmlToStr(soup.find(\"div\", {\"class\": \"abstract-text\"}))\n",
    "    print(\"Abstract:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "\n",
    "    articleHtmlBody = soup.find(\"div\", {\"id\": \"article\"})\n",
    "    if articleHtmlBody == None:\n",
    "        raise ValueError\n",
    "\n",
    "    strElement, countH2 = _convertHtmlToStr(articleHtmlBody.find_all(\"h2\"))\n",
    "    print(\"Section titles:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "    strElement, countH3 = _convertHtmlToStr(articleHtmlBody.find_all(\"h3\"))\n",
    "    print(\"Subsection titles:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "    strElement, countH4 = _convertHtmlToStr(articleHtmlBody.find_all(\"h4\"))\n",
    "    print(\"Subsubsection titles:\\n{}\\n\\n\".format(strElement))\n",
    "    article += strElement\n",
    "    strElement, countP = _convertHtmlToStr(articleHtmlBody.find_all(\"p\"))\n",
    "    article += strElement\n",
    "    countP += 1\n",
    "\n",
    "    driver.close()\n",
    "\n",
    "    counts = [countTitle, countAbstract, countH2, countH3, countH4, countP]\n",
    "    return article, counts\n",
    "\n",
    "# Example article\n",
    "url = \"https://ieeexplore.ieee.org/document/6809191\"\n",
    "article, counts = scrape_article(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not asked in task\n",
    "# Keyword search and analysis by tesking yake library\n",
    "\n",
    "w_extractor = yake.KeywordExtractor()\n",
    "\n",
    "language = \"en\"\n",
    "max_ngram_size = 2\n",
    "deduplication_threshold = 0.9\n",
    "numOfKeywords = 50 #alunperin 10\n",
    "\n",
    "custom_kw_extractor = yake.KeywordExtractor(lan=language, n=max_ngram_size, dedupLim=deduplication_threshold, top=numOfKeywords, features=None)\n",
    "keywords = custom_kw_extractor.extract_keywords(article)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2\n",
    "Write down a simple python script that allows you to output the histogram of word frequency in the document, excluding the stopwords (see examples in online NLTK book). Use SpaCy named-entity tagger to identify person-named entities and organization-named entities in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "all_stopwords = stopwords.words('english')\n",
    "all_stopwords.append('The')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = word_tokenize(article)\n",
    "tokens_without_sw = [word for word in text_tokens if word.isalpha() and word not in all_stopwords]\n",
    "#print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "### Count histogram from every word manually ###\n",
    "wordCounts = Counter(tokens_without_sw)\n",
    "wordCounts = wordCounts.most_common()\n",
    "\n",
    "wordCounts = wordCounts[0:20]\n",
    "\n",
    "words = list(zip(*wordCounts))[0]\n",
    "occurency = list(zip(*wordCounts))[1]\n",
    "fig, ax = plt.subplots(figsize=(18,5))\n",
    "plt.bar(np.arange(len(words)), occurency, align='center')\n",
    "plt.xticks(np.arange(len(words)), words, rotation='vertical')\n",
    "plt.ylabel('Keyword count')\n",
    "plt.xlabel('Keyword id')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NCC - ORG\n",
      "NN - ORG\n",
      "NN - ORG\n",
      "KNN - ORG\n",
      "RNN - ORG\n",
      "KNN - ORG\n",
      "P K View Source KNN - ORG\n",
      "RKNN - ORG\n",
      "RKNN - ORG\n",
      "Variations - ORG\n",
      "Leibe - PERSON\n",
      "Schindler - PERSON\n",
      "Jégou - PERSON\n",
      "Babenko Lempitsky - PERSON\n",
      "LSH - ORG\n",
      "Variants LSH LSH - ORG\n",
      "LSH Forest - ORG\n",
      "LSH - ORG\n",
      "Nearest - ORG\n",
      "Wang - PERSON\n",
      "Bawa - PERSON\n",
      "LSH Forest - ORG\n",
      "downhill simplex - PERSON\n",
      "Bergstra Bengio - PERSON\n",
      "Exact - PERSON\n",
      "Fig - PERSON\n",
      "Fig - ORG\n",
      "Fig - PERSON\n",
      "ORB - ORG\n",
      "Hamming - ORG\n",
      "Depending - PERSON\n",
      "Section Data - ORG\n",
      "Fig - ORG\n",
      "Fig - ORG\n",
      "Fig - PERSON\n",
      "Fig - PERSON\n",
      "Trevi - PERSON\n",
      "Trevi Fountain - PERSON\n",
      "Fig - PERSON\n",
      "SIFT - ORG\n",
      "SIFT - PERSON\n",
      "Fig - PERSON\n",
      "Fig ANN - PERSON\n",
      "Comparison - ORG\n",
      "SIFT - ORG\n",
      "Trevi Fountain - ORG\n",
      "SIFT SURF - PERSON\n",
      "ORB - ORG\n",
      "Fig - ORG\n",
      "SIFT SURF - PERSON\n",
      "ORB - ORG\n",
      "ORB - ORG\n",
      "Fig LSH - PERSON\n",
      "Comparison - ORG\n",
      "Fitting - ORG\n",
      "N - ORG\n",
      "MPI - ORG\n",
      "Fig - PERSON\n",
      "MPI - ORG\n",
      "Aly - PERSON\n",
      "LSH - ORG\n",
      "MPI - ORG\n",
      "Fig - ORG\n",
      "MPI - ORG\n",
      "Fig - PERSON\n",
      "Fig - ORG\n",
      "Direct - ORG\n",
      "Fig - ORG\n",
      "N - ORG\n",
      "N times - ORG\n",
      "Fast Library Approximate Nearest Neighbors FLANN - ORG\n",
      "FLANN - PERSON\n",
      "PCL - ORG\n",
      "ROS FLANN - ORG\n",
      "Ubuntu Fedora Arch Gentoo - PERSON\n"
     ]
    }
   ],
   "source": [
    "# Use SpaCy to identify person-named entities and organization-named entities\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import en_core_web_sm\n",
    "\n",
    "# Identifying person and organization-named entities\n",
    "wordsInStr = \"\"\n",
    "for word in tokens_without_sw:\n",
    "    wordsInStr += word\n",
    "    wordsInStr += \" \"\n",
    "    \n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp(wordsInStr)\n",
    "\n",
    "# Print only ORG or PERSON labeled entities\n",
    "if doc.ents:\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" or ent.label_ == \"PERSON\":\n",
    "            print(ent.text+ \" - \" + ent.label_)\n",
    "else:\n",
    "    print(\"No named entities found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 3\n",
    "\n",
    "We would like the summarizer to contain frequent wording (excluding stopwords) and as many named-entities as possible. For this purpose, use the following heuristic to construct the summarizer. First we shall assume each sentence of the document as individual sub-document. Use TfIdf vectorizer to output the individual tfidef score of each word of each sentence (after initial preprocessing and wordnet lemmatization stage). Then consider only sentences that contain person or organization named-entities and use similar approach to output the tfidf score of the named-entities in each sentence. Finally construct the sentence (S) weight as a  weighted sum:\n",
    "<br>\n",
    "$$S_{weight}=\\sum_{w\\varepsilon S}W_{TfiDf}+2\\sum_{NM\\varepsilon S}NM_{TfiDf}+POS_s$$\n",
    "<br>\n",
    "where NMTfiDF stands for the TfIdF of named-entity NM in sentence S.  POSS corresponds to the sentence weight associated to the location of the sentence. So that the sentence location weight will be maximum (1) if located in the title of the document, 0.5 if located  in the title of one of the subsection, 0.25 if located in the title one of the subsubsection, 0.1 if located in one representative object of the document, and 0 if located only in the main text. Make sure to normalize the term tfidf and Nm tfidf weights and suggest a script to implement the preceding accordingly, so that the summarizer will contain the 10 sentences with the highest Sweight scores.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Markus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Markus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    \"1\": 1,\n",
    "    \"2\": 0.1,\n",
    "    \"3\": 0.5,\n",
    "    \"4\": 0.25,\n",
    "    \"else\": 0.0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calculateFullScores(sentenceScores, namedEntityScores, counts):\n",
    "    scaler = MinMaxScaler()\n",
    "    weightList= []\n",
    "\n",
    "    if len(counts) > 0:\n",
    "        if counts[2] == 0:\n",
    "            counts.pop[2]\n",
    "    else:\n",
    "        counts = [0, 0, 0, 0, len(sentenceScores)]\n",
    "\n",
    "    for i in range(len(counts)):\n",
    "        for j in range(counts[i]):\n",
    "            if i > 3:\n",
    "                weightList.append(weights[\"else\"])\n",
    "            else:\n",
    "                weightList.append(weights[str(i+1)])\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"Weights\": weightList,\n",
    "        \"SentenceScores\": sentenceScores,\n",
    "        \"EntityScores\": namedEntityScores,\n",
    "    })\n",
    "\n",
    "    df[[\"SentencesScaled\"]] = scaler.fit_transform(df[[\"SentenceScores\"]])\n",
    "    df[[\"EntitiesScaled\"]] = scaler.fit_transform(df[[\"EntityScores\"]])\n",
    "    df[\"S_weight\"] = df[\"SentencesScaled\"] + (2 * df[\"EntitiesScaled\"]) + df[\"Weights\"]\n",
    "    return df[\"S_weight\"].tolist()\n",
    "\n",
    "\n",
    "def _getNamedEntities(article):\n",
    "    nlp = en_core_web_sm.load()\n",
    "    doc = nlp(article)\n",
    "    namedEntities = []\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"ORG\" or ent.label_ == \"PERSON\":\n",
    "                namedEntities.append(ent.text)\n",
    "\n",
    "    return namedEntities\n",
    "\n",
    "\n",
    "def _getSentencesWithMaxWeights(weights, sentences, numberOfSentences):\n",
    "    arr = np.array(weights)\n",
    "    indexes = np.argpartition(arr, -numberOfSentences)[-numberOfSentences:]\n",
    "    sentences = np.array(sentences)\n",
    "    return sentences[indexes]\n",
    "\n",
    "\n",
    "def _preProcess(document):\n",
    "    stopwords = list(set(nltk.corpus.stopwords.words('english')))\n",
    "    WN_lemmatizer = WordNetLemmatizer()\n",
    "    sentences = sent_tokenize(document)\n",
    "    processedSentences = []\n",
    "    tokens = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        words = [WN_lemmatizer.lemmatize(word, pos=\"v\") for word in words]\n",
    "\n",
    "        # get rid of numbers and Stopwords\n",
    "        words = [word for word in words if word.isalpha() and word not in stopwords]\n",
    "        processedSentences.append(' '.join(word for word in words))\n",
    "        tokens.extend(words)\n",
    "\n",
    "    return processedSentences, tokens\n",
    "\n",
    "\n",
    "def _tfidfScores(corpus, sentences):\n",
    "    tfidf = TfidfVectorizer()\n",
    "    fittedVectorizer = tfidf.fit(corpus)\n",
    "    vectors = fittedVectorizer.transform(sentences).toarray()\n",
    "\n",
    "    scores = []\n",
    "    for i in range(len(vectors)):\n",
    "        score = 0\n",
    "        for j in range(len(vectors[i])):\n",
    "            score = score + vectors[i][j]\n",
    "\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our experiments have revealed that the optimal algorithm for approximate nearest neighbor search is highly dependent\n",
      " on several factors such as the data dimensionality, size and structure of the data set (whether there is any\n",
      " correlation between the features in the data set) and the desired search precision.\n",
      "\n",
      "For the comparison we use a combination of both vector features such as SIFT, SURF, image patches and\n",
      " binary features such as BRIEF and ORB.\n",
      "\n",
      "After evaluating many different algorithms for approximate nearest neighbor search on data sets with a wide range of\n",
      " dimensionality [14][47], we have\n",
      " found that one of two algorithms gave the best performance: the priority search k-means tree or\n",
      " the multiple randomized k-d trees.\n",
      "\n",
      "We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set.\n",
      "\n",
      "Scalable Nearest Neighbor Algorithms for High Dimensional Data.\n",
      "\n",
      "All this research has been released as an open source library called fast library for approximate nearest neighbors (FLANN), which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching.\n",
      "\n",
      "Each point on the graph is computed using the best\n",
      " performing algorithm for that particular feature type (randomized kd-trees or priority search k-means tree for SIFT,\n",
      " SURF, image patches and the hierarchical clustering algorithm for BRIEF and ORB).\n",
      "\n",
      "We have found the randomized k-d forest to be very effective in many situations, however on other data sets a\n",
      " different algorithm, the priority search k-means tree, has been more effective at finding approximate\n",
      " nearest neighbors, especially when a high precision is required.\n",
      "\n",
      "We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple\n",
      " randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN \n",
      "[11] and LSH algorithms [29]\n",
      " on the first data set of 100,000 SIFT features.\n",
      "\n",
      "The partitioning of the data set into independent subsets, as described above and implemented in FLANN, has the\n",
      " advantage that it doesn’t depend on the type of index used (randomized kd-trees, priority search k-means tree,\n",
      " hierarchical clustering, LSH) and can be applied to any current or future nearest neighbor algorithm in FLANN.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def findTopSentences(document, numberOfSentences, isUrl):\n",
    "    sentences, tokens = _preProcess(document)\n",
    "    sentenceTfidfScores = _tfidfScores(tokens, sentences)\n",
    "    namedEntitiesTfidfScores = _tfidfScores(_getNamedEntities(document), sentences)\n",
    "    time.sleep(0.1)\n",
    "    SWeight = []\n",
    "    if isUrl:\n",
    "        SWeight = _calculateFullScores(sentenceTfidfScores, namedEntitiesTfidfScores, counts)\n",
    "    else:\n",
    "        SWeight = _calculateFullScores(sentenceTfidfScores, namedEntitiesTfidfScores, [])\n",
    "    topSentences = _getSentencesWithMaxWeights(SWeight, sent_tokenize(document), numberOfSentences)\n",
    "    return list(topSentences)\n",
    "\n",
    "topSentences = findTopSentences(article, 10, True)\n",
    "for sentence in topSentences:\n",
    "    print(\"{}\\n\".format(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TASK 4\n",
    "Test the above approach with Opinosis dataset available at https://kavita-ganesan.com/opinosis-opinion-dataset/#.YVw6J5ozY2x,  and record the corresponding Rouge-2 and Rouge-3 evaluation score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rouge 2 ja 3 scoring\n",
    "# https://towardsdatascience.com/the-ultimate-performance-metric-in-nlp-111df6c64460\n",
    "# pip install git+git://github.com/bdusell/rougescore.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rougescore as rouge\n",
    "\n",
    "def getRouge(peer, model):\n",
    "    rougeBi = rouge.rouge_2(peer, model, 1)\n",
    "    rougeTri = rouge.rouge_3(peer, model, 1)\n",
    "    \n",
    "    return rougeBi,rougeTri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_folder(dir):\n",
    "    \n",
    "    topic = []\n",
    "    \n",
    "    for file in os.listdir(dir):   \n",
    "        with open(os.path.join(dir + file)) as f:\n",
    "            doc = f.readlines()\n",
    "            f.close()\n",
    "            doc = \" \".join(doc)\n",
    "            topic.append(doc)\n",
    "            \n",
    "    return topic           \n",
    "\n",
    "def create_model(dir):\n",
    "    \n",
    "    model = []\n",
    "    \n",
    "    for folder in os.listdir(directory + \"summaries-gold/\"):\n",
    "        \n",
    "        gold = read_folder(directory + \"summaries-gold/\" + folder + \"/\")\n",
    "        model.append(gold)\n",
    "        \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(dir):\n",
    "    \n",
    "    list_summary = []\n",
    "    for file in os.listdir(directory + \"topics/\"):\n",
    "            \n",
    "        with open(os.path.join(dir + \"topics/\" + file)) as f:\n",
    "            doc = f.readlines()\n",
    "            f.close()\n",
    "            doc = \" \".join(doc)\n",
    "            \n",
    "            summary = findTopSentences(doc, 10, False)\n",
    "            list_summary.append(summary)\n",
    "            \n",
    "    return list_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "directory = \"C:/Users/Markus/Documents/studies/NLP/data/Opinosis_dataset/\"\n",
    "\n",
    "summary = summary(directory)\n",
    "model = create_model(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_score = []\n",
    "\n",
    "for i in range(len(summary)):\n",
    "    summary_str = \" \".join(summary[i])\n",
    "    bi, tri = getRouge(summary_str, model[i])\n",
    "    list_score.append((bi,tri))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>topic number</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rouge3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.052142</td>\n",
       "      <td>0.041615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.067094</td>\n",
       "      <td>0.052058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.065919</td>\n",
       "      <td>0.051844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.052494</td>\n",
       "      <td>0.038054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.045128</td>\n",
       "      <td>0.033889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.043790</td>\n",
       "      <td>0.032983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.074895</td>\n",
       "      <td>0.057781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.046805</td>\n",
       "      <td>0.037890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.063447</td>\n",
       "      <td>0.044701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.040630</td>\n",
       "      <td>0.029268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.050883</td>\n",
       "      <td>0.035592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.031318</td>\n",
       "      <td>0.024251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.056122</td>\n",
       "      <td>0.042389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.054089</td>\n",
       "      <td>0.042948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.034907</td>\n",
       "      <td>0.026539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.074092</td>\n",
       "      <td>0.058873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.043627</td>\n",
       "      <td>0.034845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.046476</td>\n",
       "      <td>0.036178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.072588</td>\n",
       "      <td>0.049904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.091080</td>\n",
       "      <td>0.067815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.059640</td>\n",
       "      <td>0.047376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.038121</td>\n",
       "      <td>0.031804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.054201</td>\n",
       "      <td>0.040379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.039851</td>\n",
       "      <td>0.032070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.084475</td>\n",
       "      <td>0.055157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.066485</td>\n",
       "      <td>0.051427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.073361</td>\n",
       "      <td>0.055776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.051705</td>\n",
       "      <td>0.039956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.064855</td>\n",
       "      <td>0.050348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.052850</td>\n",
       "      <td>0.033281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.025995</td>\n",
       "      <td>0.021940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.048163</td>\n",
       "      <td>0.037322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.029744</td>\n",
       "      <td>0.024631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.048331</td>\n",
       "      <td>0.041015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.048601</td>\n",
       "      <td>0.033480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.038725</td>\n",
       "      <td>0.032448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.038835</td>\n",
       "      <td>0.030233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.076742</td>\n",
       "      <td>0.062539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.062967</td>\n",
       "      <td>0.046832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.026699</td>\n",
       "      <td>0.020693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.040480</td>\n",
       "      <td>0.032542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.062765</td>\n",
       "      <td>0.051399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.038750</td>\n",
       "      <td>0.029326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.051791</td>\n",
       "      <td>0.040655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.055079</td>\n",
       "      <td>0.043170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.038325</td>\n",
       "      <td>0.028673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.039497</td>\n",
       "      <td>0.028184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.044591</td>\n",
       "      <td>0.035512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.075248</td>\n",
       "      <td>0.059222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.037406</td>\n",
       "      <td>0.029803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.034094</td>\n",
       "      <td>0.025618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.052076</td>\n",
       "      <td>0.039848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "topic number    rouge2    rouge3\n",
       "0             0.052142  0.041615\n",
       "1             0.067094  0.052058\n",
       "2             0.065919  0.051844\n",
       "3             0.052494  0.038054\n",
       "4             0.045128  0.033889\n",
       "5             0.043790  0.032983\n",
       "6             0.074895  0.057781\n",
       "7             0.046805  0.037890\n",
       "8             0.063447  0.044701\n",
       "9             0.040630  0.029268\n",
       "10            0.050883  0.035592\n",
       "11            0.031318  0.024251\n",
       "12            0.056122  0.042389\n",
       "13            0.054089  0.042948\n",
       "14            0.034907  0.026539\n",
       "15            0.074092  0.058873\n",
       "16            0.043627  0.034845\n",
       "17            0.046476  0.036178\n",
       "18            0.072588  0.049904\n",
       "19            0.091080  0.067815\n",
       "20            0.059640  0.047376\n",
       "21            0.038121  0.031804\n",
       "22            0.054201  0.040379\n",
       "23            0.039851  0.032070\n",
       "24            0.084475  0.055157\n",
       "25            0.066485  0.051427\n",
       "26            0.073361  0.055776\n",
       "27            0.051705  0.039956\n",
       "28            0.064855  0.050348\n",
       "29            0.052850  0.033281\n",
       "30            0.025995  0.021940\n",
       "31            0.048163  0.037322\n",
       "32            0.029744  0.024631\n",
       "33            0.048331  0.041015\n",
       "34            0.048601  0.033480\n",
       "35            0.038725  0.032448\n",
       "36            0.038835  0.030233\n",
       "37            0.076742  0.062539\n",
       "38            0.062967  0.046832\n",
       "39            0.026699  0.020693\n",
       "40            0.040480  0.032542\n",
       "41            0.062765  0.051399\n",
       "42            0.038750  0.029326\n",
       "43            0.051791  0.040655\n",
       "44            0.055079  0.043170\n",
       "45            0.038325  0.028673\n",
       "46            0.039497  0.028184\n",
       "47            0.044591  0.035512\n",
       "48            0.075248  0.059222\n",
       "49            0.037406  0.029803\n",
       "50            0.034094  0.025618\n",
       "mean          0.052076  0.039848"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataframe_image as dfi\n",
    "\n",
    "df = pd.DataFrame(list_score)\n",
    "df.columns=['rouge2', 'rouge3']\n",
    "df.loc['mean'] = df.mean()\n",
    "df.columns.names = ['topic number']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 5\n",
    "\n",
    "[x] We would like to improve the summarization by taking into account the diversity among the sentence in the sense that we would like to minimize redundancy among sentences. For this purpose, we shall use the sentence-to-sentence semantic similarity introduced in the NLP lab. \n",
    "\n",
    "[x] Next, instead of recording only the 10 sentences with highest Sweight scores, we shall record the 20 top sentences in terms of $S_{weight}$ scores. Then the selection of the top 10 sentences among the 20 sentences follows the following approach. \n",
    "\n",
    "[x] First, order the 20 sentences in the decreasing order of their $S_{weight}$ scores, say S1, S2, …, S20 (where S1 is the top ranked and S20 the 20th ranked sentence). \n",
    "\n",
    "[x] Second, we shall assume that S1 is always included in the summarizer, we shall then attempt to find the other sentences among S2 till S20 to be included into the summarizer. \n",
    "\n",
    "[x] Calculate the sentence-to-sentence similarity Sim(S1,Si) for i=1 to 20, the Sentence Sj that yields the minimum similarity with S1 will therefore be included in the summarizer. \n",
    "\n",
    "[x] Next, for each of the remaining sentences Sk (with k different from 1 and j), we calculate the sentence similarity with Sj. Therefore the sentence Sp that yields minimum value of “Sim(Sp, S1)+Sim(Sp,Sj)” will be included in the summarizer (Note: the quantity Sim(Sp, S1) is already calculated in previous step).  \n",
    "\n",
    "[x] Similarly in the next phase, we should select a sentence Sl (l different from 1, j and k) so that  “Sim(Sl, S1)+Sim(Sl,Sj)+Sim(Sl,Sp)”, Etc.. \n",
    "\n",
    "[x] You then stop once you reached 10 sentences included in the summarizer. \n",
    "\n",
    "[ ] Suggest a script that includes this process.. and illustrate its functioning in the example you chosen in 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download larger pipeline package for spaCy\n",
    "# python -m spacy download en_core_web_lg #tarkempi mutta 770mb kokoinen\n",
    "\n",
    "# python -m spacy download en_core_web_sm #paljon pienempi mutta ei yhtä tarkka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s1 määritys\n",
    "def find_first_sentence(sentences):\n",
    "    picked_sentences = []\n",
    "\n",
    "    # choose dictionary\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    #nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # find first sentence with highest s_weigth\n",
    "    for sentence in sentences: \n",
    "        # add code to calculate s(weight), or pick the first if list is in order\n",
    "        s1 = sentence\n",
    "\n",
    "    # add sumamrization to list  \n",
    "    picked_sentences.append(s1)\n",
    "    sentences.remove(s1)\n",
    "\n",
    "    return picked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rest 9 sentences\n",
    "def sentence_to_sentence(sentences):\n",
    "# List of similarity scores\n",
    "    sim_score = []\n",
    "    picked_sentences = find_first_sentence(sentences)\n",
    "    #while until 10 sentences are found\n",
    "    while(len(picked_sentences)<10):\n",
    "        sim_score.clear()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            nlp_sentence = nlp(str(sentence))\n",
    "            score = 0\n",
    "\n",
    "            for p_sentence in picked_sentences:\n",
    "                #vertaa kahta lausetta\n",
    "                nlp_p_sentence = nlp(str(p_sentence))\n",
    "\n",
    "                score += nlp_p_sentence.similarity(nlp_sentence)\n",
    "\n",
    "            sim_score.append(score)\n",
    "\n",
    "        min_value = min(sim_score)\n",
    "        min_index = sim_score.index(min_value)   \n",
    "\n",
    "        picked_sentences.append(sentences[min_index])\n",
    "        sentences.remove(sentences[min_index])\n",
    "    return picked_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n"
     ]
    }
   ],
   "source": [
    "picked_sentences = sentence_to_sentence(findTopSentences(article, 20, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarized text\n",
      "['The partitioning of the data set into independent subsets, as described above and implemented in FLANN, has the\\n advantage that it doesn’t depend on the type of index used (randomized kd-trees, priority search k-means tree,\\n hierarchical clustering, LSH) and can be applied to any current or future nearest neighbor algorithm in FLANN.', 'Scalable Nearest Neighbor Algorithms for High Dimensional Data.', 'For the comparison we use a combination of both vector features such as SIFT, SURF, image patches and\\n binary features such as BRIEF and ORB.', 'We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature.', 'We have found the randomized k-d forest to be very effective in many situations, however on other data sets a\\n different algorithm, the priority search k-means tree, has been more effective at finding approximate\\n nearest neighbors, especially when a high precision is required.', 'We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple\\n randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN \\n[11] and LSH algorithms [29]\\n on the first data set of 100,000 SIFT features.', 'In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper.', '[31] show that the performance of the standard LSH\\n algorithm is critically dependent on the length of the hashing key and propose the LSH Forest, a self-tuning algorithm\\n that eliminates this data dependent parameter.', 'Having an efficient\\n algorithm for performing fast nearest neighbor matching in large data sets can bring speed improvements of several\\n orders of magnitude to many applications.', 'The work presented in this paper has been made publicly available as an open source library named Fast Library for\\n Approximate Nearest Neighbors [59].']\n"
     ]
    }
   ],
   "source": [
    "print(\"Summarized text\")\n",
    "print(picked_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6\n",
    "\n",
    "We would like to make the choice of keywords not based on histogram frequency but using the open source RAKE https://www.airpair.com/nlp/keyword-extraction-tutorial. Repeat the previous process of selecting the sentences that are associated to the ten first keywords generated by RAKE. Comment on the quality of this summarizer based on your observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Installation file in Github repo does not work on Windows before doing following fixes\n",
    "# git clone https://github.com/zelandiya/RAKE-tutorial\n",
    "# cd RAKE-tutorial\n",
    "\n",
    "# Before installing go to setup.py file and delete slash (/) from paths: \n",
    "# package_dir={'nlp_rake': './'} ja \n",
    "# package_data={'nlp_rake': ['data/']}\n",
    "\n",
    "# muutin \"nlp-rake\" nimen pelkäksi \"rake\" asennus tiedostossa.\n",
    "\n",
    "# Image in repo\n",
    "# python setup.py install \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add module to path\n",
    "import sys \n",
    "sys.path.append(\"C:/Users/Markus/Documents/studies/NLP/RAKE-tutorial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake \n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give correct path\n",
    "rake_object = rake.Rake(\"C:/Users/Markus/Documents/studies/NLP/NLP/SmartStoplist.txt\", 5, 3, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_file = open(\"C:/NLP/RAKE-tutorial/data/docs/fao_test/w2167e.txt\", 'r') # for running with sample file\n",
    "# text = sample_file.read()\n",
    "sentenceList = rake.split_sentences(article)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nearest neighbor matching', 'matching binary features', 'large data sets', 'tree build time', 'nearest neighbor search', 'hierarchical clustering tree', 'nearest neighbor', 'data sets', 'build time', 'data set']\n"
     ]
    }
   ],
   "source": [
    "keywords = rake_object.run(article)\n",
    "keywords_topten = []\n",
    "\n",
    "for i in range(10):\n",
    "    keywords_topten.append(keywords[i][0])\n",
    "    \n",
    "print(keywords_topten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences associated to ten first keywords:\n",
      "\n",
      " the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data\n",
      " We propose new algorithms for approximate nearest neighbor matching and evaluate and compare them with previous algorithms\n",
      " We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature\n",
      " We show that the optimal nearest neighbor algorithm and its parameters depend on the data set characteristics and describe an automated configuration procedure for finding the best algorithm to search a particular data set\n",
      " In order to scale to very large data sets that would otherwise not fit in the memory of a single machine\n",
      " we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper\n",
      " All this research has been released as an open source library called fast library for approximate nearest neighbors \n",
      " which has been incorporated into OpenCV and is now one of the most popular libraries for nearest neighbor matching\n",
      " also referred to as nearest neighbor matching\n",
      " algorithm for performing fast nearest neighbor matching in large data sets can bring speed improvements of several\n",
      " features in large data sets \n",
      " in large data sets \n",
      " The nearest neighbor search problem is also of major\n",
      " but for large data sets the performance of the algorithms employed quickly\n",
      " In many cases the nearest neighbor search is just a part of a larger application\n",
      " and discuss the problem of scaling to very large data sets using compute clusters\n",
      " released all this work as an open source library named fast library for approximate nearest neighbors \n",
      " In this paper we are concerned with the problem of efficient nearest neighbor search in metric spaces\n",
      " The nearest neighbor problem consists of finding a method to pre\n",
      "nearest neighbor \n",
      " search where the goal is to find the closest K points from the query point and radius nearest neighbor search \n",
      "nearest neighbor search more formally in the following manner\n",
      " The radius nearest neighbor search can be defined as follows\n",
      " return any number of points between zero and the whole data set\n",
      "nearest neighbor \n",
      "nearest neighbor search and radius\n",
      " We review the most widely used nearest neighbor techniques\n",
      " best known nearest neighbor algorithms\n",
      "p∗ is the true nearest neighbor\n",
      " This method of approximating the nearest neighbor search is also referred to as “error\n",
      " Another way of approximating the nearest neighbor search is by limiting the time spent during the search\n",
      " results for some data sets\n",
      " which they decompose the space into low dimensional subspaces and represent the data sets points by compact codes\n",
      " searching large data sets and they should be considered for further evaluation and possible incorporation into FLANN\n",
      " Perhaps the best known hashing based nearest neighbor technique is locality sensitive hashing \n",
      " its nearest neighbors\n",
      " to their nearest neighbors\n",
      " The nearest neighbor graph methods suffer from a quite expensive construction of the k\n",
      " nearest neighbor graph\n",
      " There have been hundreds of papers published on nearest neighbor search algorithms\n",
      " of the nearest neighbor literature\n",
      " we have proposed an automatic nearest neighbor\n",
      " papers that apply such techniques to finding optimum parameters for nearest neighbor algorithms\n",
      " After evaluating many different algorithms for approximate nearest neighbor search on data sets with a wide range of\n",
      " is an approximate nearest neighbor\n",
      " as this performs well across all our data sets and does not benefit significantly from further tuning\n",
      " returning the best nearest neighbor\n",
      "100K SIFT features data set\n",
      " nearest neighbor will be in the same cell\n",
      " The nearest neighbor is across a decision boundary from the query point in\n",
      " however on other data sets a\n",
      " nearest neighbors\n",
      " substantially reduce the tree build time and results in a slightly less than optimal clustering \n",
      " the nearest neighbor search performance is similar to that of the\n",
      " but requiring less than 10 percent of the build time\n",
      " In case of the time constrained approximate nearest neighbor search\n",
      " either not efficient or not suitable for matching binary features \n",
      " called the hierarchical clustering tree\n",
      " which we found to be very effective at matching binary features\n",
      " The hierarchical clustering tree performs a decomposition of the search space by recursively clustering the input\n",
      " data set using random data points as the cluster centers of the non\n",
      " we have found that building multiple hierarchical clustering trees and searching them in\n",
      " Our experiments have revealed that the optimal algorithm for approximate nearest neighbor search is highly dependent\n",
      " size and structure of the data set \n",
      " correlation between the features in the data set\n",
      " automatic selection of the best nearest neighbor algorithm to use for a particular data set and for choosing its\n",
      " By considering the nearest neighbor algorithm itself as a parameter of a generic nearest neighbor search routine \n",
      " tree build time\n",
      " the tree build time \n",
      " cases both the tree build time and search time must be small \n",
      " tree build time and memory overhead for the tree\n",
      " used to control the relative importance of the build time and memory overhead in the overall cost\n",
      " controls the importance of the tree build time relative to the search time\n",
      " which is defined as the optimal search and build time if memory usage were not a factor\n",
      " In FLANN the optimization can be run on the full data set for the most accurate results or using just a fraction of\n",
      " the data set to have a faster auto\n",
      " type of data set\n",
      " and the optimum parameter values can be saved and applied to all future data sets of the same type\n",
      " For the experiments presented in this section we used a selection of data sets with a wide range of sizes and data\n",
      " Among the data sets used are the Winder/Brown patch data set \n",
      " data sets of randomly sampled data of different dimensionality\n",
      " data sets of SIFT features of different sizes\n",
      " obtained by sampling from the CD cover data set of \n",
      " as well as a data set\n",
      " We measure the accuracy of an approximate nearest neighbor algorithm using the search precision \n",
      " are exact nearest neighbors\n",
      " Data dimensionality is one of the factors that has a great impact on the nearest neighbor matching performance\n",
      " The data sets in this case each contain 105\n",
      " These random data sets are one of the most\n",
      " difficult problems for nearest neighbor search\n",
      " with data sets of size 100K\n",
      "world data sets\n",
      " 6 shows four examples of queries on the Trevi data set of patches for\n",
      " Example of nearest neighbor queries with different patch sizes\n",
      " The Trevi Fountain patch data set was\n",
      " while the following five patches are the nearest neighbors from a set of 100\n",
      " We use several data sets of different sizes for the experiments in Fig\n",
      " construct 100K and 1 million SIFT feature data sets by randomly sampling a data set of over 5 million SIFT\n",
      " We also use the 31 million SIFT feature data set from the same source\n",
      " Search speedup for different data set sizes\n",
      "the sift1M data set\n",
      " We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors \n",
      " on the first data set of 100\n",
      " nearest neighbors\n",
      " to find the nearest neighbors we have used the approach suggested in the E2LSH\n",
      " computed the precision achieved as the percentage of the query points for which the nearest neighbors were correctly\n",
      " Comparison of the search efficiency for several nearest neighbor algorithms\n",
      " 9 compares the performance of nearest neighbor matching when the data\n",
      " a match in which the query and the nearest neighbor point represent the same entity\n",
      " million SIFT features data set and it contains only false matches for each feature in the test set\n",
      "t have “true” matches in the data set versus the\n",
      "means tree for one of the Winder/Brown patches data set\n",
      " This shows the importance of performing algorithm selection on each data set\n",
      " Search speedup for the Trevi Fountain patches data set\n",
      "4 on a data set containing 100K randomly\n",
      " For the build time weight\n",
      "t care about the tree build time\n",
      " build time and search time have the same importance and 0\n",
      " search time but we also want to avoid a large build time\n",
      " This section evaluates the performance of the hierarchical clustering tree described in \n",
      " We use the Winder/Brown patches data set \n",
      " neighbor search performance of the hierarchical clustering tree to that of other well known nearest neighbor search\n",
      " nearest neighbor search times for the different feature types\n",
      " 12 we compare the hierarchical clustering tree with a multi\n",
      " data sets of BRIEF and ORB features extracted from the recognition benchmark images data set of \n",
      " hierarchical clustering index outperforms the LSH implementation for this data set\n",
      " requires significantly more memory compared to the hierarchical clustering trees for when high precision is required\n",
      " images data set of about 5 million features\n",
      "parametric methods in conjunction with large scale data sets can lead\n",
      " large data sets is a difficult task\n",
      " the size of the raw tiny images data set of \n",
      " Fitting the data in memory is even more problematic for data sets of the size of those used in \n",
      " several computers and using a distributed nearest neighbor search algorithm\n",
      " In FLANN we used the approach of performing distributed nearest neighbor search across\n",
      " In order to scale to very large data sets\n",
      " compute cluster and perform the nearest neighbor search using all the machines in parallel\n",
      " have to index and search 1/N of the whole data set \n",
      " The final result of the nearest neighbor\n",
      " In order to distribute the nearest neighbor matching on a compute cluster we implemented a Map\n",
      " Algorithm 4 describes the procedure for building a distributed nearest neighbor matching index\n",
      " cluster executes in parallel and reads from a distributed filesystem a fraction of the data set\n",
      " the nearest neighbor search index in parallel using their respective data set fractions\n",
      " Scaling nearest neighbor search on a compute cluster using message passing interface standard\n",
      " nearest neighbor matching in parallel on its own fraction of the data\n",
      "computed as they are returned by the nearest neighbor search operations on each server\n",
      " When distributing a large data set for the purpose of nearest neighbor search we chose to partition the data into\n",
      " broadcast to all the indexes and each of them performs the nearest neighbor search within its associated data\n",
      " The partitioning of the data set into independent subsets\n",
      " and can be applied to any current or future nearest neighbor algorithm in FLANN\n",
      " For these experiments we have used the 80 million patch data set of \n",
      " Distributing nearest neighbor search on a single multi\n",
      " 16 shows the search speedup for the data set of 80 million tiny images\n",
      " performance scales well with the data set size and it benefits from using multiple parallel processes\n",
      " All the previous experiments have shown that distributing the nearest neighbor search to multiple machines results\n",
      " however in practice for approximate nearest neighbor search the speedup\n",
      " data set\n",
      " This paper addresses the problem of fast nearest neighbor search in high dimensional spaces\n",
      " to very large size data sets by proposing an algorithm for distributed nearest neighbor matching on compute clusters\n"
     ]
    }
   ],
   "source": [
    "rake_sentences = []\n",
    "# extract sentences using keywords\n",
    "for sentence in sentenceList:\n",
    "    for keyword in keywords_topten:\n",
    "        if keyword in sentence:\n",
    "            rake_sentences.append(sentence)\n",
    "            break\n",
    "\n",
    "# Sentences with max value, outputs only one sentence with example article\n",
    "# dct = {}\n",
    "# for sentence in sentenceList:\n",
    "#    dct[sentence] = sum(1 for word in keywords_topten if word in sentence)\n",
    "\n",
    "# rake_sentences = [key for key,value in dct.items() if value == max(dct.values())]\n",
    "\n",
    "print(\"Sentences associated to ten first keywords:\\n\")\n",
    "print(\"\\n\".join(rake_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences in original text: 1873, summarized amount: 158\n"
     ]
    }
   ],
   "source": [
    "# Comparing results\n",
    "print(\"Sentences in original text: {}, summarized amount: {}\".format(len(sentenceList),len(rake_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7\n",
    "\n",
    "It is also suggested to explore alternative implementations with larger number of summarization approaches implemented- https://github.com/miso-belica/sumy. Show how each of the implemented summarizer behaves when inputted with the same document you used in previous case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/miso-belica/sumy\n",
    "# pip install sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as LSASummarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer as LexRankSummarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer as LuhnSummarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2.1 Nearest Neighbor Matching Algorithms.', 'After evaluating many different algorithms for approximate nearest neighbor search on data sets with a wide range of dimensionality [14][47], we have found that one of two\\xa0algorithms gave the best performance: the priority search k-means tree or the multiple randomized k-d trees.', 'The randomized k-d tree algorithm [13], is an approximate nearest neighbor search algorithm that builds multiple randomized k-d trees which are searched in parallel.', 'Our experiments have revealed that the optimal algorithm for approximate nearest neighbor search is highly dependent on several factors such as the data dimensionality, size and structure of the data set (whether there is any correlation between the features in the data set) and the desired search precision.', 'We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN [11] and LSH algorithms [29] on the first data set of 100,000 SIFT features.', '10 shows the difference in performance between the randomized kd-trees and the priority search k-means tree for one of the Winder/Brown patches data set.', '7 shows that the k-means tree can perform better for other data sets (especially for high precisions).', 'We use the Winder/Brown patches data set [53] to compare the nearest neighbor search performance of the hierarchical clustering tree to that of other well known nearest neighbor search algorithms.', 'Each point on the graph is computed using the best performing algorithm for that particular feature type (randomized kd-trees or priority search k-means tree for SIFT, SURF, image patches and the hierarchical clustering algorithm for BRIEF and ORB).', 'In order to scale to very large data sets, we use the approach of distributing the data to multiple machines in a compute cluster and perform the nearest neighbor search using all the machines in parallel.']\n",
      "\n",
      "\n",
      "['We also propose a new algorithm for matching binary features by searching multiple hierarchical clustering trees and show it outperforms methods typically used in the literature.', 'In order to scale to very large data sets that would otherwise not fit in the memory of a single machine, we propose a distributed nearest neighbor matching framework that can be used with any of the algorithms described in the paper.', 'Having an efficient algorithm for performing fast nearest neighbor matching in large data sets can bring speed improvements of several orders of magnitude to many applications.', 'In practice, and in most of the nearest neighbor literature, setting the algorithm parameters is a manual process carried out by using various heuristics and rarely make use of more systematic approaches.', 'In a previous paper [14] we have proposed an automatic nearest neighbor algorithm configuration method by combining grid search with a finer grained Nelder-Mead downhill simplex optimization process [43].', 'The number of clusters K to use when partitioning the data at each node is a parameter of the algorithm, called the branching factor and choosing K is important for obtaining good search performance.', 'However, we have observed that even when using a small number of iterations, the nearest neighbor search performance is similar to that of the tree constructed by running the clustering until convergence, as illustrated by Fig.', 'In order to distribute the nearest neighbor matching on a compute cluster we implemented a Map-Reduce like algorithm using the message passing interface (MPI) specification.', 'All the previous experiments have shown that distributing the nearest neighbor search to multiple machines results in an overall increase in performance in addition to the advantage of being able to use more memory.', 'We address the issues arising when scaling to very large size data sets by proposing an algorithm for distributed nearest neighbor matching on compute clusters.']\n",
      "\n",
      "\n",
      "['However, the most computationally expensive part of many computer vision and machine learning algorithms consists of finding nearest neighbor matches to high dimensional vectors that represent the training data.', 'Examples of such problems include finding the best matches for local image features in large data sets [1][2] clustering local features into visual words using the k-means or similar algorithms [3], global image feature matching for scene recognition [4], human pose estimation [5], matching deformable shapes for object recognition [6] or performing normalized cross-correlation (NCC) to compare image patches in large data sets [7].', 'In this paper we evaluate the most promising nearest-neighbor search algorithms in the literature, propose new algorithms and improvements to existing ones, present a method for performing automatic algorithm selection and parameter optimization, and discuss the problem of scaling to very large data sets using compute clusters.', 'In this case, the search can be performed in several ways, depending on the number of neighbors returned and their distance to the query point: K-nearest neighbor (KNN) search where the goal is to find the closest K points from the query point and radius nearest neighbor search (RNN), where the goal is to find all the points located closer than some distance R from the query point.', 'The performance of hashing methods is highly dependent on the quality of the hashing functions they use and a large body of research has been targeted at improving hashing methods by using data-dependent hashing functions computed using various learning techniques: parameter sensitive hashing [5], spectral hashing [32], randomized LSH hashing from learned metrics [33], kernelized LSH [34], learnt binary embeddings [35], shift-invariant kernel hashing [36], semi-supervised hashing [37], optimized kernel hashing [38] and complementary hashing [39].', 'The trees are built in a similar manner to the classic k-d tree [9] [10], with the difference that where the classic kd-tree algorithm splits data on the dimension with the highest variance, for the randomized k-d trees the split dimension is chosen randomly\\xa0from the top ND dimensions with the highest variance.', 'Many algorithms suitable for matching vector based features, such as the randomized kd-tree and priority search k-means tree, are either not efficient or not suitable for matching binary features (for example, the priority search k-means tree requires the points to be in a vector space where their dimensions can be independently averaged).', 'In contrast to the priority search k-means tree presented above, for which using more than one tree did not bring significant improvements, we have found that building multiple hierarchical clustering trees and searching them in parallel using a common priority queue (the same approach that has been found to work well for randomized kd-trees [13]) resulted in significant improvements in the search performance.', 'Among the data sets used are the Winder/Brown patch data set [53] , data sets of randomly sampled data of different dimensionality, data sets of SIFT features of different sizes obtained by sampling from the CD cover data set of [24] as well as a data set of SIFT features extracted from the overlapping images forming panoramas.', 'We compare the two algorithms we found to be the best at finding fast approximate nearest neighbors (the multiple randomized kd-trees and the priority search k-means tree) with existing approaches, the ANN [11] and LSH algorithms [29] on the first data set of 100,000 SIFT features.']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def sumySummarize(article):\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "    summarizers = [LexRankSummarizer(stemmer), LSASummarizer(stemmer), LuhnSummarizer(stemmer)]\n",
    "    parser = PlaintextParser.from_string(article, Tokenizer(LANGUAGE))\n",
    "    results = []\n",
    "    \n",
    "    for summarizer in summarizers:\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "        sentences = []\n",
    "        for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "            sentences.append(str(sentence))\n",
    "        results.append(sentences)\n",
    "    \n",
    "    return results\n",
    "\n",
    "sumySentences = sumySummarize(article)\n",
    "for sentences in sumySentences:\n",
    "    print(\"{}\\n\\n\".format(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 8\n",
    "\n",
    "Now we would like to compare the above summarizers and those in 3), 5) and 7) on a new dataset constructed as follows. First select an Elsevier journal of your own and select 10 papers highly ranked in the journal according to citation index (The journal papers should be well structured to contain Abstract, Introduction and Conclusion). \n",
    "\n",
    "For each of the ten papers, consider the introduction as the main document to seek to apply summarizer, and consider the Abstract and Conclusion as two golden summary of the document that you can use for assessment using ROUGE-1 and ROUGE-2 evaluation. \n",
    "\n",
    "Report in a table the evaluation score of each summarizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []\n",
    "golds = []\n",
    "for i in range(1,11):\n",
    "    with open('C:/Users/Markus/Documents/studies/NLP/NLP/Data/Task8_articles/article{}.txt'.format(i), encoding=\"utf8\") as f:\n",
    "        text = f.readlines()\n",
    "        text = \" \".join(text)\n",
    "        res = text.split(\"\\n \\n\")\n",
    "        files.append(res[1])\n",
    "        temp = []\n",
    "        temp.append(res[0])\n",
    "        temp.append(res[2])\n",
    "        golds.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "C:\\Users\\Markus\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n"
     ]
    }
   ],
   "source": [
    "def summarize_introduction(introduction):\n",
    "    task_3_result = findTopSentences(introduction, 10, False)\n",
    "    task_5_result = sentence_to_sentence(findTopSentences(introduction, 20, False))\n",
    "    task_7_results = sumySummarize(introduction)\n",
    "    result = [task_3_result, task_5_result, task_7_results[0], task_7_results[1], task_7_results[2]]\n",
    "    return result\n",
    "\n",
    "def calculate_rouge(peer, model):\n",
    "\n",
    "    rougeUno = rouge.rouge_1(peer, model, 1)\n",
    "    rougeBi = rouge.rouge_2(peer, model, 1)\n",
    "\n",
    "    return rougeUno,rougeBi\n",
    "\n",
    "task8_rouge_scores = []\n",
    "\n",
    "for i in range(10):\n",
    "    summarization_results = summarize_introduction(files[i])\n",
    "    results = []\n",
    "    for result in summarization_results:\n",
    "        result_str = \" \".join(result)\n",
    "        rouge_score = calculate_rouge(result_str, golds[i])\n",
    "        results.append(rouge_score)\n",
    "    task8_rouge_scores.append(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_transpose = []\n",
    "\n",
    "for i in task8_rouge_scores:\n",
    "    new_list = []\n",
    "    \n",
    "    for a in i:\n",
    "        new_list.append(a[0])\n",
    "        new_list.append(a[1])\n",
    "        \n",
    "    score_transpose.append(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Sweight</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Sentence to sentence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">LexRank</th>\n",
       "      <th colspan=\"2\" halign=\"left\">LSA</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Luhn</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Rouge 1</th>\n",
       "      <th>Rouge 2</th>\n",
       "      <th>Rouge 1</th>\n",
       "      <th>Rouge 2</th>\n",
       "      <th>Rouge 1</th>\n",
       "      <th>Rouge 2</th>\n",
       "      <th>Rouge 1</th>\n",
       "      <th>Rouge 2</th>\n",
       "      <th>Rouge 1</th>\n",
       "      <th>Rouge 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.786417</td>\n",
       "      <td>0.721710</td>\n",
       "      <td>0.836462</td>\n",
       "      <td>0.764451</td>\n",
       "      <td>0.881780</td>\n",
       "      <td>0.770568</td>\n",
       "      <td>0.793777</td>\n",
       "      <td>0.744200</td>\n",
       "      <td>0.778105</td>\n",
       "      <td>0.723498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814556</td>\n",
       "      <td>0.719457</td>\n",
       "      <td>0.865453</td>\n",
       "      <td>0.752024</td>\n",
       "      <td>0.913803</td>\n",
       "      <td>0.790072</td>\n",
       "      <td>0.928253</td>\n",
       "      <td>0.804403</td>\n",
       "      <td>0.865105</td>\n",
       "      <td>0.770384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.772098</td>\n",
       "      <td>0.636921</td>\n",
       "      <td>0.816362</td>\n",
       "      <td>0.664568</td>\n",
       "      <td>0.753766</td>\n",
       "      <td>0.657744</td>\n",
       "      <td>0.736401</td>\n",
       "      <td>0.622330</td>\n",
       "      <td>0.726089</td>\n",
       "      <td>0.644539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.849633</td>\n",
       "      <td>0.761986</td>\n",
       "      <td>0.942759</td>\n",
       "      <td>0.824017</td>\n",
       "      <td>0.847242</td>\n",
       "      <td>0.762476</td>\n",
       "      <td>0.866905</td>\n",
       "      <td>0.791071</td>\n",
       "      <td>0.789841</td>\n",
       "      <td>0.714029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.766896</td>\n",
       "      <td>0.647851</td>\n",
       "      <td>0.825667</td>\n",
       "      <td>0.706805</td>\n",
       "      <td>0.847481</td>\n",
       "      <td>0.726174</td>\n",
       "      <td>0.761547</td>\n",
       "      <td>0.652283</td>\n",
       "      <td>0.668436</td>\n",
       "      <td>0.569714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.490887</td>\n",
       "      <td>0.430440</td>\n",
       "      <td>0.522996</td>\n",
       "      <td>0.452512</td>\n",
       "      <td>0.609344</td>\n",
       "      <td>0.523740</td>\n",
       "      <td>0.576275</td>\n",
       "      <td>0.497893</td>\n",
       "      <td>0.440459</td>\n",
       "      <td>0.395546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.710258</td>\n",
       "      <td>0.626785</td>\n",
       "      <td>0.727599</td>\n",
       "      <td>0.650646</td>\n",
       "      <td>0.787852</td>\n",
       "      <td>0.707930</td>\n",
       "      <td>0.718450</td>\n",
       "      <td>0.645504</td>\n",
       "      <td>0.738662</td>\n",
       "      <td>0.655506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.930017</td>\n",
       "      <td>0.829865</td>\n",
       "      <td>0.962865</td>\n",
       "      <td>0.850697</td>\n",
       "      <td>0.944962</td>\n",
       "      <td>0.850816</td>\n",
       "      <td>0.955183</td>\n",
       "      <td>0.845333</td>\n",
       "      <td>0.950058</td>\n",
       "      <td>0.848635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.805026</td>\n",
       "      <td>0.737957</td>\n",
       "      <td>0.873221</td>\n",
       "      <td>0.789644</td>\n",
       "      <td>0.916003</td>\n",
       "      <td>0.815580</td>\n",
       "      <td>0.844484</td>\n",
       "      <td>0.773928</td>\n",
       "      <td>0.846774</td>\n",
       "      <td>0.769874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.434484</td>\n",
       "      <td>0.408990</td>\n",
       "      <td>0.441798</td>\n",
       "      <td>0.408498</td>\n",
       "      <td>0.423375</td>\n",
       "      <td>0.406331</td>\n",
       "      <td>0.520435</td>\n",
       "      <td>0.473978</td>\n",
       "      <td>0.419658</td>\n",
       "      <td>0.397820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.736027</td>\n",
       "      <td>0.652196</td>\n",
       "      <td>0.781518</td>\n",
       "      <td>0.686386</td>\n",
       "      <td>0.792561</td>\n",
       "      <td>0.701143</td>\n",
       "      <td>0.770171</td>\n",
       "      <td>0.685092</td>\n",
       "      <td>0.722319</td>\n",
       "      <td>0.648955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sweight           Sentence to sentence             LexRank            \\\n",
       "       Rouge 1   Rouge 2              Rouge 1   Rouge 2   Rouge 1   Rouge 2   \n",
       "1     0.786417  0.721710             0.836462  0.764451  0.881780  0.770568   \n",
       "2     0.814556  0.719457             0.865453  0.752024  0.913803  0.790072   \n",
       "3     0.772098  0.636921             0.816362  0.664568  0.753766  0.657744   \n",
       "4     0.849633  0.761986             0.942759  0.824017  0.847242  0.762476   \n",
       "5     0.766896  0.647851             0.825667  0.706805  0.847481  0.726174   \n",
       "6     0.490887  0.430440             0.522996  0.452512  0.609344  0.523740   \n",
       "7     0.710258  0.626785             0.727599  0.650646  0.787852  0.707930   \n",
       "8     0.930017  0.829865             0.962865  0.850697  0.944962  0.850816   \n",
       "9     0.805026  0.737957             0.873221  0.789644  0.916003  0.815580   \n",
       "10    0.434484  0.408990             0.441798  0.408498  0.423375  0.406331   \n",
       "mean  0.736027  0.652196             0.781518  0.686386  0.792561  0.701143   \n",
       "\n",
       "           LSA                Luhn            \n",
       "       Rouge 1   Rouge 2   Rouge 1   Rouge 2  \n",
       "1     0.793777  0.744200  0.778105  0.723498  \n",
       "2     0.928253  0.804403  0.865105  0.770384  \n",
       "3     0.736401  0.622330  0.726089  0.644539  \n",
       "4     0.866905  0.791071  0.789841  0.714029  \n",
       "5     0.761547  0.652283  0.668436  0.569714  \n",
       "6     0.576275  0.497893  0.440459  0.395546  \n",
       "7     0.718450  0.645504  0.738662  0.655506  \n",
       "8     0.955183  0.845333  0.950058  0.848635  \n",
       "9     0.844484  0.773928  0.846774  0.769874  \n",
       "10    0.520435  0.473978  0.419658  0.397820  \n",
       "mean  0.770171  0.685092  0.722319  0.648955  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = pd.MultiIndex.from_arrays([['Sweight','Sweight', 'Sentence to sentence','Sentence to sentence', 'LexRank','LexRank', 'LSA','LSA', 'Luhn','Luhn'],\n",
    "                                [\"Rouge 1\", \"Rouge 2\",\"Rouge 1\", \"Rouge 2\",\"Rouge 1\", \"Rouge 2\", \"Rouge 1\", \"Rouge 2\", \"Rouge 1\", \"Rouge 2\"]])\n",
    "data = pd.DataFrame(score_transpose, columns=col)\n",
    "\n",
    "\n",
    "data.index += 1\n",
    "data.loc['mean'] = data.mean()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sweight               Rouge 1    0.736027\n",
       "                      Rouge 2    0.652196\n",
       "Sentence to sentence  Rouge 1    0.781518\n",
       "                      Rouge 2    0.686386\n",
       "LexRank               Rouge 1    0.792561\n",
       "                      Rouge 2    0.701143\n",
       "LSA                   Rouge 1    0.770171\n",
       "                      Rouge 2    0.685092\n",
       "Luhn                  Rouge 1    0.722319\n",
       "                      Rouge 2    0.648955\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 9\n",
    "\n",
    "Design a simple GUI that allows the user to input a text or a link to a document to be summarized and output the summarizer according to 3), algorithms implemented in 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run simpleGUI.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
