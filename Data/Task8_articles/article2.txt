Sparse representation has attracted much attention from researchers in fields of signal processing, image processing, computer vision, and pattern recognition. Sparse representation also has a good reputation in both theoretical research and practical applications. Many different algorithms have been proposed for sparse representation. The main purpose of this paper is to provide a comprehensive study and an updated review on sparse representation and to supply guidance for researchers. The taxonomy of sparse representation methods can be studied from various viewpoints. For example, in terms of different norm minimizations used in sparsity constraints, the methods can be roughly categorized into five groups: 1) sparse representation with l 0 -norm minimization; 2) sparse representation with lp-norm (0 <; p <; 1) minimization; 3) sparse representation with l 1 -norm minimization; 4) sparse representation with l 2 ,1-norm minimization; and 5) sparse representation with l2-norm minimization. In this paper, a comprehensive overview of sparse representation is provided. The available sparse representation algorithms can also be empirically categorized into four groups: 1) greedy strategy approximation; 2) constrained optimization; 3) proximity algorithm-based optimization; and 4) homotopy algorithm-based sparse representation. The rationales of different algorithms in each category are analyzed and a wide range of sparse representation applications are summarized, which could sufficiently reveal the potential nature of the sparse representation theory. In particular, an experimentally comparative study of these sparse representation algorithms was presented.

With advancements in mathematics, linear representation methods (LRBM) have been well studied and have recently received considerable attention [1], [2]. The sparse representation method is the most representative methodology of the LRBM and has also been proven to be an extraordinary powerful solution to a wide range of application fields, especially in signal processing, image processing, machine learning, and computer vision, such as image denoising, debluring, inpainting, image restoration, super-resolution, visual tracking, image classification and image segmen-tation [3]–[4][5][6][7][8][9][10]. Sparse representation has shown huge potential capabilities in handling these problems.
Sparse representation, from the viewpoint of its origin, is directly related to compressed sensing (CS) [11]–[12][13], which is one of the most popular topics in recent years. Donoho [11] first proposed the original concept of compressed sensing. CS theory suggests that if a signal is sparse or compressive, the original signal can be reconstructed by exploiting a few measured values, which are much less than the ones suggested by previously used theories such as Shannon’s sampling theorem (SST). Candès et al. [13], from the mathematical perspective, demonstrated the rationale of CS theory, i.e. the original signal could be precisely reconstructed by utilizing a small portion of Fourier transformation coefficients. Baraniuk [12] provided a concrete analysis of compressed sensing and presented a specific interpretation on some solutions of different signal reconstruction algorithms. All these literature [11]–[12][13][14][15][16][17] laid the foundation of CS theory and provided the theoretical basis for future research. Thus, a large number of algorithms based on CS theory have been proposed to address different problems in various fields. Moreover, CS theory always includes the three basic components: sparse representation, encoding measuring, and reconstructing algorithm. As an indispensable prerequisite of CS theory, the sparse representation theory [4], [7]–[8][9][10], [17] is the most outstanding technique used to conquer difficulties that appear in many fields. For example, the methodology of sparse representation is a novel signal sampling method for the sparse or compressible signal and has been successfully applied to signal processing [4]–[5][6].
Sparse representation has attracted much attention in recent years and many examples in different fields can be found where sparse representation is definitely beneficial and favorable [18], [19]. One example is image classification, where the basic goal is to classify the given test image into several predefined categories. It has been demonstrated that natural images can be sparsely represented from the perspective of the properties of visual neurons. The sparse representation based classification (SRC) method [20] first assumes that the test sample can be sufficiently represented by samples from the same subject. Specifically, SRC exploits the linear combination of training samples to represent the test sample and computes sparse representation coefficients of the linear representation system, and then calculates the reconstruction residuals of each class employing the sparse representation coefficients and training samples. The test sample will be classified as a member of the class, which leads to the minimum reconstruction residual. The literature [20] has also demonstrated that the SRC method has great superiorities when addressing the image classification issue on corrupted or disguised images. In such cases, each natural image can be sparsely represented and the sparse representation theory can be utilized to fulfill the image classification task.
For signal processing, one important task is to extract key components from a large number of clutter signals or groups of complex signals in coordination with different requirements. Before the appearance of sparse representation, SST and Nyquist sampling law (NSL) were the traditional methods for signal acquisition and the general procedures included sampling, coding compression, transmission, and decoding. Under the frameworks of SST and NSL, the greatest difficulty of signal processing lies in efficient sampling from mass data with sufficient memory-saving. In such a case, sparse representation theory can simultaneously break the bottleneck of conventional sampling rules, i.e. SST and NSL, so that it has a very wide application prospect. Sparse representation theory proposes to integrate the processes of signal sampling and coding compression. Especially, sparse representation theory employs a more efficient sampling rate to measure the original sample by abandoning the pristine measurements of SST and NSL, and then adopts an optimal reconstruction algorithm to reconstruct samples. In the context of compressed sensing, it is first assumed that all the signals are sparse or approximately sparse enough [4], [6], [7]. Compared to the primary signal space, the size of the set of possible signals can be largely decreased under the constraint of sparsity. Thus, massive algorithms based on the sparse representation theory have been proposed to effectively tackle signal processing issues such as signal reconstruction and recovery. To this end, the sparse representation technique can save a significant amount of sampling time and sample storage space and it is favorable and advantageous.

Sparse representation has been extensively studied in recent years. This paper summarizes and presents various available sparse representation methods and discusses their motivations, mathematical representations and extensive applications. More specifically, we have analyzed their relations in theory and empirically introduced the applications including dictionary learning based on sparse representation and real-world applications such as image processing, image classification, and visual tracking.
Sparse representation has become a fundamental tool, which has been embedded into various learning systems and also has received dramatic improvements and unprecedented achievements. Furthermore, dictionary learning is an extremely popular topic and is closely connected with sparse representation. Currently, efficient sparse representation, robust sparse representation, and dictionary learning based on sparse representation seem to be the main streams of research on sparse representation methods. The low-rank representation technique has also recently aroused intensive research interests and sparse representation has been integrated into low-rank representation for constructing more reliable representation models. However, the mathematical justification of low-rank representation seems not to be elegant as sparse representation. Because employing the ideas of sparse representation as a prior can lead to state-of-the-art results, incorporating sparse representation with low-rank representation is worth further research. Moreover, subspace learning also has been becoming one of the most prevailing techniques in pattern recognition and computer vision. It is necessary to further study the relationship between sparse representation and subspace learning, and constructing more compact models for sparse subspace learning becomes one of the popular topics in various research fields. The transfer learning technique has emerged as a new learning framework for classification, regression and clustering problems in data mining and machine learning. However, sparse representation research still has been not fully applied to the transfer learning framework and it is significant to unify the sparse representation and low-rank representation techniques into the transfer learning framework to solve domain adaption, multitask learning, sample selection bias and covariate shift problems. Furthermore, researches on deep learning seems to become an overwhelming trend in the computer vision field. However, dramatically expensive training effort is the main limitation of current deep learning technique and how to fully introduce current sparse representation methods into the framework of deep learning is valuable and unsolved.
The application scope of sparse representation has emerged and has been widely extended to machine learning and computer vision fields. Nevertheless, the effectiveness and efficiency of sparse representation methods cannot perfectly meet the need for real-world applications. Especially, the complexities of sparse representation have greatly affected the applicability, especially the applicability to large scale problems. Enhancing the robustness of sparse representation is considered as another indispensable problem when researchers design algorithms. For image classification, the robustness should be seriously considered, such as the robustness to random corruptions, varying illuminations, outliers, occlusion and complex backgrounds. Thus, developing an efficient and robust sparse representation method for sparse representation is still the main challenge and to design a more effective dictionary is being expected and is beneficial to the performance improvement.
Sparse representation still has wide potential for various possible applications, such as event detection, scene reconstruction, video tracking, object recognition, object pose estimation, medical image processing, genetic expression and natural language processing. For example, the study of sparse representation in visual tracking is an important direction and more depth studies are essential to future further improvements of visual tracking research.
In addition, most sparse representation and dictionary learning algorithms focus on employing the l0 -norm or l1 -norm regularization to obtain a sparse solution. However, there are still only a few studies on l2,1 -norm regularization based sparse representation and dictionary learning algorithms. Moreover, other extended studies of sparse representation may be fruitful. In summary, the recent prevalence of sparse representation has extensively influenced different fields. It is our hope that the review and analysis presented in this paper can help and motivate more researchers to propose perfect sparse representation methods.