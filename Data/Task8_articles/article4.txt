Tensors or multiway arrays are functions of three or more indices (i, j, k, . . . )-similar to matrices (two-way arrays), which are functions of two indices (r, c) for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth and depth that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.

Tensors 1 (of order higher than two) are arrays indexed by three or more indices, say (i,j,k,…) – a generalization of matrices, which are indexed by two indices, say (r,c) for (row, column). Matrices are two-way arrays, and there are three- and higher-way arrays (or higher-order) tensors.
Tensor algebra has many similarities but also many striking differences with matrix algebra – e.g., low-rank tensor factorization is essentially unique under mild conditions; determining tensor rank is NP-hard, on the other hand, and the best low-rank approximation of a higher rank tensor may not even exist. Despite such apparent paradoxes and the learning curve needed to digest tensor algebra notation and data manipulation, tensors have already found many applications in signal processing (speech, audio, communications, radar, biomedical), machine learning (clustering, dimensionality reduction, latent factor models, subspace learning), and well beyond. Psychometrics (loosely defined as mathematical methods for the analysis of personality data) and later Chemometrics (likewise, for chemical data) have historically been two important application areas driving theoretical and algorithmic developments. Signal processing followed, in the 90's, but the real spark that popularized tensors came when the computer science community (notably those in machine learning, data mining, computing) discovered the power of tensor decompositions, roughly a decade ago [1]–[3]. There are nowadays many hundreds, perhaps thousands of papers published each year on tensor-related topics. Signal processing applications include, e.g., unsupervised separation of unknown mixtures of speech signals [4] and code-division communication signals without knowledge of their codes [5]; and emitter localization for radar, passive sensing, and communication applications [6], [7]. There are many more applications of tensor techniques that are not immediately recognized as such, e.g., the analytical constant modulus algorithm [8], [9]. Machine learning applications include face recognition, mining musical scores, and detecting cliques in social networks – see [10]–[12] and references therein. More recently, there has been considerable work on tensor decompositions for learning latent variable models, particularly topic models [13], and connections between orthogonal tensor decomposition and the method of moments for computing the Latent Dirichlet Allocation (LDA – a widely used topic model).
After two decades of research on tensor decompositions and applications, the senior co-authors still couldn't point their new graduate students to a single “point of entry” to begin research in this area. This article has been designed to address this need: to provide a fairly comprehensive and deep overview of tensor decompositions that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing related algorithms and software. While no single reference fits this bill, there are several very worthy tutorials and overviews that offer different points of view in certain aspects, and we would like to acknowledge them here. Among them, the highly-cited and clearly-written tutorial [14] that appeared 7 years ago in SIAM Review is perhaps the one closest to this article. It covers the basic models and algorithms (as of that time) well, but it does not go deep into uniqueness, advanced algorithmic, or estimation-theoretic aspects. The target audience of [14] is applied mathematics (SIAM). The recent tutorial [11] offers an accessible introduction, with many figures that help ease the reader into three-way thinking. It covers most of the bases and includes many motivating applications, but it also covers a lot more beyond the basics and thus stays at a high level. The reader gets a good roadmap of the area, without delving into it enough to prepare for research. Another recent tutorial on tensors is [15], which adopts a more abstract point of view of tensors as mappings from a linear space to another, whose coordinates transform multilinearly under a change of bases. This article is more suited for people interested in tensors as a mathematical concept, rather than how to use tensors in science and engineering. It includes a nice review of tensor rank results and a brief account of uniqueness aspects, but nothing in the way of algorithms or tensor computations. An overview of tensor techniques for large-scale numerical computations is given in [16], [17], geared towards a scientific computing audience; see [18] for a more accessible introduction. A gentle introduction to tensor decompositions can be found in the highly cited Chemometrics tutorial [19] – a bit outdated but still useful for its clarity – and the more recent book [20]. Finally, [21] is an upcoming tutorial with emphasis on scalability and data fusion applications – it does not go deep into tensor rank, identifiability, decomposition under constraints, or statistical performance benchmarking.
None of the above offers a comprehensive overview that is sufficiently deep to allow one to appreciate the underlying mathematics, the rapidly expanding and diversifying toolbox of tensor decomposition algorithms, and the basic ways in which tensor decompositions are used in signal processing and machine learning – and they are quite different. Our aim in this paper is to give the reader a tour that goes ‘under the hood’ on the technical side, and, at the same time, serve as a bridge between the two areas. Whereas we cannot include detailed proofs of some of the deepest results, we do provide insightful derivations of simpler results and sketch the line of argument behind more general ones. For example, we include a one-page self-contained proof of Kruskal's condition when one factor matrix is full column rank, which illuminates the role of Kruskal-rank in proving uniqueness. We also ‘translate’ between the signal processing (SP) and machine learning (ML) points of view. In the context of the canonical polyadic decomposition (CPD), also known as parallel factor analysis (PARAFAC), SP researchers (and Chemists) typically focus on the columns of the factor matrices A, B, C and the associated rank-1 factors af⊚bf⊚cf of the decomposition (where ⊚ denotes the outer product, see Section II-C), because they are interested in separation. ML researchers often focus on the rows of A, B, C, because they think of them as parsimonious latent space representations. For a user × item × context ratings tensor, for example, a row of A is a representation of the corresponding user in latent space, and likewise a row of B ( C) is a representation of the corresponding item (context) in the same latent space. The inner product of these three vectors is used to predict that user's rating of the given item in the given context. This is one reason why ML researchers tend to use inner (instead of outer) product notation. SP researchers are interested in model identifiability because it guarantees separability; ML researchers are interested in identifiability to be able to interpret the dimensions of the latent space. In co-clustering applications, on the other hand, the rank-1 tensors af⊚bf⊚cf capture latent concepts that the analyst seeks to learn from the data (e.g., cliques of users buying certain types of items in certain contexts). SP researchers are trained to seek optimal solutions, which is conceivable for small to moderate data; they tend to use computationally heavier algorithms. ML researchers are nowadays trained to think about scalability from day one, and thus tend to choose much more lightweight algorithms to begin with. There are many differences, but also many similarities and opportunities for cross-fertilization. Being conversant in both communities allows us to bridge the ground between and help SP and ML researchers better understand each other.

As we wrap up this admittedly long article, we would like to point out some widely available resources that can help bring the reader up to speed experimenting with tensors in minutes. Matlab provides native support for tensor objects, but working with tensors is facilitated by these freely available toolboxes:
The n-way toolbox http://www.models.life.ku.dk/nwaytoolbox by Bro et al. [137], based on ALS (with Gauss-Newton, line-search and other methods as an option) incorporates many non-parametric types of constraints, such as non-negativity;
The tensor toolbox http://www.sandia.gov/∼tgkolda/TensorToolbox/index-2.6.html by Kolda et al. [138], [139] was the first to provide support for sparse, dense, and factored tensor classes, alongside standard ALS and all-at-once algorithms for CPD, MLSVD, and other factorizations;
Tensorlab http://www.tensorlab.net/ by De Lathauwer et al. [140], builds upon the complex optimization framework and offers numerical algorithms for computing CPD, MLSVD and more general block term decompositions. It includes a library of constraints and regularization penalties and offers the possibility to combine and jointly factorize dense, sparse, structured and incomplete tensors. It provides special routines for large-scale problems and visualization.
SPLATT http://glaros.dtc.umn.edu/gkhome/splatt/overview by Smith et al. is a high-performance computing software toolkit for parallel sparse tensor factorization. It contains memory- and operation-efficient algorithms that allows it to compute PARAFAC decompositions of very large sparse datasets. SPLATT is written in C and OpenMP.
The TensorPackage http://www.gipsa-lab.fr/∼pierre.comon/TensorPackage/tensorPackage.html by Comon et al., which includes various algorithms for CPD and employs enhanced line search [94] .
While these toolboxes are great to get you going and for rapid prototyping, when it comes to really understanding what you’re doing with tensors, there is nothing as valuable as programming ALS for CPD and ⊥-Tucker yourself, and trying them on real data. Towards this end, we have produced educational “plain-vanilla” programs (CPD-ALS, MLSVD, ⊥-Tucker-ALS, CPD-GD, CPD-SGD), and simple but instructive demos (multichannel speech separation, and faces tensor compression) which are provided as supplementary material together with this article.
Tensor decomposition has come a long way since Hitchcock ’27, [141], Cattell ’44 [142], and later Harshman ’70-’72 [31], [32], Carroll and Chang [143], and Kruskal's ’77 [35] seminal papers. It is now a vibrant field that is well-represented in major IEEE, ACM, SIAM, and other mainstream conferences. The cross-disciplinary community that nurtured tensor decomposition research during the years that it was a niche area has two dedicated workshops that usually happen every three years: the TRICAP (Three-way methods In Chemistry and Psychology) workshop, which was last organized in 2015 at Pecol – Val di Zoldo (Belluno), Italy http://people.ece.umn.edu/∼nikos/TRICAP_home.html; and the TDA (Tensor Decompositions and Applications) workshop, which was last organized in 2016 at Leuven, Belgium http://www.esat.kuleuven.be/stadius/TDA2016/.
In terms of opportunities and challenges ahead, we see the need for more effective and tractable tensor rank detection criteria, and flexible and scalable algorithms that can handle very big datasets while offering identifiability, convergence, and parameter RMSE performance guarantees – at least under certain reasonable conditions. Data fusion, in the form of coupled decomposition of several related tensors and matrices is a very promising direction with numerous applications. More broadly, we believe that machine learning offers a wide range of potential applications, and this is one of the reasons why we wrote this article. Tensor decomposition in higher rank blocks is another interesting but relatively under-explored area with many applications. Finally, using multilinear models such as tensor trains as “universal approximants” has been gaining traction and will continue to grow in the foreseeable future, as a way to get away from the “curse of dimensionality” [56], [108]– [116], [144] –[158].